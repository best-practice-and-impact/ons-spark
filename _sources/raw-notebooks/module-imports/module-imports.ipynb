{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naming Conflicts in Module Imports\n",
    "\n",
    "Importing modules in Python and R can lead to naming conflicts if a function with that name already exists. This article demonstrates why you should be careful when importing modules to ensure that these conflicts do not occur.\n",
    "\n",
    "A common example in Python is using [`from pyspark.sql.functions import *`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html), which will overwrite some built-in Python functions (e.g. `sum()`). Instead, it is good practice to use `from pyspark.sql import functions as F`, where you prefix the functions  with `F`, e.g. [`F.sum()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.sum.html).\n",
    "\n",
    "### Naming variables\n",
    "\n",
    "When writing code, it is important to give your variables sensible names, that are informative but not too long. A good reference on this is the [Clean Code](https://best-practice-and-impact.github.io/qa-of-code-guidance/core_programming.html#clean-code) section from [QA of Code for Analysis and Research](https://best-practice-and-impact.github.io/qa-of-code-guidance/intro.html). **You should avoid using the names of existing built in functions for user-defined variables**.\n",
    "\n",
    "### Keywords\n",
    "\n",
    "Some words are reserved: for instance, in Python you cannot have a variable called `def`, `False` or `lambda`. These are referred to as *keywords* and the code will not even compile if you try, raising a `SyntaxError`. You can generate a list of these with [`keyword.kwlist`](https://docs.python.org/3/library/keyword.html).\n",
    "\n",
    "In R, use `?reserved` to get a list of the reserved words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['False', 'None', 'True', 'and', 'as', 'assert', 'break', 'class', 'continue', 'def', 'del', 'elif', 'else', 'except', 'finally', 'for', 'from', 'global', 'if', 'import', 'in', 'is', 'lambda', 'nonlocal', 'not', 'or', 'pass', 'raise', 'return', 'try', 'while', 'with', 'yield']\n"
     ]
    }
   ],
   "source": [
    "import keyword\n",
    "print(keyword.kwlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "?reserved\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Built in functions and module imports in Python\n",
    "\n",
    "<details>\n",
    "    \n",
    "<summary><b>Python Example</b></summary>\n",
    "\n",
    "You might notice that the Python keyword list is quite short and that some common Python functionality is not listed, for instance, `sum()` or `round()`. This means that it is possible to overwrite these; obviously this is not good practice and should be avoided. \n",
    "\n",
    "This can be surprisingly easy to do in PySpark, and can be hard to debug if you do not know the reason for the error.\n",
    "\n",
    "#### Python Example\n",
    "\n",
    "First, look at the documentation for `sum`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function sum in module builtins:\n",
      "\n",
      "sum(iterable, start=0, /)\n",
      "    Return the sum of a 'start' value (default: 0) plus an iterable of numbers\n",
      "    \n",
      "    When the iterable is empty, return the start value.\n",
      "    This function is intended specifically for use with numeric values and may\n",
      "    reject non-numeric types.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show that `sum` works with a simple example: adding three integers together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([1, 2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now import the modules we need to use Spark. The recommended way to do this is `import pyspark.sql.functions as F`, which means that whenever you want to access a function from this module you prefix it with `F`, e.g. `F.sum()`. Sometimes the best way to see why something is recommended is to try a different method and show it is a bad idea, in this case, importing all the `functions` as `*`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempting to sum the integers will now give an error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute '_jvm'\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    sum([1, 2, 3])\n",
    "except AttributeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see why this error exists, take another look at `help(sum)`; we can see that the documentation is different to previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function sum in module pyspark.sql.functions:\n",
      "\n",
      "sum(col)\n",
      "    Aggregate function: returns the sum of all values in the expression.\n",
      "    \n",
      "    .. versionadded:: 1.3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So by importing all the PySpark functions we have overwritten some key Python functionality. Note that this would also apply if you imported individual functions, e.g. `from pyspark.sql.functions import sum`.\n",
    "\n",
    "You can also overwrite functions with your own variables, often unintentionally. As an example, first Start a Spark session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession.builder.master(\"local[2]\")\n",
    "         .appName(\"module-imports\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a small DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "| id|double_id|\n",
      "+---+---------+\n",
      "|  0|        0|\n",
      "|  1|        2|\n",
      "|  2|        4|\n",
      "|  3|        6|\n",
      "|  4|        8|\n",
      "+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf = spark.range(5).withColumn(\"double_id\", col(\"id\") * 2)\n",
    "sdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop through the columns, using `col` as the control variable. This will work, but is not a good idea as it is overwriting `col()` from `functions`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "+---+\n",
      "\n",
      "+---------+\n",
      "|double_id|\n",
      "+---------+\n",
      "|        0|\n",
      "|        2|\n",
      "|        4|\n",
      "|        6|\n",
      "|        8|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for col in sdf.columns:\n",
    "    sdf.select(col).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we try adding another column with `col()` then it will not work as we have now reassigned `col` to be `double_id`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'str' object is not callable\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    sdf = sdf.withColumn(\"triple_id\", col(\"id\") * 3)\n",
    "except TypeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'double_id'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the PySpark `functions` as `F` and using [`F.col()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.col.html) solves this problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---------+\n",
      "| id|double_id|triple_id|\n",
      "+---+---------+---------+\n",
      "|  0|        0|        0|\n",
      "|  1|        2|        3|\n",
      "|  2|        4|        6|\n",
      "|  3|        6|        9|\n",
      "|  4|        8|       12|\n",
      "+---+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F \n",
    "sdf = sdf.withColumn(\"triple_id\", F.col(\"id\") * 3)\n",
    "sdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "### Built in functions and package imports in R\n",
    "\n",
    "<details>\n",
    "<summary><b>R Example</b></summary>\n",
    "\n",
    "It is advised to use `::` to directly call a function from a package. For instance, there is a `filter` function in both [`stats`](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/filter.html) and [`dplyr`](https://dplyr.tidyverse.org/reference/filter.html); you can specify exactly which to use with `dplyr::filter()` or `stats::filter()`.\n",
    "\n",
    "Note that despite being commonly used for an R DataFrame, [`df` is actually a built-in function for the F distribution](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/Fdist.html). As such, it is not recommended to use `df` for DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "?df\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "### Further Resources\n",
    "\n",
    "PySpark Documentation:\n",
    "- [`pyspark.sql.functions`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html)\n",
    "- [`F.col()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.col.html)\n",
    "- [`F.sum()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.sum.html)\n",
    "\n",
    "sparklyr and tidyverse Documentation:\n",
    "- [`dplyr::filter()`](https://dplyr.tidyverse.org/reference/filter.html)\n",
    "\n",
    "Python Documentation:\n",
    "- [`keyword.kwlist`](https://docs.python.org/3/library/keyword.html)\n",
    "\n",
    "R Documentation:\n",
    "- [`stats::df()`](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/Fdist.html)\n",
    "- [`stats::filter()`](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/filter.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
