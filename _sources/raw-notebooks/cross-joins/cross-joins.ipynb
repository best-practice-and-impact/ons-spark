{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86c153a9",
   "metadata": {},
   "source": [
    "## Cross Joins\n",
    "\n",
    "A *cross join* is used to return every combination of the rows of two DataFrames. Cross joins are also referred to as the *cartesian product* of two DataFrames. It is different to other types of joins, which depend on matching values by using join keys.\n",
    "\n",
    "As a cross join will return every combination of the rows, the size of the returned DataFrame is equal to the product of the row count of both source DataFrames; this can quickly get large and overwhelm your Spark session. As such, use them carefully!\n",
    "\n",
    "The syntax for cross joins is different in PySpark and sparklyr. In PySpark, DataFrames have a [`.crossJoin()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.crossJoin.html) method. In sparklyr, use [`full_join()`](https://spark.rstudio.com/packages/sparklyr/latest/reference/join.tbl_spark.html) with `by=character()`.\n",
    "\n",
    "One use case for cross joins is to return every combination when producing results that involve grouping and aggregation, even when some of these are zero. Cross joins are also commonly used in [salted joins](../spark-concepts/salted-joins), used to improve the efficiency of a join when the join keys are skewed.\n",
    "\n",
    "### Example: producing all combinations of results\n",
    "\n",
    "First, start a Spark session (disabling broadcast joins by default) read in the Animal Rescue data, group by `animal_group` and `cal_year`, and then get the sum of `total_cost`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9949854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+--------+-------+\n",
      "|animal_group                    |cal_year|cost   |\n",
      "+--------------------------------+--------+-------+\n",
      "|Snake                           |2018    |333.0  |\n",
      "|Deer                            |2015    |1788.0 |\n",
      "|Unknown - Domestic Animal Or Pet|2017    |2622.0 |\n",
      "|Bird                            |2012    |32240.0|\n",
      "|Hedgehog                        |2009    |520.0  |\n",
      "+--------------------------------+--------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import yaml\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "\n",
    "spark = (SparkSession.builder.master(\"local[2]\")\n",
    "         .appName(\"joins\")\n",
    "         # Disable Broadcast join by default\n",
    "         .config(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
    "         .getOrCreate())\n",
    "\n",
    "with open(\"../../../config.yaml\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "    \n",
    "rescue_path = config[\"rescue_path\"]\n",
    "\n",
    "rescue = (spark.read.parquet(rescue_path)\n",
    "          .groupBy(\"animal_group\", \"cal_year\")\n",
    "          .agg(F.sum(\"total_cost\").alias(\"cost\")))\n",
    "\n",
    "rescue.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e9d95a",
   "metadata": {},
   "source": [
    "```r\n",
    "library(sparklyr)\n",
    "library(dplyr)\n",
    "\n",
    "sc <- sparklyr::spark_connect(\n",
    "    master = \"local[2]\",\n",
    "    app_name = \"joins\",\n",
    "    config = sparklyr::spark_config())\n",
    "\n",
    "config <- yaml::yaml.load_file(\"ons-spark/config.yaml\")\n",
    "\n",
    "rescue <- sparklyr::spark_read_parquet(sc, config$rescue_path) %>%\n",
    "    dplyr::group_by(animal_group, cal_year) %>%\n",
    "    dplyr::summarise(cost = sum(total_cost)) %>%\n",
    "    dplyr::ungroup()\n",
    "\n",
    "rescue %>%\n",
    "    head(5) %>%\n",
    "    sparklyr::collect() %>%\n",
    "    print()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17676b9",
   "metadata": {},
   "source": [
    "If there are no animals rescued in a particular year then there are no rows to sum, and so nothing will be returned. In some cases we would prefer a zero value to be returned, rather than an empty DataFrame.\n",
    "\n",
    "For instance, no data will be returned for `Hamster` and `2012`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01703b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+----+\n",
      "|animal_group|cal_year|cost|\n",
      "+------------+--------+----+\n",
      "+------------+--------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rescue.filter((F.col(\"animal_group\") == \"Hamster\") & (F.col(\"cal_year\") == 2012)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5667a4eb",
   "metadata": {},
   "source": [
    "```r\n",
    "rescue %>%\n",
    "    dplyr::filter(animal_group == \"Hamster\" & cal_year == 2012) %>%\n",
    "    sparklyr::collect() %>%\n",
    "    print()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1038ea",
   "metadata": {},
   "source": [
    "A cross join can be used here, to return every combination of `animal_group` and `cal_year`, which then serves as a base DataFrame which `rescue` can be joined to. First, create a DataFrame for unique `animals` and `cal_years` and sort them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3587ef3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct animal count: 27\n",
      "Distinct year count: 11\n"
     ]
    }
   ],
   "source": [
    "animals = rescue.select(\"animal_group\").distinct().orderBy(\"animal_group\")\n",
    "cal_years = rescue.select(\"cal_year\").distinct().orderBy(\"cal_year\")\n",
    "\n",
    "print(f\"Distinct animal count: {animals.count()}\")\n",
    "print(f\"Distinct year count: {cal_years.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca8cbe4",
   "metadata": {},
   "source": [
    "```r\n",
    "\n",
    "animals <- rescue %>%\n",
    "    sparklyr::select(animal_group) %>%\n",
    "    sparklyr::sdf_distinct() %>%\n",
    "    dplyr::arrange(animal_group)\n",
    "    \n",
    "cal_years <- rescue %>%\n",
    "    sparklyr::select(cal_year) %>%\n",
    "    sparklyr::sdf_distinct() %>%\n",
    "    dplyr::arrange(cal_year)\n",
    "\n",
    "print(paste(\"Distinct animal count:\",  sparklyr::sdf_nrow(animals), sep=\" \"))\n",
    "print(paste(\"Distinct year count:\",  sparklyr::sdf_nrow(cal_years), sep=\" \"))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514bab93",
   "metadata": {},
   "source": [
    "There can be issues in Spark when joining values from a DF to itself, which can be resolved by checkpointing to break the lineage of the DataFrame. See the article on [checkpointing](../raw-notebooks/checkpoint-staging/checkpoint-staging) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5536a424",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = config[\"checkpoint_path\"]\n",
    "spark.sparkContext.setCheckpointDir(checkpoint_path)\n",
    "\n",
    "animals = animals.checkpoint()\n",
    "cal_years = cal_years.checkpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b17a25",
   "metadata": {},
   "source": [
    "```r\n",
    "sparklyr::spark_set_checkpoint_dir(sc, config$checkpoint_path)\n",
    "\n",
    "animals <- sparklyr::sdf_checkpoint(animals)\n",
    "cal_years <- sparklyr::sdf_checkpoint(cal_years)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0829ec34",
   "metadata": {},
   "source": [
    "Now cross join `cal_years` and `animals`. In PySpark, use [`.crossJoin()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.crossJoin.html), which is a method applied to the DataFrame and takes one argument, the DF to cross join to. In sparklyr, there is no native cross join function, so use [`full_join()`](https://spark.rstudio.com/packages/sparklyr/latest/reference/join.tbl_spark.html) with `by=character()`.\n",
    "\n",
    "Note that apart from the order of the columns the result will be the same regardless of which DF is on the left and which is on the right. The `result` DF will have $27 \\times 11 = 297$ rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16414435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "297"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = animals.crossJoin(cal_years).orderBy(\"animal_group\", \"cal_year\")\n",
    "result.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a98f9d",
   "metadata": {},
   "source": [
    "```r\n",
    "result <- animals %>%\n",
    "    sparklyr::full_join(cal_years, by=character()) %>%\n",
    "    dplyr::arrange(animal_group, cal_year)\n",
    "    \n",
    "sparklyr::sdf_nrow(result)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284d7d12",
   "metadata": {},
   "source": [
    "Previewing `result` we can see that every combination has been returned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58a55256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+\n",
      "|animal_group|cal_year|\n",
      "+------------+--------+\n",
      "|        Bird|    2009|\n",
      "|        Bird|    2010|\n",
      "|        Bird|    2011|\n",
      "|        Bird|    2012|\n",
      "|        Bird|    2013|\n",
      "|        Bird|    2014|\n",
      "|        Bird|    2015|\n",
      "|        Bird|    2016|\n",
      "|        Bird|    2017|\n",
      "|        Bird|    2018|\n",
      "|        Bird|    2019|\n",
      "|      Budgie|    2009|\n",
      "|      Budgie|    2010|\n",
      "|      Budgie|    2011|\n",
      "|      Budgie|    2012|\n",
      "+------------+--------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.show(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96f90c0",
   "metadata": {},
   "source": [
    "```r\n",
    "result %>%\n",
    "    head(15) %>%\n",
    "    sparklyr::collect() %>%\n",
    "    print()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03497f2",
   "metadata": {},
   "source": [
    "Now left join `rescue` to `result`, to return the `cost`. Combinations which do not exist in `rescue` will return a null `cost`.\n",
    "\n",
    "Note that the DF has been reordered during the join, as [sort merge join](../spark-concepts/join-concepts.html#sort-merge-join) was used rather than a [broadcast join](../spark-concepts/join-concepts.html#broadcast-join). See the article on [optimising joins](../spark-concepts/join-concepts)for more information on this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ca2bf31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+-------+\n",
      "|        animal_group|cal_year|   cost|\n",
      "+--------------------+--------+-------+\n",
      "|                Deer|    2015| 1788.0|\n",
      "|               Snake|    2018|  333.0|\n",
      "|Unknown - Animal ...|    2011|   null|\n",
      "|                Goat|    2013|   null|\n",
      "|                Bird|    2012|32240.0|\n",
      "|            Hedgehog|    2009|  520.0|\n",
      "|               Sheep|    2013|   null|\n",
      "|Unknown - Domesti...|    2017| 2622.0|\n",
      "|              Ferret|    2017|   null|\n",
      "|               Horse|    2012|16120.0|\n",
      "+--------------------+--------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = result.join(rescue, on = [\"animal_group\", \"cal_year\"], how=\"left\")\n",
    "result.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0109da",
   "metadata": {},
   "source": [
    "```r\n",
    "result <- result %>%\n",
    "    sparklyr::left_join(rescue, by = c(\"animal_group\", \"cal_year\"))\n",
    "\n",
    "result %>%\n",
    "    head(10) %>%\n",
    "    sparklyr::collect() %>%\n",
    "    print()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fba41ca",
   "metadata": {},
   "source": [
    "The final stage is to change any null values to zero and re-order the DF.\n",
    "\n",
    "We can now filter on `Hamster` to see that `2012` now exists and is returned with a zero value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc1ec6ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+------+\n",
      "|animal_group|cal_year|  cost|\n",
      "+------------+--------+------+\n",
      "|     Hamster|    2009|   0.0|\n",
      "|     Hamster|    2010| 780.0|\n",
      "|     Hamster|    2011| 780.0|\n",
      "|     Hamster|    2012|   0.0|\n",
      "|     Hamster|    2013| 870.0|\n",
      "|     Hamster|    2014| 295.0|\n",
      "|     Hamster|    2015|   0.0|\n",
      "|     Hamster|    2016|1630.0|\n",
      "|     Hamster|    2017|   0.0|\n",
      "|     Hamster|    2018|   0.0|\n",
      "|     Hamster|    2019|   0.0|\n",
      "+------------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = result.fillna(0, \"cost\").orderBy(\"animal_group\", \"cal_year\")\n",
    "result.filter(F.col(\"animal_group\") == \"Hamster\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a04f65a",
   "metadata": {},
   "source": [
    "```r\n",
    "result <- result %>%\n",
    "    sparklyr::mutate(cost = ifnull(cost, 0)) %>%\n",
    "    dplyr::arrange(animal_group, cal_year)\n",
    "    \n",
    "result %>%\n",
    "    sparklyr::filter(animal_group == \"Hamster\") %>%\n",
    "    sparklyr::collect() %>%\n",
    "    print()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6887b9",
   "metadata": {},
   "source": [
    "### Accidental Cross Joins\n",
    "\n",
    "Cross joins can be dangerous due to the size of DataFrame that they return. In PySpark, it is possible to create a cross join accidentally when using a regular join where the key column only has one distinct value in it. In these cases Spark will sometimes process the join as a cross join and return an error, depending on the lineage of the DataFrame. You can disable this error with `.config(\"spark.sql.crossJoin.enabled\", \"true\")`.\n",
    "\n",
    "Note that this error is specific to Spark 2; in Spark 3, [this option is enabled by default](https://github.com/apache/spark/pull/25520) and so the error will only occur if you explicitly enable it.\n",
    "\n",
    "This error will not occur in sparklyr.\n",
    "\n",
    "To demonstrate this in PySpark (version `2.4.0`), create a new DF, `dogs`, with just one column, `animal_group` containing just `Dog`, and join another DF to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4140f70b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Detected implicit cartesian product for LEFT OUTER join between logical plans\\nRange (0, 5, step=1, splits=Some(2))\\nand\\nProject [animal_noise#203]\\n+- Filter (isnotnull(animal_group#202) && (Dog = animal_group#202))\\n   +- LogicalRDD [animal_group#202, animal_noise#203], false\\nJoin condition is missing or trivial.\\nEither: use the CROSS JOIN syntax to allow cartesian products between these\\nrelations, or: enable implicit cartesian products by setting the configuration\\nvariable spark.sql.crossJoin.enabled=true;'\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "dogs = spark.range(5).withColumn(\"animal_group\", F.lit(\"Dog\"))\n",
    "\n",
    "animal_noise = spark.createDataFrame([\n",
    "    [\"Cat\", \"Meow\"],\n",
    "    [\"Dog\", \"Woof\"],\n",
    "    [\"Cow\", \"Moo\"]],\n",
    "    [\"animal_group\", \"animal_noise\"])\n",
    "\n",
    "try:\n",
    "    dogs.join(animal_noise, on=\"animal_group\", how=\"left\").show()\n",
    "except AnalysisException as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd0ac4a",
   "metadata": {},
   "source": [
    "To resolve this, start a new Spark session and add `.config(\"spark.sql.crossJoin.enabled\", \"true\")` to the config. The DFs will have been cleared by closing the Spark session and so need to be created again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7539ec63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---+------------+\n",
      "|animal_group| id|animal_noise|\n",
      "+------------+---+------------+\n",
      "|         Dog|  0|        Woof|\n",
      "|         Dog|  1|        Woof|\n",
      "|         Dog|  2|        Woof|\n",
      "|         Dog|  3|        Woof|\n",
      "|         Dog|  4|        Woof|\n",
      "+------------+---+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.stop()\n",
    "\n",
    "spark = (SparkSession.builder.master(\"local[2]\")\n",
    "         .appName(\"joins\")\n",
    "         .config(\"spark.sql.crossJoin.enabled\", \"true\")\n",
    "         .getOrCreate())\n",
    "\n",
    "dogs = spark.range(5).withColumn(\"animal_group\", F.lit(\"Dog\"))\n",
    "\n",
    "animal_noise = spark.createDataFrame([\n",
    "    [\"Cat\", \"Meow\"],\n",
    "    [\"Dog\", \"Woof\"],\n",
    "    [\"Cow\", \"Moo\"]],\n",
    "    [\"animal_group\", \"animal_noise\"])\n",
    "\n",
    "dogs.join(animal_noise, on=\"animal_group\", how=\"left\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d1168a",
   "metadata": {},
   "source": [
    "Finally, clear the checkpoints that were used in the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b550fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = subprocess.run(f\"hdfs dfs -rm -r -skipTrash {checkpoint_path}\", shell=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0a3a49",
   "metadata": {},
   "source": [
    "```r\n",
    "system(paste0(\"hdfs dfs -rm -r -skipTrash \", config$checkpoint_path))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59af4b15",
   "metadata": {},
   "source": [
    "### Salted Joins\n",
    "\n",
    "Cross joins can also be used when *salting* a join. Salting improves the efficiency of a join by reducing the skew in the DataFrame, by introducing new join keys. See the article on [salted joins](../spark-concepts/salted-joins) for an example.\n",
    "\n",
    "### Further Resources\n",
    "\n",
    "Spark at the ONS Articles:\n",
    "- [Salted Joins](../spark-concepts/salted-joins)\n",
    "- [Checkpoint and Staging Tables](../raw-notebooks/checkpoint-staging/checkpoint-staging)\n",
    "- [Optimising Joins](../spark-concepts/join-concepts)\n",
    "    - [Sort Merge Join](../spark-concepts/join-concepts.html#sort-merge-join)\n",
    "\t- [Broadcast Join](../spark-concepts/join-concepts.html#broadcast-join)\n",
    "    \n",
    "PySpark Documentation:\n",
    "- [`.crossJoin()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.crossJoin.html)\n",
    "- [`.checkpoint()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.checkpoint.html)\n",
    "- [`.distinct()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.distinct.html)\n",
    "\n",
    "sparklyr Documentation:\n",
    "- [`full_join()`](https://spark.rstudio.com/packages/sparklyr/latest/reference/join.tbl_spark.html): there is no native cross join function in sparklyr; the documentation recommends using `by=character()`\n",
    "- [`sdf_checkpoint()`](https://spark.rstudio.com/packages/sparklyr/latest/reference/sdf_checkpoint.html)\n",
    "- [`sdf_distinct()`](https://spark.rstudio.com/packages/sparklyr/latest/reference/sdf_distinct.html)\n",
    "\n",
    "Other links:\n",
    "- [GitHub: SPARK-28621](https://github.com/apache/spark/pull/25520): pull request for setting `spark.sql.crossJoin.enabled` to be `true` by default in Spark 3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
