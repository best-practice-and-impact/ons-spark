{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managing Partitions\n",
    "\n",
    "DataFrames in Spark are *distributed*, so although we treat them as one object they might be split up into multiple partitions over many machines on the cluster. The benefit of having multiple partitions is that some operations can be performed on the partitions in parallel. \n",
    "\n",
    "More partitions means greater *parallelisation*. However, there is a cost associated with multiple partitions, for example scheduling delay and data serialisation. In the [Spark Application and UI](../spark-concepts/spark-application-and-ui) article we saw that putting a small DataFrame into one or two partitions can lead to more efficient processing. \n",
    "\n",
    "In this notebook we will explore these partitions and how the partitioning of a DataFrame is affected by importing, processing and writing data in Spark. The goal is not to find the optimum number of partitions for a given DataFrame, the goal is to have greater awareness of partitioning so we can avoid the extreme cases of over-partitioning, under-partitioning and highly skewed partitioning.\n",
    "\n",
    "### Investigating partitions\n",
    "\n",
    "We will start by investigating the partitions of some DataFrames. We will look at the partitions of a newly created DataFrame, an imported DataFrame and a processed DataFrame. But first, we'll need to do some imports and create a local application in the usual way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "from pyspark.sql import SparkSession, Window, functions as F\n",
    "\n",
    "spark = (SparkSession.builder.master(\"local[2]\")\n",
    "         .appName(\"partitions\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "library(sparklyr)\n",
    "\n",
    "sc <- sparklyr::spark_connect(\n",
    "    master = \"local[2]\",\n",
    "    app_name = \"partitions\",\n",
    "    config = sparklyr::spark_config())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Newly created DataFrame\n",
    "\n",
    "Let's create a DataFrame called `rand_df` with an `id` column from 0 to 4,999 and a `rand_val` column containing random numbers from 1 to 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|rand_val|\n",
      "+---+--------+\n",
      "|  0|       7|\n",
      "|  1|       9|\n",
      "|  2|      10|\n",
      "|  3|       9|\n",
      "|  4|       5|\n",
      "|  5|       6|\n",
      "|  6|       1|\n",
      "|  7|       2|\n",
      "|  8|       4|\n",
      "|  9|       7|\n",
      "+---+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "row_ct = 5000\n",
    "seed_no = 42 #this is used to create the pseudo-random numbers\n",
    "\n",
    "rand_df = spark.range(0, row_ct)\n",
    "rand_df = rand_df.withColumn(\"rand_val\", F.ceil(F.rand(seed=seed_no)*10))\n",
    "\n",
    "rand_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "row_ct <- 5000\n",
    "seed_no <- 42L #this is used to create the pseudo-random numbers\n",
    "\n",
    "rand_df <- sparklyr::sdf_seq(sc, 0, row_ct - 1) %>%\n",
    "    sparklyr::mutate(rand_val = ceiling(rand(seed_no)*10))\n",
    "\n",
    "rand_df %>% head(10) %>% sparklyr::collect()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the number of partitions of a DataFrame in PySpark we need to access the underlying RDD structures that make up the DataFrame by using `.rdd` after referencing the DataFrame. Then we can use the [`.getNumPartitions()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.RDD.getNumPartitions.html) method to return the number of partitions. In sparklyr we can just use the function [`sdf_num_partitions()`](https://spark.rstudio.com/packages/sparklyr/latest/reference/sdf_num_partitions.html).\n",
    "\n",
    "We can also find out how many rows are in each partition. There is more than one way of doing this, one method was introduced in the [Shuffling](../spark-concepts/shuffling) article and will be used later in this article, a second method is shown below. Again, in PySpark we will need to access the underlying RDDs then map on a `lambda` function that will loop over the rows in each partition and return a count of these rows in a list. In sparklyr we can use [`spark_apply()`](https://spark.rstudio.com/packages/sparklyr/latest/reference/spark_apply.html) to give us a row count for each partition.\n",
    "\n",
    "Don't worry too much about understanding this line of code, it's the result which is important here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions:\t\t 2\n",
      "Number of rows per partition:\t [2500, 2500]\n"
     ]
    }
   ],
   "source": [
    "print('Number of partitions:\\t\\t', rand_df.rdd.getNumPartitions())\n",
    "\n",
    "rows_in_part = rand_df.rdd.mapPartitions(lambda x: [sum(1 for _ in x)]).collect()\n",
    "print('Number of rows per partition:\\t', rows_in_part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "print(paste0('Number of partitions: ', rand_df %>% sparklyr::sdf_num_partitions()))\n",
    "\n",
    "rows_in_part <- sparklyr::spark_apply(rand_df, function(x) nrow(x)) %>% sparklyr::collect()\n",
    "print(paste0('Number of rows per partition: ', rows_in_part)) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the size of the partitions is the same. We will see later, if there was skew in partition sizes we would always have to wait for the largest partition to finish processing before moving on to the next task, this is commonly referred to as a bottleneck. So Spark understandably puts a similar number of rows in each partition.\n",
    "\n",
    "The number of partitions was set by default. The property that controls this number is `spark.default.parallelism`, and to modify the default we must override it in the Spark session. See the [Guidance on Spark Sessions](../spark-overview/spark-session-guidance) for more information on how to do this. To see what the default is we can look at [the Spark documentation](https://spark.apache.org/docs/latest/configuration.html#execution-behavior) (if you follow the link use Ctrl+F to search for the property name).\n",
    "\n",
    ">For operations like parallelize with no parent RDDs, it depends on the cluster manager:\n",
    ">- Local mode: number of cores on the local machine\n",
    ">- Mesos fine grained mode: 8\n",
    ">- Others: total number of cores on all executor nodes or 2, whichever is larger\n",
    "\n",
    "We're running Spark in local mode with 2 cores, hence without overriding this property it will default to 2.\n",
    "\n",
    "Modifying the `spark.default.parallelism` property will set the number of partitions for all newly created DataFrames in this session. But what if we want to create a DataFrame that is unusually large or small for our session? For this case we can specify how many partitions we want when creating the DataFrame, using an extra argument `numPartitions`/`repartition` for PySpark/sparklyr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n",
      "| id|rand_value|\n",
      "+---+----------+\n",
      "|  0|         7|\n",
      "|  1|         9|\n",
      "|  2|        10|\n",
      "|  3|         9|\n",
      "|  4|         5|\n",
      "|  5|         6|\n",
      "|  6|         1|\n",
      "|  7|         2|\n",
      "|  8|         4|\n",
      "|  9|         7|\n",
      "+---+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_rand = spark.range(0, 10, numPartitions=1) #just 10 rows, so we'll put them into one partition\n",
    "small_rand = small_rand.withColumn(\"rand_value\",  F.ceil(F.rand(seed=seed_no)*10))\n",
    "\n",
    "small_rand.show()\n",
    "small_rand.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "small_rand <- sparklyr::sdf_seq(sc, 0, 9, repartition=1) #just 10 rows, so we'll put them into one partition\n",
    "small_rand <- small_rand %>% \n",
    "    sparklyr::mutate(rand_val = ceiling(rand(seed_no)*10))\n",
    "    \n",
    "small_rand %>% sparklyr::collect()\n",
    "small_rand %>% sparklyr::sdf_num_partitions()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imported DataFrame\n",
    "\n",
    "Next, let's import the `animal_rescue.csv` file and see how many partitions we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../../../config.yaml\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "    \n",
    "rescue_path = config[\"rescue_path_csv\"]\n",
    "\n",
    "rescue = spark.read.csv(rescue_path, header=True, inferSchema=True)\n",
    "\n",
    "rescue.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "config <- yaml::yaml.load_file(\"ons-spark/config.yaml\")\n",
    "\n",
    "rescue <- sparklyr::spark_read_csv(sc, config$rescue_path_csv, header=TRUE, inferSchema=TRUE) \n",
    "\n",
    "rescue %>% sparklyr::sdf_num_partitions()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we didn't set this number anywhere, and it obviously wasn't set by `spark.default.parallelism` either. \n",
    "\n",
    "How many rows and columns do each DataFrame have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in rescue DataFrame:\t\t 5898\n",
      "Number of columns in rescue DataFrame:\t\t 26\n",
      "Total number of cells in rescue DataFrame:\t 153348\n",
      "\n",
      "Number of rows in random_df DataFrame:\t\t 5000\n",
      "Number of columns in random_df DataFrame:\t 2\n",
      "Total number of cells in random_df DataFrame:\t 10000\n"
     ]
    }
   ],
   "source": [
    "rescue_rows = rescue.count()\n",
    "rescue_columns = len(rescue.columns)\n",
    "\n",
    "rand_df_columns = len(rand_df.columns)\n",
    "\n",
    "print('Number of rows in rescue DataFrame:\\t\\t', rescue_rows)\n",
    "print('Number of columns in rescue DataFrame:\\t\\t', rescue_columns)\n",
    "print('Total number of cells in rescue DataFrame:\\t', rescue_rows*rescue_columns)\n",
    "print('\\nNumber of rows in random_df DataFrame:\\t\\t', row_ct)\n",
    "print('Number of columns in random_df DataFrame:\\t', rand_df_columns)\n",
    "print('Total number of cells in random_df DataFrame:\\t', len(rand_df.columns)*row_ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "rescue_rows <- rescue %>% sparklyr::sdf_nrow()\n",
    "rescue_columns <- rescue %>% sparklyr::sdf_ncol()\n",
    "    \n",
    "rand_df_columns <- rand_df %>% sparklyr::sdf_ncol()\n",
    " \n",
    "print(paste0('Number of rows in rescue DataFrame: ', rescue_rows))\n",
    "print(paste0('Number of columns in rescue DataFrame:', rescue_columns))\n",
    "print(paste0('Total number of cells in rescue DataFrame:', rescue_rows*rescue_columns))\n",
    "print(paste0('Number of rows in random_df DataFrame:', row_ct))\n",
    "print(paste0('Number of columns in random_df DataFrame:', rand_df_columns))\n",
    "print(paste0('Total number of cells in random_df DataFrame:', rand_df_columns*row_ct))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are more cells in the rescue DataFrame, but Spark put it into just one partition.\n",
    "\n",
    "The reason for this is that the original csv file is stored as a single file on disk, which means that Spark will read it in as one partition. Data on HDFS can be stored in multiple files. In general, when a Spark DataFrame with $P$ partitions is written onto disk, it will be stored in $P$ files, and whenever we read that dataset into a Spark session it will again have $P$ partitions. We will revisit writing partitions to disk later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processed DataFrame\n",
    "\n",
    "Let's see how applying *narrow* and *wide* transformations to the DataFrame affects the number of partitions. Firstly we'll filter the data, which is a *narrow* operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------------------------------+\n",
      "|AnimalGroupParent|FinalDescription                            |\n",
      "+-----------------+--------------------------------------------+\n",
      "|Cat              |TO ASSIST RSPCA WITH CAT TRAPPED UP TREE,B15|\n",
      "|Cat              |ASSIST RSPCA WITH CAT STUCK UP TREE, B15    |\n",
      "|Cat              |CAT STUCK UP TREE,B15                       |\n",
      "+-----------------+--------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "Number of partitions in filtered_rescue DataFrame:\t 1\n"
     ]
    }
   ],
   "source": [
    "filtered_rescue = rescue.filter(F.col('AnimalGroupParent') == 'Cat')\n",
    "filtered_rescue.select('AnimalGroupParent', 'FinalDescription').show(3, truncate=False)\n",
    "\n",
    "print('Number of partitions in filtered_rescue DataFrame:\\t',filtered_rescue.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "filtered_rescue <- rescue %>% sparklyr::filter(AnimalGroupParent == \"Cat\")\n",
    "filtered_rescue %>% sparklyr::select(AnimalGroupParent, FinalDescription) %>% \n",
    "    head(3) %>% \n",
    "    sparklyr::collect()\n",
    "\n",
    "print(paste0('Number of partitions in filtered_rescue DataFrame: ',filtered_rescue %>% sparklyr::sdf_num_partitions()))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No change. In general, for a narrow transformations the contents of the partitions will change according to the transformation applied, but no data will move from one partition to another and the total number of partitions will remain the same.\n",
    "\n",
    "Now let's group the data, which is a wide operation, by `PostcodeDistrict` and then aggregate to find a count of incidents by area. How many partitions will we have in the new DataFrame?\n",
    "\n",
    "*Note PySpark and sparklyr will give different results here; we will explain why later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+\n",
      "|PostcodeDistrict|Count|\n",
      "+----------------+-----+\n",
      "|            SE17|   20|\n",
      "|            EC1Y|    1|\n",
      "|             CR0|   98|\n",
      "|             EN3|   45|\n",
      "|            SW14|   12|\n",
      "+----------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "Number of partitions in rescue_by_area DataFrame:\t 200\n"
     ]
    }
   ],
   "source": [
    "count_by_area = (rescue.groupBy('PostcodeDistrict')\n",
    "    .agg(F.count('IncidentNumber').alias('Count')))\n",
    "\n",
    "count_by_area.show(5)\n",
    "print('Number of partitions in rescue_by_area DataFrame:\\t', count_by_area.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "count_by_area <- rescue %>% dplyr::group_by(PostcodeDistrict) %>%\n",
    "    dplyr::summarise(count=n())\n",
    "    \n",
    "count_by_area %>% head(5) %>% sparklyr::collect()\n",
    "print(paste0('Number of partitions in count_by_area DataFrame:', count_by_area %>% sparklyr::sdf_num_partitions()))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How many?!?**  \n",
    "\n",
    "\n",
    "<font size=\"6\"> 😵 </font> \n",
    "\n",
    "\n",
    "Let's check how many rows are in each partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows per partition:\t [1, 1, 1, 0, 0, 0, 1, 1, 2, 1, 2, 1, 0, 1, 1, 0, 1, 2, 2, 4, 1, 2, 1, 0, 2, 3, 2, 3, 1, 2, 0, 1, 1, 3, 0, 0, 2, 3, 0, 1, 1, 2, 1, 1, 3, 0, 2, 2, 1, 2, 1, 2, 0, 3, 5, 4, 1, 1, 1, 2, 0, 2, 2, 1, 0, 1, 1, 0, 0, 0, 1, 1, 4, 3, 1, 2, 3, 3, 1, 4, 0, 2, 2, 5, 1, 1, 0, 0, 3, 1, 3, 2, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 2, 3, 3, 0, 2, 0, 1, 2, 2, 0, 1, 6, 0, 2, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 2, 2, 1, 3, 1, 3, 1, 3, 0, 0, 1, 4, 1, 4, 1, 0, 0, 0, 1, 1, 3, 1, 2, 3, 1, 0, 1, 0, 2, 1, 1, 2, 1, 0, 2, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 2, 4, 2, 0, 1, 0, 0, 1, 2, 3, 2, 1, 0, 1, 0, 3, 2, 0, 2, 1, 0, 4, 0, 2, 0, 0, 0, 3]\n"
     ]
    }
   ],
   "source": [
    "rows_in_part = count_by_area.rdd.mapPartitions(lambda x: [sum(1 for _ in x)]).collect()\n",
    "print('Number of rows per partition:\\t', rows_in_part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "# Note that this code takes a long time to run in sparklyr\n",
    "rows_in_part <- sparklyr::spark_apply(count_by_area, function(x) nrow(x)) %>% sparklyr::collect()\n",
    "print(paste0('Number of rows per partition: ', rows_in_part)) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quite a few empty partitions- this is a clear case of overpartitioning. In the [Spark Application and UI](../spark-concepts/spark-application-and-ui) article it was shown that overpartitioning leads to slower processing as a larger proportion of time is spent on scheduling tasks and (de)serialising data instead of processing the task.\n",
    "\n",
    "Again, there is a property we can modify in the Spark session configuration to change this behaviour. The property to override is `spark.sql.shuffle.partitions`. *Shuffle partitions* means when a shuffle occurs, for example a *wide* operation. So this property says \"give the resulting DataFrame this many partitions\". See the article on [Shuffling](../spark-concepts/shuffling) for more information on shuffles. \n",
    "\n",
    "The [Spark documentation](https://spark.apache.org/docs/latest/configuration.html#runtime-sql-configuration) shows that the default value for `spark.sql.shuffle.partitions` is 200. This default is obviously too high for our case of grouping a small dataset with a small number of groups i.e. `PostcodeDistrict`. This is one of the more useful properties to consider changing depending on the size of the data you want to process with Spark.\n",
    "\n",
    "We noted above that sparklyr would give different results. Using a local sparklyr session there is another property that is used for partitioning after a wide operation, `spark.sql.shuffle.partitions.local`, which is set to 16 by default.\n",
    "\n",
    "### How to change partitioning\n",
    "\n",
    "Before we look at how to modify DataFrame partitioning, let's revisit the motivation for wanting to do this.\n",
    "\n",
    "#### Motivation\n",
    "\n",
    "1. We can change the number of partitions of a DataFrame to achieve less or more parallelisation. More parallel processing is great for large DataFrames, but there are some scheduling and data serialisation overheads involved in processing multiple partitions. So for small DataFrames it's best to have just a small number of partition because the overheads involved in organising and writing multiple partitions will result in slower processing. However, we don't always get to choose the partitioning, e.g. to do a join Spark must first put rows with the same join keys on the same partition. More on this later.\n",
    "\n",
    "2. We can also partition by a specified column(s) in the DataFrame. This can be useful when writing to a Hive table or file becuase we can then read in single partitions. For example, say we had a DataFrame containing records from the past 20 years. We could partition this DataFrame by year when writing it to disk, then in future we could read in one year at a time or multiple years to speed up future tasks. \n",
    "\n",
    "\n",
    "#### Change partitioning\n",
    "\n",
    "Here are two ways of changing the number of partitions of a DataFrame. One is [`.repartition()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.repartition.html)/[`sdf_repartition()`](https://spark.rstudio.com/packages/sparklyr/latest/reference/sdf_repartition.html) and the other is `coalesce()`/[`sdf_coalesce()`](https://spark.rstudio.com/packages/sparklyr/latest/reference/sdf_coalesce.html).\n",
    "\n",
    "Let's see how to use these functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "rand_df %>% sparklyr::sdf_num_partitions()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_df = rand_df.coalesce(1)\n",
    "rand_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "rand_df <- rand_df %>% sparklyr::sdf_coalesce(1)\n",
    "rand_df %>% sparklyr::sdf_num_partitions()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_df = rand_df.coalesce(10)\n",
    "rand_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "rand_df <- rand_df %>% sparklyr::sdf_coalesce(10)\n",
    "rand_df %>% sparklyr::sdf_num_partitions()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is that the answer we were expecting? Note that [`.coalesce()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.coalesce.html) should only be used to *decrease* the number of partitions. When increasing it can only increase to a previous state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_df = rand_df.repartition(1)\n",
    "rand_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "rand_df <- rand_df %>% sparklyr::sdf_repartition(1)\n",
    "rand_df %>% sparklyr::sdf_num_partitions()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_df = rand_df.repartition(10)\n",
    "rand_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "rand_df <- rand_df %>% sparklyr::sdf_repartition(10)\n",
    "rand_df %>% sparklyr::sdf_num_partitions()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repartition can be used to increase parallelisation.\n",
    "\n",
    "The other important difference is that `.repartition()` incurs a *full* shuffle of the DataFrame, meaning it rewrites all the data into the new partitions. Shuffling takes time, especially for large amounts of data, so it's best to avoid shuffling more data than needed. To learn more about shuffling have a look at the [Shuffling](../spark-concepts/shuffling) article.\n",
    "\n",
    "On the other hand `.coalesce()` involves moving just some of the data. Let's demonstrate by an example.\n",
    "\n",
    "Say we have a simple DataFrame where the partitions contain the following numbers:\n",
    "    \n",
    "    Partition 1: 1, 2, 3\n",
    "    Partition 2: 4, 5, 6\n",
    "    Partition 3: 7, 8, 9\n",
    "    Partition 4: 10, 11, 12\n",
    "\n",
    "We then decide we want two partitions as opposed to four so we apply `.coalesce(2)` to the DataFrame. The partitions would now contain the following:\n",
    "\n",
    "    Partition 1: 1, 2, 3 + (7, 8, 9)\n",
    "    Partition 2: 4, 5, 6 + (10, 11, 12)\n",
    "\n",
    "So the data on Partitions 1 and 2 have not been shuffled. We have just moved the data on Partitions 3 to Partition 1 and Partition 4 to Partition 2, this is sometimes referred to as a partial shuffle. The gains from this simple example would be negligible, but scale this process up to millions of rows and then it becomes significant.\n",
    "\n",
    "It's also possible to use a column name in `.repartition()` or even a number and column name, see documentation for more information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partition sizes\n",
    "A key concept in both Hadoop and Spark is that files and partitions should optimally be [around 128MB](https://spark.apache.org/docs/latest/sql-performance-tuning.html#other-configuration-options). It is however common that data are stored as many small files rather than fewer large ones. This issue can be solved by reducing the number of partitions before writing to HDFS, using [`.coalesce()`](https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.coalesce) or [`.repartition()`](https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.repartition) in PySpark and [`sdf_coalesce()`](https://spark.rstudio.com/reference/sdf_coalesce.html) or [`sdf_repartition()`](https://spark.rstudio.com/reference/sdf_repartition.html) in sparklyr.\n",
    "\n",
    "Below are two reasons why it's a good idea to avoid writing out many small files.\n",
    "\n",
    "#### Avoiding the small files issue on HDFS\n",
    "HDFS consists of many DataNodes, which store the files, and a single NameNode, which essentially works as an index for the file system. The small files issue occurs as every file path is loaded into the NameNode which leads to an inefficient use of the memory capacity, e.g. if a 600MB file is stored as 200 files of 3MB then the NameNode needs to load 200 file paths, instead of the 5 file paths it would have to load if stored as 5 files of 120MB each.\n",
    "\n",
    "#### More efficient processing with Spark\n",
    "Spark will by default use one partition per file on disk when reading in the data. If there are many small files on disk then these will be converted to many small partitions, which leads to slower processing as proportionally more time is spent scheduling tasks and serialising data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intermediate partitions in wide operations\n",
    "\n",
    "Now onto a more complex topic of intermediate partitioining.\n",
    "\n",
    "We saw earlier that for narrow transformations the number of partitions of input and output DataFrames is the same. We also saw that for wide transformations the number of partitions of the output DataFrame is changed to the value of the `spark.sql.shuffle.partitions` property set in the Spark session. \n",
    "\n",
    "But what is inside the partitions of the output DataFrame after a wide operation? This depends on the operation, so in this section we will look at three wide transformations: join, group by and [window functions](../spark-functions/window-functions). Understanding how these functions work is particularly useful when we deal with skewed data, or more specifically skew in the join key, group variable or windows. We will demonstrate this with some skewed data and show that knowing your data can help you make informed decisions on scaling jobs vertically or horizontally, or employing an alternative strategy like a [salted join](../spark-concepts/salted-joins).\n",
    "\n",
    "#### Set up the problem\n",
    "\n",
    "Let's start with a DataFrame with a skewed variable `skew_col`, but uniform partition sizes. It also has a column `rand_val` containing some random numbers to perform some calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------+\n",
      "| id|skew_col|rand_val|\n",
      "+---+--------+--------+\n",
      "|  0|       A|       7|\n",
      "|  1|       A|       9|\n",
      "|  2|       A|       9|\n",
      "|  3|       A|       9|\n",
      "|  4|       A|       4|\n",
      "+---+--------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "row_ct = 10**7\n",
    "seed_no = 42\n",
    "\n",
    "skewed_df = spark.range(row_ct, numPartitions=2)\n",
    "\n",
    "skewed_df = (\n",
    "    skewed_df.withColumn(\"skew_col\", F.when(F.col(\"id\") < 100, \"A\")\n",
    "                               .when(F.col(\"id\") < 1000, \"B\")\n",
    "                               .when(F.col(\"id\") < 10000, \"C\")\n",
    "                               .when(F.col(\"id\") < 100000, \"D\")\n",
    "                               .otherwise(\"E\"))\n",
    "              .withColumn(\"rand_val\", F.rint(F.rand(seed_no)*10).cast(\"int\"))\n",
    ")\n",
    "\n",
    "skewed_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "row_ct <- 10**7\n",
    "seed_no <- 42L\n",
    "\n",
    "skewed_df <- sparklyr::sdf_seq(sc, 0, row_ct - 1, repartition=2) %>%\n",
    "    sparklyr::mutate(\n",
    "        skew_col = dplyr::case_when(\n",
    "            id < 100 ~ \"A\",\n",
    "            id < 1000 ~ \"B\",\n",
    "            id < 10000 ~ \"C\",\n",
    "            id < 100000 ~ \"D\",\n",
    "            TRUE ~ \"E\"),\n",
    "        rand_val = ceiling(rand(seed_no)*10))\n",
    "\n",
    "skewed_df %>% head(5) %>% sparklyr::collect()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To confirm the details of the partitioning we will add a column with the partition ID, then group that column and count how many rows are in each partition. This method of counting how many rows are in each partition is very useful and much quicker than the method shown earlier, but it will not show any empty partitions like the previous method. Therefore, we will also show the total number of partitions.\n",
    "\n",
    "We'll put this into a function so we can run it again on other DataFrames later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def print_partitioning_info(sdf):\n",
    "    sdf.withColumn(\"part_id\", F.spark_partition_id()).groupBy(\"part_id\").count().show()\n",
    "    print(f\"Number of partitions: {sdf.rdd.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "print_partitioning_info <- function(sdf) {\n",
    "    sdf %>% \n",
    "        sparklyr::mutate(\n",
    "            part_id = spark_partition_id()) %>%\n",
    "        dplyr::group_by(part_id) %>%\n",
    "        dplyr::summarise(count=n()) %>%\n",
    "        sparklyr::collect() %>%\n",
    "        print()\n",
    "\n",
    "    print(paste0(\"Number of partitions: \", sparklyr::sdf_num_partitions(sdf)))\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|part_id|  count|\n",
      "+-------+-------+\n",
      "|      1|5000000|\n",
      "|      0|5000000|\n",
      "+-------+-------+\n",
      "\n",
      "Number of partitions: 2\n"
     ]
    }
   ],
   "source": [
    "print_partitioning_info(skewed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "print_partitioning_info(skewed_df)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform the join we will also need a second DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "small_df = spark.createDataFrame([\n",
    "    [\"A\", 1],\n",
    "    [\"B\", 2],\n",
    "    [\"C\", 3],\n",
    "    [\"D\", 4],\n",
    "    [\"E\", 5]\n",
    "], [\"skew_col\", \"number_col\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "small_df <- sparklyr::sdf_copy_to(sc, data.frame(\n",
    "    \"skew_col\" = LETTERS[1:5], \n",
    "    \"number_col\" = 1:5)) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how Spark executes each plan in full we will write the output DataFrames to disk to initiate the Spark job, then delete the file afterwards. Of course, the information on the Spark job will remain in the UI for us to inspect after deleting the file. In Spark 3 there is a nicer way of doing this by using the `noop` argument, meaning *no operation*. \n",
    "\n",
    "We will use the `checkpoint_path` from the config.yml file to write the data before deleting it. See the [Checkpoint and Staging Tables](../raw-notebooks/checkpoint-staging/checkpoint-staging) article for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def write_delete(sdf):\n",
    "    path = config[\"checkpoint_path\"] + \"/temp.parquet\"\n",
    "    sdf.write.mode(\"overwrite\").parquet(path)\n",
    "    cmd = f'rm -r -skipTrash {path}'\n",
    "    p = subprocess.run(cmd, shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "write_delete <- function(sdf) {\n",
    "    path <- paste0(config$checkpoint_path, \"/temp.parquet\")\n",
    "    sdf %>% sparklyr::spark_write_parquet(path=path, mode=\"overwrite\")\n",
    "    cmd <- paste0(\"hdfs dfs -rm -r -skipTrash \", path)\n",
    "    system(cmd)\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need a link to the Spark UI to view the details of how Spark partitions the data. When using a local session we can access the Spark UI with this URL, http://localhost:4040/jobs/.\n",
    "\n",
    "#### Run the jobs and investigate UI\n",
    "\n",
    "Next we will carry out the wide transformations on the `skewed_df` and apply the above function to create jobs. We have also added custom job descriptions to make it easier to find the relevant stage details in the UI.\n",
    "\n",
    "Note that the join key, group variable and windows are all set on the `skew_col` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "spark.sparkContext.setJobDescription(\"join\")\n",
    "joined_df = skewed_df.join(small_df, on=\"skew_col\", how=\"left\")\n",
    "write_delete(joined_df)\n",
    "\n",
    "spark.sparkContext.setJobDescription(\"groupby\")\n",
    "grouped_df = skewed_df.groupBy(\"skew_col\").agg(F.sum(\"rand_val\").alias(\"sum_rand\"))\n",
    "write_delete(grouped_df)\n",
    "\n",
    "spark.sparkContext.setJobDescription(\"window\")\n",
    "window_df = skewed_df.withColumn(\"skew_window_sum\", F.sum(\"rand_val\").over(Window.partitionBy(\"skew_col\")))\n",
    "write_delete(window_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "sc %>%\n",
    "    sparklyr::spark_context() %>%\n",
    "    sparklyr::invoke(\"setJobDescription\", \"join\")\n",
    "\n",
    "joined_df <- skewed_df %>% sparklyr::left_join(small_df, by=\"skew_col\")\n",
    "write_delete(joined_df)\n",
    "\n",
    "sc %>%\n",
    "    sparklyr::spark_context() %>%\n",
    "    sparklyr::invoke(\"setJobDescription\", \"groupby\")\n",
    "    \n",
    "grouped_df <- skewed_df %>% \n",
    "    dplyr::group_by(skew_col) %>% \n",
    "    dplyr::summarise(sum_rand = sum(rand_val))\n",
    "\n",
    "write_delete(grouped_df)\n",
    "\n",
    "sc %>%\n",
    "    sparklyr::spark_context() %>%\n",
    "    sparklyr::invoke(\"setJobDescription\", \"window\")\n",
    "\n",
    "window_df <- skewed_df %>%\n",
    "    dplyr::group_by(skew_col) %>%\n",
    "    dplyr::mutate(skew_window_sum = sum(rand_val)) %>%\n",
    "    dplyr::ungroup()\n",
    "\n",
    "write_delete(window_df)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are images of the task timeline for the above jobs containing the wide transformations. Note that the images might look slightly different if you are running the source notebook. The processing times will also vary.\n",
    "\n",
    "The main message in this set of images is that we see a clear bottleneck in the case of the join and window function, but there is no bottleneck in the group by.\n",
    "\n",
    "```{figure} ../images/partition_skew_join.PNG\n",
    "---\n",
    "width: 100%\n",
    "name: JoinTimeline\n",
    "alt: Task timeline for skewed join showing unevenly sized tasks\n",
    "---\n",
    "Task timeline for skewed join\n",
    "```\n",
    "\n",
    "```{figure} ../images/partition_skew_groupby.PNG\n",
    "---\n",
    "width: 100%\n",
    "name: GroupbyTimeline\n",
    "alt: Task timeline for skewed group by showing evenly sized tasks\n",
    "---\n",
    "Task timeline for skewed group by\n",
    "```\n",
    "\n",
    "```{figure} ../images/partition_skew_window.PNG\n",
    "---\n",
    "width: 100%\n",
    "name: WindowTimeline\n",
    "alt: Task timeline for skewed windows showing unevenly sized tasks\n",
    "---\n",
    "Task timeline for skewed windows\n",
    "```\n",
    "\n",
    "It's also useful to look at the Tasks table below the timeline to see how many records were processes in each of the 200 tasks. In the images below we have sorted the table by the *Output Size / Records* column (circled red) so that the largest tasks are at the top.\n",
    "\n",
    "```{figure} ../images/partition_skew_join_table.PNG\n",
    "---\n",
    "width: 100%\n",
    "name: JoinDetails\n",
    "alt: Task details for skewed join showing number of records in uneven partitions\n",
    "---\n",
    "Task details for skewed join\n",
    "```\n",
    "\n",
    "```{figure} ../images/partition_skew_groupby_table.PNG\n",
    "---\n",
    "width: 100%\n",
    "name: GroupbyDetails\n",
    "alt: Task details for skewed group by showing number of records in even partitions\n",
    "---\n",
    "Task details for skewed group by\n",
    "```\n",
    "\n",
    "```{figure} ../images/partition_skew_window_table.PNG\n",
    "---\n",
    "width: 100%\n",
    "name: WindowDetails\n",
    "alt: Task details for skewed windows\n",
    "---\n",
    "Task details for skewed windows\n",
    "```\n",
    "\n",
    "One last piece of information we will gather is the partitioning information of the output DataFrames using the `print_partitioning_info()` function we defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|part_id|  count|\n",
      "+-------+-------+\n",
      "|     78|  90000|\n",
      "|     43|9900000|\n",
      "|     49|    900|\n",
      "|    106|    100|\n",
      "|     89|   9000|\n",
      "+-------+-------+\n",
      "\n",
      "Number of partitions: 200\n"
     ]
    }
   ],
   "source": [
    "print_partitioning_info(joined_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "print_partitioning_info(joined_df)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|part_id|count|\n",
      "+-------+-----+\n",
      "|     78|    1|\n",
      "|     43|    1|\n",
      "|     49|    1|\n",
      "|    106|    1|\n",
      "|     89|    1|\n",
      "+-------+-----+\n",
      "\n",
      "Number of partitions: 200\n"
     ]
    }
   ],
   "source": [
    "print_partitioning_info(grouped_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "print_partitioning_info(grouped_df)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|part_id|  count|\n",
      "+-------+-------+\n",
      "|      1|5000000|\n",
      "|      0|5000000|\n",
      "+-------+-------+\n",
      "\n",
      "Number of partitions: 200\n"
     ]
    }
   ],
   "source": [
    "print_partitioning_info(window_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "print_partitioning_info(window_df)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion\n",
    "\n",
    "Now that we have all the information on how Spark performed these processes we can compare the three cases.\n",
    "\n",
    "All three operations involve an intermediate shuffle where the join key/grouped variable/windows are moved to the same partitions. This is called `hashpartitioning()` in the execution plan. As we mentioned previously, these are all wide transformations so the number of partitions of the output DataFrames are the same, 200. Listed below are the differences in processing and output DataFrames for the three operations.\n",
    "\n",
    "**Join**\n",
    "\n",
    "There was much more work to be done on the larger partition than the smaller ones. The Spark UI shows there were more records to process on the large partition therefore the task takes much longer. Note the number of records processed in each task matches the number of rows in each join key. The output DataFrame is partitioned by the join key. \n",
    "\n",
    "**Group by**\n",
    "\n",
    "In the case of the group by the skewed variable isn't too much of an issue because the aggregation is quite simple and is optimised, therefore it is easy to process each partition whether there is skew or not. Again, the output DataFrame is partitioned by the grouped variable, this time there is one row on each partition.\n",
    "\n",
    "**Window**\n",
    "\n",
    "Like the join, there is more work to do on the larger partitions. However, unlike the join, the rows of the output DataFrame have the same partition IDs as the input DataFrame.\n",
    "\n",
    "#### What does this mean in practice?\n",
    "\n",
    "Highly skewed data is a common issue that causes slow processing with Spark. Above we have seen that skewed data causes skewed partitions when Spark processes wide transformations and can result in bottlenecks, where some tasks take much longer than others therefore not utilising the full potential of parallel processing. An even worse situation is where the skew causes spill, where the large partition cannot fit into its allocated memory on an executor and overflows temporarily onto disk. We cannot recreate the issue of spill in a local session but they are easy to spot in the Stage details page in the Spark UI.\n",
    "\n",
    "This is where knowing your data can be useful. To help with the explanation we will refer to the join keys/groups/windows as groups. If a join/group by/window function is causing you issues, try to work out how many groups there are and the sizes of these groups. If there are lots on smaller groups it might help to increase `spark.sql.shuffle.partitions` and aim for greater parallel power with lots of smaller executors. If there are fewer groups you might want to scale more vertically so decrease `spark.sql.shuffle.partitions` and aim for a smaller number of bulkier executors.\n",
    "\n",
    "If you are doing a join on a highly skewed DataFrame you might want to try a [salted join](../spark-concepts/salted-joins). If you are dealing with skew in a window function you can apply a group by and join to achieve the same result. However, these are alternative solutions when dealing with highly skewed data and in most cases a regular join or window function are more efficient and makes the code more readable. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How should I partition my data?\n",
    "\n",
    "The short answer is- don't worry about it too much!\n",
    "\n",
    "Let's flip the question around. What are the consequences of getting the number of partitions \"wrong\"? Most of the time- nothing. You could spend hours trying to optimise the number of partitions and perhaps speed up the processing time to be twice as quick, but sometimes there are simpler things you can do that might speed up the processing by ten times or a hundred times the processing speed. \n",
    "\n",
    "As the computer scientist Donald Knuth once wrote: \n",
    "\n",
    ">Premature optimization is the root of all evil (or at least most of it) in programming\n",
    ">\n",
    ">    -> *Computer Programming as an Art (1974)*\n",
    "\n",
    "More importantly it's good practice to be aware of the size of the DataFrame and the number of partitions as this will help to avoid obvious issues or over-partitioning, under-partitioning and highly skewed processes.\n",
    "\n",
    "A more accurate answer depends on a variety of factors including: the size of the data, data types, distributions within the data, type of processing and other properties defined in the [`SparkSession`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.SparkSession.html). Here are some tips for quick wins:\n",
    "\n",
    "- As suggested above, the first step is getting the code right. Only look to optimise if you're running into performance issues.\n",
    "- Decreasing the `spark.sql.shuffle.partitions` parameter is sensible for smaller datasets.\n",
    "- If you search for an answer on the web, you might find the optimum size for a Spark partitions is 100-200MB. In practice, this is impossible to keep track of in an analysis script, but it gives you an idea of what is considered big or small.\n",
    "- It makes sense for the number of partitions to be some multiple of the number of cores in the Spark session. This will help to avoid redundant cores.\n",
    "\n",
    "Remember, it's only worth experimenting on the *optimum* number if you have a real performance issue. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitions when Unioning or Binding DataFrames\n",
    "\n",
    "You can append two DataFrames in PySpark that have the same schema with the [`.union()`](https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.union) operation. In sparklyr, the equivalent operation is [`sdf_bind_rows()`](https://spark.rstudio.com/reference/sdf_bind.html) and R users will often refer to appending two DataFrames as *binding*.\n",
    "\n",
    "The `.union()` function in PySpark and the `sdf_bind_rows()` function in sparklyr are both equivalent to `UNION ALL` in SQL. A regular `UNION` operation in SQL will remove duplicates, whereas `.union()` in PySpark and `sdf_bind_rows()` in sparklyr will not.\n",
    "\n",
    "Unlike joining two DataFrames, `.union()`/`sdf_bind_rows()`  does not involve a full shuffle, as the data does not move between partitions. Instead, the number of partitions in the unioned DataFrame is equal to the sum of the number of partitions in the two source DataFrames, i.e. if you union a DataFrame consisting of 100 partitions and one consisting of 50 partitions, your unioned DataFrame will have 150 partitions.\n",
    "\n",
    "To avoid excessive number of partitions, you can use [`.coalesce()`](https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.coalesce) or [`.repartition()`](https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.repartition) to reduce the number of partitions (use [`sdf_coalesce()`](https://spark.rstudio.com/reference/sdf_coalesce.html) or [`sdf_repartition()`](https://spark.rstudio.com/reference/sdf_repartition.html)\n",
    " in sparklyr). The DataFrame will also get repartitioned when a *wide transformation* is applied to the DataFrame (also called a shuffle), e.g. with a `.groupBy()` or `.orderBy()`.\n",
    " \n",
    "It is also worth being aware that storing data on HDFS as many small files is inefficient, both in terms of how the data is stored and in reading it in. Unioning many DataFrames and then writing straight to HDFS can be a cause of this issue. For more information on the small file issue, see the previous section on [Partition sizes](#partition-sizes). \n",
    "\n",
    "First we will stop the previous spark sessions and start a new Spark session which limits the number of partitions to 12, read the Animal Rescue data, group by animal and year, and then count. The grouping and aggregation will cause a shuffle, meaning that the DataFrame will have 12 partitions, as we set `spark.sql.shuffle.partitions` to `12` in the `SparkSession.builder`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>animal_group</th>\n",
       "      <th>cal_year</th>\n",
       "      <th>animal_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bird</td>\n",
       "      <td>2010</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hamster</td>\n",
       "      <td>2011</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Unknown - Domestic Animal Or Pet</td>\n",
       "      <td>2012</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Unknown - Heavy Livestock Animal</td>\n",
       "      <td>2012</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sheep</td>\n",
       "      <td>2012</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       animal_group  cal_year  animal_count\n",
       "0                              Bird      2010            99\n",
       "1                           Hamster      2011             3\n",
       "2  Unknown - Domestic Animal Or Pet      2012            18\n",
       "3  Unknown - Heavy Livestock Animal      2012             4\n",
       "4                             Sheep      2012             1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "\n",
    "# Stop previous spark session and create new spark session\n",
    "\n",
    "spark.stop()\n",
    "\n",
    "spark = (SparkSession.builder.master(\"local[2]\")\n",
    "         .appName(\"partitions\")\n",
    "         .config(\"spark.sql.shuffle.partitions\", 12)\n",
    "         .getOrCreate())\n",
    "\n",
    "\n",
    "rescue_path_csv = config[\"rescue_path_csv\"]\n",
    "\n",
    "# # Read in and shuffle data\n",
    "rescue = (spark.read.csv(rescue_path_csv, header=True, inferSchema=True)  \n",
    "          .withColumnRenamed(\"IncidentNumber\", \"incident_number\")\n",
    "          .withColumnRenamed(\"AnimalGroupParent\", \"animal_group\")\n",
    "          .withColumnRenamed(\"CalYear\", \"cal_year\")\n",
    "          .groupBy(\"animal_group\", \"cal_year\")\n",
    "          .agg(F.count(\"incident_number\").alias(\"animal_count\")))\n",
    "\n",
    "rescue.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "\n",
    "# Stop previous spark session and create new spark session limiting number of partitions\n",
    "spark_disconnect(sc)\n",
    "\n",
    "small_config <- sparklyr::spark_config()\n",
    "small_config$spark.sql.shuffle.partitions <- 12\n",
    "\n",
    "sc <- sparklyr::spark_connect(\n",
    "  master = \"local\",\n",
    "  app_name = \"partitions\",\n",
    "  config = small_config)\n",
    "\n",
    "sparklyr::spark_connection_is_open(sc)\n",
    "\n",
    "# Set the data path\n",
    "config <- yaml::yaml.load_file(\"ons-spark/config.yaml\")\n",
    "# Read in and shuffle data\n",
    "rescue <- sparklyr::spark_read_csv(sc, config$rescue_path_csv, header=TRUE, infer_schema=TRUE)\n",
    "\n",
    "rescue <- rescue %>%\n",
    "    dplyr::rename(\n",
    "        incident_number = IncidentNumber,\n",
    "        animal_group = AnimalGroupParent,\n",
    "        cal_year = CalYear) %>%\n",
    "        dplyr::group_by(animal_group,cal_year) %>%\n",
    "        dplyr::count(animal_count = count())\n",
    "\n",
    "rescue %>% head(5)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can confirm the number of partitions of the `rescue` DataFrame using `.rdd.getNumPartitions()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rescue partitions: 12\n"
     ]
    }
   ],
   "source": [
    "print('Rescue partitions:',rescue.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "print(paste0(\"Rescue partitions: \", sparklyr::sdf_num_partitions(rescue)))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create will create some smaller DataFrames, containing different animals, by filtering the rescue data. We will preview just the `dogs` data, as all others will be of a similar format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>animal_group</th>\n",
       "      <th>cal_year</th>\n",
       "      <th>animal_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dog</td>\n",
       "      <td>2011</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dog</td>\n",
       "      <td>2017</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dog</td>\n",
       "      <td>2009</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dog</td>\n",
       "      <td>2012</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dog</td>\n",
       "      <td>2014</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  animal_group  cal_year  animal_count\n",
       "0          Dog      2011           103\n",
       "1          Dog      2017            81\n",
       "2          Dog      2009           132\n",
       "3          Dog      2012           100\n",
       "4          Dog      2014            90"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dogs = rescue.filter(F.col(\"animal_group\") == \"Dog\")\n",
    "cats = rescue.filter(F.col(\"animal_group\") == \"Cat\")\n",
    "hamsters = rescue.filter(F.col(\"animal_group\") == \"Hamster\")\n",
    "\n",
    "dogs.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "dogs <- rescue %>% sparklyr::filter(animal_group == 'Dog')\n",
    "cats <- rescue %>% sparklyr::filter(animal_group == 'Cat')\n",
    "hamsters <- rescue %>% sparklyr::filter(animal_group == 'Hamster')\n",
    "\n",
    "dogs %>% head(5)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check that each of these DataFrames has 12 partitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Dog partitions: 12 \n",
      " Cat partitions: 12 \n",
      " Hamster partitions: 12\n"
     ]
    }
   ],
   "source": [
    "print(' Dog partitions:', dogs.rdd.getNumPartitions(),\n",
    "      '\\n Cat partitions:', cats.rdd.getNumPartitions(),\n",
    "      '\\n Hamster partitions:', hamsters.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "print(paste0(\"Dogs partitions: \", sparklyr::sdf_num_partitions(dogs)))\n",
    "print(paste0(\"Cats partitions: \", sparklyr::sdf_num_partitions(cats)))\n",
    "print(paste0(\"Hamsters partitions: \", sparklyr::sdf_num_partitions(hamsters)))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we apply the union function, the number of partitions in the unioned DataFrame will be the sum of partitions in each DataFrame. In this case we will now have 12 + 12 = 24 partitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dogs and Cats union partitions: 24\n"
     ]
    }
   ],
   "source": [
    "dogs_and_cats = dogs.union(cats)\n",
    "print('Dogs and Cats union partitions:',dogs_and_cats.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "dogs_and_cats = sparklyr::sdf_bind_rows(dogs,cats)\n",
    "print(paste0(\"Dogs and Cats union partitions: \", sparklyr::sdf_num_partitions(dogs_and_cats)))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unioning another DataFrame adds another 12 partitions to make 36 (24 + 12):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dogs, Cats and Hamsters union partitions: 36\n"
     ]
    }
   ],
   "source": [
    "dogs_cats_and_hamsters = dogs_and_cats.union(hamsters)\n",
    "print('Dogs, Cats and Hamsters union partitions:',dogs_cats_and_hamsters.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "dogs_cats_and_hamsters = sparklyr::sdf_bind_rows(dogs_and_cats,hamsters)\n",
    "print(paste0(\"Dogs, Cats and Hamsters union partitions: \", sdf_num_partitions(dogs_cats_and_hamsters)))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we only have 36 partitions here it is easy to see how this might get excessive with too many `.union()` statements. This can also become a bigger problem if the number of partitons is left to its default value of 200, where we would end up with 600 partitions! \n",
    "\n",
    "A subsequent shuffle (e.g. sorting the DataFrame) will reset the number of partitions to that specified in `spark.sql.shuffle.partitions`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dogs_cats_and_hamsters.orderBy(\"animal_group\", \"cal_year\").rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "sparklyr::sdf_num_partitions(dogs_cats_and_hamsters %>% sparklyr::sdf_sort(c('animal_group','cal_year')))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "You can also use `.repartition()` or `.coalesce()`. `.repartition()` involves a shuffle of the DataFrame and puts the data into roughly equal partition sizes, whereas `.coalesce()` combines partitions without a full shuffle, and so is more efficient, although at the potential cost of less equal partition sizes and therefore potential skew in the data. For more information see the previous section on [Partition sizes](#partition-sizes).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dogs_cats_and_hamsters.repartition(20).rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "sparklyr::sdf_num_partitions(dogs_cats_and_hamsters %>% sparklyr::sdf_repartition(20))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further information on managing partitions while writing data can now be found in the [Reading and Writing Data in Spark](../spark-overview/reading-and-writing-data-spark) page. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further resources\n",
    "\n",
    "Spark at the ONS Articles:\n",
    "- [Spark Application and UI](../spark-concepts/spark-application-and-ui)\n",
    "- [Shuffling](../spark-concepts/shuffling)\n",
    "- [Guidance on Spark Sessions](../spark-overview/spark-session-guidance)\n",
    "- [Window Functions in Spark](../spark-functions/window-functions)\n",
    "- [Salted Joins](../spark-concepts/salted-joins)\n",
    "- [Checkpoint and Staging Tables](../spark-concepts/checkpoint-staging)\n",
    "- [Reading and Writing Data in Spark](../spark-overview/reading-and-writing-data-spark)\n",
    "\n",
    "PySpark Documentation:\n",
    "- [`.coalesce()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.coalesce.html)\n",
    "- [`SparkSession`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.SparkSession.html)\n",
    "- [`.getNumPartitions()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.RDD.getNumPartitions.html)\n",
    "- [`.repartition()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.repartition.html)\n",
    "- [`.coalesce()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.coalesce.html)\n",
    "- [`.rdd.mapPartitions()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.RDD.mapPartitions.html)\n",
    "- [`DataFrameWriter.partitionBy()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.partitionBy.html)\n",
    "- [`.union()`](https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.union)\n",
    "\n",
    "Python Documentation:\n",
    "- [`subprocess`](https://docs.python.org/3/library/subprocess.html) \n",
    "\n",
    "sparklyr and tidyverse Documentation:\n",
    "- [`sdf_coalesce()`](https://spark.rstudio.com/packages/sparklyr/latest/reference/sdf_coalesce.html)\n",
    "- [`sdf_repartition()`](https://spark.rstudio.com/packages/sparklyr/latest/reference/sdf_repartition.html)\n",
    "- [`sdf_num_partitions()`](https://spark.rstudio.com/packages/sparklyr/latest/reference/sdf_num_partitions.html)\n",
    "- [`spark_apply()`](https://spark.rstudio.com/packages/sparklyr/latest/reference/spark_apply.html)\n",
    "- [`sdf_bind_rows()`](https://spark.rstudio.com/reference/sdf_bind.html)\n",
    "\n",
    "\n",
    "Spark Documentation:\n",
    "- [Spark Configuration](https://spark.apache.org/docs/latest/configuration.html):\n",
    "    - [Execution Behaviour](https://spark.apache.org/docs/latest/configuration.html#execution-behavior)\n",
    "    - [Runtime SQL Configuration](https://spark.apache.org/docs/latest/configuration.html#runtime-sql-configuration)\n",
    "- [`round`](https://spark.apache.org/docs/latest/api/sql/index.html#round)\n",
    "- [`bround`](https://spark.apache.org/docs/latest/api/sql/index.html#bround)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
