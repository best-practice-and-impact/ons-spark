{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Window Functions in Spark\n",
    "\n",
    "Window functions use values from other rows within the same group, or *window*, and return a value in a new column for every row. This can be in the form of aggregations (similar to a [`.groupBy()`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.groupBy.html)/[`group_by()`](https://dplyr.tidyverse.org/reference/group_by.html) but preserving the original DataFrame), ranking rows within groups, or returning values from previous rows. If you're familiar with SQL then a window function in PySpark works in the same way.\n",
    "\n",
    "This article explains how to use window functions in three ways: for aggregation, ranking, and referencing the previous row. An SQL example is also given.\n",
    "\n",
    "### Window Functions for Aggregations\n",
    "\n",
    "You can use a window function for aggregations. Rather than returning an aggregated DataFrame, the result of the aggregation will be placed in a new column.\n",
    "\n",
    "One example of where this is useful is for deriving a total to be used as the denominator for another calculation. For instance, in the Animal Rescue data we may want to work out what percentage of animals rescued each year are dogs. We can do this by getting the total of all animals by year, then dividing each animal group count by this. \n",
    "\n",
    "First, import the relevant packages and start a Spark session. To use window functions in PySpark, we need to import [`Window`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.Window.html) from `pyspark.sql.window`. No extra packages are needed for sparklyr, as Spark functions are referenced inside [`mutate()`](https://dplyr.tidyverse.org/reference/mutate.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.window import Window\n",
    "import yaml\n",
    "\n",
    "with open(\"../../../config.yaml\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "    \n",
    "spark = (SparkSession.builder.master(\"local[2]\")\n",
    "         .appName(\"window-functions\")\n",
    "         .getOrCreate())\n",
    "\n",
    "rescue_path = config[\"rescue_path\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "library(sparklyr)\n",
    "library(dplyr)\n",
    "\n",
    "sc <- sparklyr::spark_connect(\n",
    "    master = \"local[2]\",\n",
    "    app_name = \"window-functions\",\n",
    "    config = sparklyr::spark_config())\n",
    "\n",
    "config <- yaml::yaml.load_file(\"ons-spark/config.yaml\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "In this example we will use an aggregated version of the Animal Rescue data, containing `animal_group`, `cal_year` and `animal_count`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------+--------+------------+\n",
      "|animal_group                                    |cal_year|animal_count|\n",
      "+------------------------------------------------+--------+------------+\n",
      "|Snake                                           |2018    |1           |\n",
      "|Deer                                            |2015    |6           |\n",
      "|Unknown - Animal Rescue From Water - Farm Animal|2014    |1           |\n",
      "|Unknown - Domestic Animal Or Pet                |2017    |8           |\n",
      "|Bird                                            |2012    |112         |\n",
      "+------------------------------------------------+--------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rescue_agg = (\n",
    "    spark.read.parquet(rescue_path)\n",
    "    .withColumn(\"animal_group\", F.initcap(F.col(\"animal_group\")))\n",
    "    .groupBy(\"animal_group\", \"cal_year\")\n",
    "    .agg(F.count(\"animal_group\").alias(\"animal_count\")))\n",
    "\n",
    "rescue_agg.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "rescue_agg <- sparklyr::spark_read_parquet(sc, config$rescue_path) %>%\n",
    "    sparklyr::mutate(animal_group = initcap(animal_group)) %>%\n",
    "    dplyr::group_by(animal_group, cal_year) %>%\n",
    "    dplyr::summarise(animal_count = n()) %>%\n",
    "    dplyr::ungroup()\n",
    "    \n",
    "rescue_agg %>%\n",
    "    head(5) %>%\n",
    "    sparklyr::collect() %>%\n",
    "    print()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to calculate the percentage of animals rescued each year that are dogs. To do this, we first need to calculate the annual totals, and can then divide the number of dogs in each year by this.\n",
    "\n",
    "We could create a new DataFrame by grouping and aggregating and then joining back to the original DF; this would get the correct result, but a window function is much more efficient as it will reduce the number of shuffles required, as well as making the code more succinct and readable.\n",
    "\n",
    "The syntax is quite different between PySpark and sparklyr, although the principle is identical in each, and Spark will process them in the same way. The process for using a window function for aggregation in PySpark is as follows:\n",
    "- First, use [`.withColumn()`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.withColumn.html), as the result is stored in a new column in the DataFrame.\n",
    "- Then do the aggregation: [`F.sum(\"animal_count\")`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.sum.html).\n",
    "- Then perform this [over](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.Column.over.html) a window with `.over(Window.partitionBy(\"cal_year\"))`. Note that this uses [`.partitionBy()`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.Window.partitionBy.html) rather than `.groupBy()` (for some window functions you will also use [`.orderBy()`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.orderBy.html), but we do not need to here).\n",
    "\n",
    "In sparklyr:\n",
    "- Use `group_by(cal_year)` to partition the data.\n",
    "- Then define a new column, `annual_count`, as [`sum(animal_count)`](https://spark.apache.org/docs/latest/api/sql/index.html#sum)) inside `mutate()` (rather than [`summarise()`](https://dplyr.tidyverse.org/reference/summarise.html), which is used for regular aggregations).\n",
    "- Finally, [`ungroup()`](https://dplyr.tidyverse.org/reference/group_by.html) to remove the grouping from the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rescue_annual = (rescue_agg\n",
    "                 .withColumn(\"annual_count\",\n",
    "                     F.sum(\"animal_count\").over(Window.partitionBy(\"cal_year\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "rescue_annual <- rescue_agg %>%\n",
    "    dplyr::group_by(cal_year) %>%\n",
    "    sparklyr::mutate(annual_count = sum(animal_count)) %>%\n",
    "    dplyr::ungroup()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now display the DF, using `Cat`, `Dog` and `Hamster` between `2012` and `2014` as an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+------------+------------+\n",
      "|animal_group|cal_year|animal_count|annual_count|\n",
      "+------------+--------+------------+------------+\n",
      "|         Cat|    2012|         305|         603|\n",
      "|         Dog|    2012|         100|         603|\n",
      "|         Cat|    2013|         313|         585|\n",
      "|         Dog|    2013|          93|         585|\n",
      "|     Hamster|    2013|           3|         585|\n",
      "|         Cat|    2014|         298|         583|\n",
      "|         Dog|    2014|          90|         583|\n",
      "|     Hamster|    2014|           1|         583|\n",
      "+------------+--------+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(rescue_annual\n",
    "    .filter(\n",
    "        (F.col(\"animal_group\").isin(\"Cat\", \"Dog\", \"Hamster\")) &\n",
    "        (F.col(\"cal_year\").between(2012, 2014)))\n",
    "    .orderBy(\"cal_year\", \"animal_group\")\n",
    "    .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "rescue_annual %>%\n",
    "    sparklyr::filter(\n",
    "        (animal_group %in% c(\"Cat\", \"Dog\", \"Hamster\")) &\n",
    "        (cal_year %in% 2012:2014)) %>%\n",
    "    sparklyr::sdf_sort(c(\"cal_year\", \"animal_group\")) %>%\n",
    "    sparklyr::collect() %>%\n",
    "    print()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values in `annual_count` are repeated in every year, as the original rows in the DF have been preserved. Had we aggregated this in the usual way we would have lost the `animal_group` and `animal_count` columns, and only returned one `annual_count` for each `cal_year`.\n",
    "\n",
    "Once we have the `annual_count` column we can complete our calculation with a simple narrow transformation to get the percentage and filter on `\"Dog\"`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+------------+------------+----------+\n",
      "|animal_group|cal_year|animal_count|annual_count|animal_pct|\n",
      "+------------+--------+------------+------------+----------+\n",
      "|         Dog|    2009|         132|         568|     23.24|\n",
      "|         Dog|    2010|         122|         611|     19.97|\n",
      "|         Dog|    2011|         103|         620|     16.61|\n",
      "|         Dog|    2012|         100|         603|     16.58|\n",
      "|         Dog|    2013|          93|         585|      15.9|\n",
      "|         Dog|    2014|          90|         583|     15.44|\n",
      "|         Dog|    2015|          88|         540|      16.3|\n",
      "|         Dog|    2016|         107|         604|     17.72|\n",
      "|         Dog|    2017|          81|         539|     15.03|\n",
      "|         Dog|    2018|          91|         609|     14.94|\n",
      "|         Dog|    2019|           1|          36|      2.78|\n",
      "+------------+--------+------------+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rescue_annual = (rescue_annual\n",
    "                 .withColumn(\"animal_pct\",\n",
    "                             F.round(\n",
    "                                 (F.col(\"animal_count\") / F.col(\"annual_count\")) * 100, 2)))\n",
    "\n",
    "rescue_annual.filter(F.col(\"animal_group\") == \"Dog\").orderBy(\"cal_year\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "rescue_annual <- rescue_annual %>%\n",
    "    sparklyr::mutate(animal_pct = round((animal_count / annual_count) * 100, 2))\n",
    "\n",
    "rescue_annual %>%\n",
    "    sparklyr::filter(animal_group == \"Dog\") %>%\n",
    "    sparklyr::sdf_sort(\"animal_group\") %>%\n",
    "    sparklyr::collect() %>%\n",
    "    print()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example used `F.sum()`/`sum()` but other aggregations are possible too, e.g. [`F.mean()`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.mean.html)/[`mean()`](https://spark.apache.org/docs/latest/api/sql/index.html#mean), [`F.max()`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.max.html)/[`max()`](https://spark.apache.org/docs/latest/api/sql/index.html#max). In PySpark, use multiple `.withColumn()` statements; in sparklyr, you can combine them in `mutate()`. In this example we filter on `\"Snake\"`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+------------+------------+----------+------------------+---------+\n",
      "|animal_group|cal_year|animal_count|annual_count|animal_pct|         avg_count|max_count|\n",
      "+------------+--------+------------+------------+----------+------------------+---------+\n",
      "|       Snake|    2018|           1|         609|      0.16|           38.0625|      305|\n",
      "|       Snake|    2013|           2|         585|      0.34|              45.0|      313|\n",
      "|       Snake|    2009|           3|         568|      0.53| 37.86666666666667|      263|\n",
      "|       Snake|    2016|           1|         604|      0.17|43.142857142857146|      297|\n",
      "|       Snake|    2017|           1|         539|      0.19|              49.0|      258|\n",
      "+------------+--------+------------+------------+----------+------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rescue_annual = (rescue_annual\n",
    "          .withColumn(\"avg_count\",\n",
    "                     F.mean(\"animal_count\").over(Window.partitionBy(\"cal_year\")))\n",
    "          .withColumn(\"max_count\",\n",
    "                     F.max(\"animal_count\").over(Window.partitionBy(\"cal_year\"))                     \n",
    "                     ))\n",
    "          \n",
    "rescue_annual.filter(F.col(\"animal_group\") == \"Snake\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "rescue_annual = rescue_annual %>%\n",
    "    dplyr::group_by(cal_year) %>%\n",
    "    sparklyr::mutate(\n",
    "        avg_count = mean(animal_count),\n",
    "        max_count = max(animal_count)) %>%\n",
    "    dplyr::ungroup()\n",
    "\n",
    "rescue_annual %>%\n",
    "    sparklyr::filter(animal_group == \"Snake\") %>%\n",
    "    head(10) %>%\n",
    "    sparklyr::collect() %>%\n",
    "    print()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The alternative to window functions is creating a new grouped and aggregated DF, then joining it back to the original one. As well as being less efficient, the code will also be harder to read. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+------------+------------+\n",
      "|cal_year|animal_group|animal_count|annual_count|\n",
      "+--------+------------+------------+------------+\n",
      "|    2009|         Dog|         132|         568|\n",
      "|    2010|         Dog|         122|         611|\n",
      "|    2011|         Dog|         103|         620|\n",
      "|    2012|         Dog|         100|         603|\n",
      "|    2013|         Dog|          93|         585|\n",
      "|    2014|         Dog|          90|         583|\n",
      "|    2015|         Dog|          88|         540|\n",
      "|    2016|         Dog|         107|         604|\n",
      "|    2017|         Dog|          81|         539|\n",
      "|    2018|         Dog|          91|         609|\n",
      "|    2019|         Dog|           1|          36|\n",
      "+--------+------------+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rescue_counts = rescue_agg.groupBy(\"cal_year\").agg(F.sum(\"animal_count\").alias(\"annual_count\"))\n",
    "rescue_annual_alternative = rescue_agg.join(rescue_counts, on=\"cal_year\", how=\"left\")\n",
    "rescue_annual_alternative.filter(F.col(\"animal_group\") == \"Dog\").orderBy(\"cal_year\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "rescue_counts <- rescue_agg %>%\n",
    "    dplyr::group_by(cal_year) %>%\n",
    "    dplyr::summarise(annual_count = sum(animal_count))\n",
    "\n",
    "rescue_annual_alternative <- rescue_agg %>%\n",
    "    sparklyr::left_join(rescue_counts, by=\"cal_year\")\n",
    "\n",
    "rescue_annual_alternative %>%\n",
    "    sparklyr::filter(animal_group == \"Dog\") %>%\n",
    "    sparklyr::collect() %>%\n",
    "    print()\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Window Functions for Ranking\n",
    "\n",
    "Window functions can also be ordered as well as grouped. This can be combined with [`F.rank()`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.rank.html)/[`rank()`](https://spark.apache.org/docs/latest/api/sql/index.html#rank) or [`F.row_number()`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.row_number.html)/[`row_number()`](https://spark.apache.org/docs/latest/api/sql/index.html#rank) to get ranks within groups. For instance, we can get the ranking of the most commonly rescued animals by year, then filter on the top three.\n",
    "\n",
    "The syntax is again different between PySpark and sparklyr. In PySpark, use the same method as described above for aggregations, but replace `F.sum()` with `F.rank()` (or another ordered function), and add `orderBy()`. In this example, use [`F.desc(\"animal_count\")`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.desc.html) to sort descending. The `.partitionBy()` step is optional; without a `.partitionBy()` it will treat the whole DataFrame as one group.\n",
    "\n",
    "In sparklyr, the method is also almost the same as using aggregations. The ordering is done directly with the `rank()` function. `desc(animal_count)` is used to sort descending."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rescue_rank = (\n",
    "    rescue_agg\n",
    "    .withColumn(\"rank\",\n",
    "                F.rank().over(\n",
    "                    Window.partitionBy(\"cal_year\").orderBy(F.desc(\"animal_count\")))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "rescue_rank = rescue_agg %>%\n",
    "    dplyr::group_by(cal_year) %>%\n",
    "    sparklyr::mutate(rank = rank(desc(animal_count))) %>%\n",
    "    dplyr::ungroup()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the `rank` column we can filter on those less than or equal to `3`, to get the top 3 animals rescued by year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+------------+----+\n",
      "|animal_group|cal_year|animal_count|rank|\n",
      "+------------+--------+------------+----+\n",
      "|Cat         |2009    |263         |1   |\n",
      "|Dog         |2009    |132         |2   |\n",
      "|Bird        |2009    |89          |3   |\n",
      "|Cat         |2010    |297         |1   |\n",
      "|Dog         |2010    |122         |2   |\n",
      "|Bird        |2010    |99          |3   |\n",
      "|Cat         |2011    |309         |1   |\n",
      "|Bird        |2011    |120         |2   |\n",
      "|Dog         |2011    |103         |3   |\n",
      "|Cat         |2012    |305         |1   |\n",
      "|Bird        |2012    |112         |2   |\n",
      "|Dog         |2012    |100         |3   |\n",
      "+------------+--------+------------+----+\n",
      "only showing top 12 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rescue_rank.filter(F.col(\"rank\") <= 3).orderBy(\"cal_year\", \"rank\").show(12, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "rescue_rank %>%\n",
    "    sparklyr::filter(rank <= 3) %>%\n",
    "    sparklyr::sdf_sort(c(\"cal_year\", \"rank\")) %>%\n",
    "    head(12) %>%\n",
    "    sparklyr::collect() %>%\n",
    "    print()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another common use case is getting just the top row from each group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+------------+----+\n",
      "|animal_group|cal_year|animal_count|rank|\n",
      "+------------+--------+------------+----+\n",
      "|Cat         |2009    |263         |1   |\n",
      "|Cat         |2010    |297         |1   |\n",
      "|Cat         |2011    |309         |1   |\n",
      "|Cat         |2012    |305         |1   |\n",
      "|Cat         |2013    |313         |1   |\n",
      "|Cat         |2014    |298         |1   |\n",
      "|Cat         |2015    |263         |1   |\n",
      "|Cat         |2016    |297         |1   |\n",
      "|Cat         |2017    |258         |1   |\n",
      "|Cat         |2018    |305         |1   |\n",
      "|Cat         |2019    |16          |1   |\n",
      "+------------+--------+------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rescue_rank.filter(F.col(\"rank\") == 1).orderBy(\"cal_year\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "rescue_rank %>%\n",
    "    sparklyr::filter(rank == 1) %>%\n",
    "    sparklyr::sdf_sort(\"cal_year\") %>%\n",
    "    sparklyr::collect() %>%\n",
    "    print()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison of ranking methods\n",
    "\n",
    "Note that you can have duplicate ranks within each group when using `rank()`; if this is not desirable then one method is to partition by more columns to break ties. There are also alternatives to `rank()` depending on your use case:\n",
    "\n",
    "- `F.rank()`/`rank()` will assign the same value to ties.\n",
    "- [`F.dense_rank()`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.dense_rank.html)/[`dense_rank()`](https://spark.apache.org/docs/latest/api/sql/index.html#dense_rank) will not skip a rank after ties.\n",
    "- `F.row_number()`/`row_number()` will give a unique number to each row within the grouping specified. Note that this can be non-deterministic if there are duplicate rows for the ordering condition specified. This can be avoided by specifying extra columns to essentially use as a tiebreaker.\n",
    "\n",
    "We can see the difference by comparing the three methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------+--------+------------+----+----------+----------+\n",
      "|animal_group                                    |cal_year|animal_count|rank|dense_rank|row_number|\n",
      "+------------------------------------------------+--------+------------+----+----------+----------+\n",
      "|Cat                                             |2012    |305         |1   |1         |1         |\n",
      "|Bird                                            |2012    |112         |2   |2         |2         |\n",
      "|Dog                                             |2012    |100         |3   |3         |3         |\n",
      "|Horse                                           |2012    |28          |4   |4         |4         |\n",
      "|Unknown - Domestic Animal Or Pet                |2012    |18          |5   |5         |5         |\n",
      "|Fox                                             |2012    |14          |6   |6         |6         |\n",
      "|Deer                                            |2012    |7           |7   |7         |7         |\n",
      "|Squirrel                                        |2012    |4           |8   |8         |8         |\n",
      "|Unknown - Wild Animal                           |2012    |4           |8   |8         |9         |\n",
      "|Unknown - Heavy Livestock Animal                |2012    |4           |8   |8         |10        |\n",
      "|Cow                                             |2012    |3           |11  |9         |11        |\n",
      "|Ferret                                          |2012    |1           |12  |10        |12        |\n",
      "|Lamb                                            |2012    |1           |12  |10        |13        |\n",
      "|Sheep                                           |2012    |1           |12  |10        |14        |\n",
      "|Unknown - Animal Rescue From Water - Farm Animal|2012    |1           |12  |10        |15        |\n",
      "+------------------------------------------------+--------+------------+----+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rank_comparison = (rescue_agg\n",
    "    .withColumn(\"rank\",\n",
    "                F.rank().over(\n",
    "                    Window\n",
    "                    .partitionBy(\"cal_year\")\n",
    "                    .orderBy(F.desc(\"animal_count\"))))\n",
    "    .withColumn(\"dense_rank\",\n",
    "                F.dense_rank().over(\n",
    "                    Window\n",
    "                    .partitionBy(\"cal_year\")\n",
    "                    .orderBy(F.desc(\"animal_count\"))))\n",
    "    .withColumn(\"row_number\",\n",
    "                F.row_number().over(\n",
    "                    Window\n",
    "                    .partitionBy(\"cal_year\")\n",
    "                    .orderBy(F.desc(\"animal_count\"))))\n",
    ")\n",
    "\n",
    "(rank_comparison\n",
    "    .filter(F.col(\"cal_year\") == 2012)\n",
    "    .orderBy(\"cal_year\", \"row_number\")\n",
    "    .show(truncate=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "rank_comparison <- rescue_agg %>%\n",
    "    dplyr::group_by(cal_year) %>%\n",
    "    sparklyr::mutate(\n",
    "        rank = rank(desc(animal_count)),\n",
    "        dense_rank = dense_rank(desc(animal_count)),\n",
    "        row_number = row_number(desc(animal_count))) %>%\n",
    "    dplyr::ungroup()\n",
    "\n",
    "rank_comparison %>%\n",
    "    sparklyr::filter(cal_year == 2012) %>%\n",
    "    sparklyr::sdf_sort(c(\"cal_year\", \"row_number\")) %>%\n",
    "    sparklyr::collect() %>%\n",
    "    print()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all the values where `animal_count` is `4`, `rank` and `dense_rank` have `8`, whereas `row_number` gives a unique number from `8` to `10`. As no other sorting columns were specified, these three rows could be assigned differently on subsequent runs of the code.\n",
    "\n",
    "For `animal_count` less than  `4`, `dense_rank` has left no gap in the ranking sequence, whereas `rank` will leave gaps.\n",
    "\n",
    "`row_number` has a unique value for each row, even for tied values.\n",
    "\n",
    "#### Generating unique row numbers\n",
    "\n",
    "Spark DataFrames do not have an index in the same way as pandas or base R DataFrames as they are partitioned on the cluster. You can however use `row_number()` to generate a unique identifier for each row. \n",
    "\n",
    "Whereas in the previous example we ranked within groups, here we need to treat the whole DataFrame as one group.\n",
    "\n",
    "To do this in PySpark, use just `Window.orderBy(col1, col2, ...)` without the `partitionBy()`. In sparklyr, just use `mutate()` without `group_by()` and `ungroup()`.\n",
    "\n",
    "Remember to be careful as this can be non-deterministic if there are duplicate rows for the ordering condition specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+--------+------------+----------+\n",
      "|animal_group                    |cal_year|animal_count|row_number|\n",
      "+--------------------------------+--------+------------+----------+\n",
      "|Hedgehog                        |2009    |1           |1         |\n",
      "|Dog                             |2009    |132         |2         |\n",
      "|Sheep                           |2009    |1           |3         |\n",
      "|Deer                            |2009    |8           |4         |\n",
      "|Lizard                          |2009    |1           |5         |\n",
      "|Unknown - Heavy Livestock Animal|2009    |14          |6         |\n",
      "|Unknown - Wild Animal           |2009    |6           |7         |\n",
      "|Rabbit                          |2009    |1           |8         |\n",
      "|Bird                            |2009    |89          |9         |\n",
      "|Horse                           |2009    |19          |10        |\n",
      "+--------------------------------+--------+------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(rescue_agg\n",
    "    .withColumn(\"row_number\",\n",
    "                F.row_number().over(Window.orderBy(\"cal_year\")))\n",
    "    .show(10, truncate=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "rescue_agg %>%\n",
    "    sparklyr::mutate(row_number = row_number(cal_year)) %>%\n",
    "    head(10) %>%\n",
    "    sparklyr::collect() %>%\n",
    "    print()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference other rows with `lag()` and `lead()`\n",
    "\n",
    "The window function [`F.lag()`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.lag.html)/[`lag()`](https://spark.apache.org/docs/latest/api/sql/index.html#lag)  allows you to reference the values of previous rows within a group, and [`F.lead()`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.lead.html)/[`lead()`](https://spark.apache.org/docs/latest/api/sql/index.html#lead) will do the same for subsequent rows. You can specify how many previous rows you want to reference with the `count` argument. By default this is `1`. Note that `count` can be negative, so `lag(col, count=1)` is the same as `lead(col, count=-1)`.\n",
    "\n",
    "The first or last row within the window partition will be null values, as they do not have a previous or subsequent row to reference. This can be changed by setting the `default` parameter, which by default is `None`.\n",
    "\n",
    "These window functions differ from `rank()` and `row_number()` as they are referencing values, rather than returning a rank. They do however use ordering in the same way.\n",
    "\n",
    "We can use `lag()` or `lead()` to get the number of animals rescued in the previous year, with the intention of calculating the annual change. The process for this is identical to the Using Window Functions for Ranking section, just using `lag()` as the function within the window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------+--------+------------+--------------+\n",
      "|animal_group                                    |cal_year|animal_count|previous_count|\n",
      "+------------------------------------------------+--------+------------+--------------+\n",
      "|Unknown - Animal Rescue From Water - Farm Animal|2012    |1           |null          |\n",
      "|Unknown - Animal Rescue From Water - Farm Animal|2014    |1           |1             |\n",
      "|Unknown - Animal Rescue From Water - Farm Animal|2019    |1           |1             |\n",
      "|Cow                                             |2010    |2           |null          |\n",
      "|Cow                                             |2012    |3           |2             |\n",
      "|Cow                                             |2014    |1           |3             |\n",
      "|Cow                                             |2016    |1           |1             |\n",
      "|Horse                                           |2009    |19          |null          |\n",
      "|Horse                                           |2010    |15          |19            |\n",
      "|Horse                                           |2011    |22          |15            |\n",
      "+------------------------------------------------+--------+------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(rescue_agg\n",
    "    .withColumn(\"previous_count\",\n",
    "                F.lag(\"animal_count\").over(\n",
    "                    Window.partitionBy(\"animal_group\").orderBy(\"cal_year\")))\n",
    "    .show(10, truncate=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "rescue_agg %>%\n",
    "    dplyr::group_by(animal_group) %>%\n",
    "    sparklyr::mutate(previous_count = lag(animal_count, order_by = cal_year)) %>%\n",
    "    dplyr::ungroup() %>%\n",
    "    head(10) %>%\n",
    "    sparklyr::collect() %>%\n",
    "    print()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be careful if using `lag()` with incomplete data: where there were no animals rescued in a year the `previous_count` will not be correct.\n",
    "\n",
    "There are several ways to resolve this; one method is using a [cross join](../spark-functions/cross-join) to get all the combinations of `animal_group` and `cal_year`, join the `rescue_agg` to this, fill the null values with `0`, and then do the window calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+------------+--------------+\n",
      "|animal_group|cal_year|animal_count|previous_count|\n",
      "+------------+--------+------------+--------------+\n",
      "|Bird        |2009    |89          |null          |\n",
      "|Bird        |2010    |99          |89            |\n",
      "|Bird        |2011    |120         |99            |\n",
      "|Bird        |2012    |112         |120           |\n",
      "|Bird        |2013    |85          |112           |\n",
      "|Bird        |2014    |110         |85            |\n",
      "|Bird        |2015    |106         |110           |\n",
      "|Bird        |2016    |120         |106           |\n",
      "|Bird        |2017    |124         |120           |\n",
      "|Bird        |2018    |126         |124           |\n",
      "|Bird        |2019    |9           |126           |\n",
      "|Budgie      |2009    |0           |null          |\n",
      "|Budgie      |2010    |0           |0             |\n",
      "|Budgie      |2011    |1           |0             |\n",
      "|Budgie      |2012    |0           |1             |\n",
      "|Budgie      |2013    |0           |0             |\n",
      "|Budgie      |2014    |0           |0             |\n",
      "|Budgie      |2015    |0           |0             |\n",
      "|Budgie      |2016    |0           |0             |\n",
      "|Budgie      |2017    |0           |0             |\n",
      "+------------+--------+------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a DF of all combinations of animal_group and cal_year\n",
    "all_animals_years = (rescue_agg\n",
    "                     .select(\"animal_group\")\n",
    "                     .distinct()\n",
    "                     .crossJoin(\n",
    "                         rescue_agg\n",
    "                         .select(\"cal_year\")\n",
    "                         .distinct()))\n",
    "\n",
    "# Use this DF as a base to join the rescue_agg DF to\n",
    "rescue_agg_prev = (\n",
    "    all_animals_years\n",
    "    .join(rescue_agg, on=[\"animal_group\", \"cal_year\"], how=\"left\")\n",
    "    # Replace null with 0\n",
    "    .fillna(0, \"animal_count\")\n",
    "    # lag will then reference previous year, even if 0\n",
    "    .withColumn(\"previous_count\",\n",
    "                F.lag(\"animal_count\").over(\n",
    "                    Window.partitionBy(\"animal_group\").orderBy(\"cal_year\"))))\n",
    "\n",
    "rescue_agg_prev.orderBy(\"animal_group\", \"cal_year\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "# Create a DF of all combinations of animal_group and cal_year\n",
    "all_animals_years <- rescue_agg %>%\n",
    "    sparklyr::select(animal_group) %>%\n",
    "    sparklyr::sdf_distinct() %>%\n",
    "    sparklyr::full_join(\n",
    "        rescue_agg %>% sparklyr::select(cal_year) %>% sparklyr::sdf_distinct(),\n",
    "        by=character()) %>%\n",
    "    sparklyr::sdf_sort(c(\"animal_group\", \"cal_year\"))\n",
    "\n",
    "# Use this DF as a base to join the rescue_agg DF to\n",
    "rescue_agg_prev <- all_animals_years %>%\n",
    "    sparklyr::left_join(rescue_agg, by=c(\"animal_group\", \"cal_year\")) %>%\n",
    "    # Replace null with 0\n",
    "    sparklyr::mutate(animal_count = ifnull(animal_count, 0)) %>%\n",
    "    dplyr::group_by(animal_group) %>%\n",
    "    # lag will then reference previous year, even if 0\n",
    "    sparklyr::mutate(previous_count = lag(animal_count, order_by = cal_year)) %>%\n",
    "    dplyr::ungroup()\n",
    "\n",
    "rescue_agg_prev %>%\n",
    "    sparklyr::sdf_sort(c(\"animal_group\", \"cal_year\")) %>%\n",
    "    head(20) %>%\n",
    "    sparklyr::collect() %>%\n",
    "    print()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Window Functions in SQL\n",
    "\n",
    "You can also use the regular SQL syntax for window functions when using Spark, `OVER(PARTITION BY...GROUP BY)`. This needs an SQL wrapper to be processed in Spark, [`spark.sql()`](https://spark.apache.org/docs/latest/sql-getting-started.html#running-sql-queries-programmatically) in PySpark and [`tbl(sc, sql())`](https://dplyr.tidyverse.org/reference/tbl.html) in sparklyr. Remember that SQL works on tables rather than DataFrames, so register the DataFrame first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+------------+------------+\n",
      "|cal_year|animal_group|animal_count|annual_count|\n",
      "+--------+------------+------------+------------+\n",
      "|    2018|       Snake|           1|         609|\n",
      "|    2013|       Snake|           2|         585|\n",
      "|    2009|       Snake|           3|         568|\n",
      "|    2016|       Snake|           1|         604|\n",
      "|    2017|       Snake|           1|         539|\n",
      "+--------+------------+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rescue_agg.registerTempTable(\"rescue_agg\")\n",
    "\n",
    "sql_window = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT\n",
    "        cal_year,\n",
    "        animal_group,\n",
    "        animal_count,\n",
    "        SUM(animal_count) OVER(PARTITION BY cal_year) AS annual_count\n",
    "    FROM rescue_agg\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "sql_window.filter(F.col(\"animal_group\") == \"Snake\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "sparklyr::sdf_register(rescue_agg, \"rescue_agg\")\n",
    "\n",
    "sql_window <- dplyr::tbl(sc, dplyr::sql(\n",
    "    \"\n",
    "    SELECT\n",
    "        cal_year,\n",
    "        animal_group,\n",
    "        animal_count,\n",
    "        SUM(animal_count) OVER(PARTITION BY cal_year) AS annual_count\n",
    "    FROM rescue_agg\n",
    "    \"))\n",
    "\n",
    "sql_window %>%\n",
    "    sparklyr::filter(animal_group == \"Snake\") %>%\n",
    "    sparklyr::collect() %>%\n",
    "    print()\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Resources\n",
    "\n",
    "Spark at the ONS Articles:\n",
    "- [Cross Joins](../spark-functions/cross-join)\n",
    "\n",
    "PySpark Documentation:\n",
    "- [`.groupBy()`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.groupBy.html)\n",
    "- [`Window`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.Window.html)\n",
    "- [`.withColumn()`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.withColumn.html)\n",
    "- [`F.sum()`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.sum.html)\n",
    "- [`.over()`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.Column.over.html)\n",
    "- [`.partitionBy()`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.Window.partitionBy.html)\n",
    "- [`.orderBy()`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.orderBy.html)\n",
    "- [`F.mean()`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.mean.html)\n",
    "- [`F.max()`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.max.html)\n",
    "- [`F.rank()`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.rank.html)\n",
    "- [`F.row_number()`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.row_number.html)\n",
    "- [`F.desc()`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.desc.html)\n",
    "- [`F.dense_rank()`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.dense_rank.html)\n",
    "- [`F.lag()`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.lag.html)\n",
    "- [`F.lead()`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.lead.html)\n",
    "- [`spark.sql()`](https://spark.apache.org/docs/latest/sql-getting-started.html#running-sql-queries-programmatically)\n",
    "\n",
    "sparklyr and tidyverse Documentation:\n",
    "- [`group_by()` and `ungroup()`](https://dplyr.tidyverse.org/reference/group_by.html)\n",
    "- [`mutate()`](https://dplyr.tidyverse.org/reference/mutate.html)\n",
    "- [`summarise()`](https://dplyr.tidyverse.org/reference/summarise.html)\n",
    "- [`tbl()`](https://dplyr.tidyverse.org/reference/tbl.html)\n",
    "\n",
    "Spark SQL Functions Documentation:\n",
    "- [`sum`](https://spark.apache.org/docs/latest/api/sql/index.html#sum)\n",
    "- [`mean`](https://spark.apache.org/docs/latest/api/sql/index.html#mean)\n",
    "- [`max`](https://spark.apache.org/docs/latest/api/sql/index.html#max)\n",
    "- [`rank`](https://spark.apache.org/docs/latest/api/sql/index.html#rank)\n",
    "- [`row_number`](https://spark.apache.org/docs/latest/api/sql/index.html#rank)\n",
    "- [`dense_rank`](https://spark.apache.org/docs/latest/api/sql/index.html#dense_rank)\n",
    "- [`lag`](https://spark.apache.org/docs/latest/api/sql/index.html#lag)\n",
    "- [`lead`](https://spark.apache.org/docs/latest/api/sql/index.html#lead)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
