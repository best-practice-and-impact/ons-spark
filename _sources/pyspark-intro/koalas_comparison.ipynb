{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark, pandas and Koalas comparison\n",
    "\n",
    "This document compares how to manipulate a DataFrame with PySpark, pandas and Koalas.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Koalas is a project by DataBricks, initially released in 2019, which aims to bridge the gap for those who know pandas and want to leverage the power of distributed computing, but who do not know PySpark. The syntax for most operations is the same as for pandas, so there is less need to learn PySpark from scratch.\n",
    "\n",
    "The Koalas project is still in development and not all pandas or PySpark functionality is available yet. As of June 2021, new versions of the package are being released on a regular basis.\n",
    "\n",
    "\n",
    "\n",
    "Koalas isn't installed on CDSW by default so will require installation from [Artifactory](http://np2rvlapxx507/DAP_CATS/guidance/-/blob/master/Artifactory.md); please see the [Koalas setup instructions](http://np2rvlapxx507/DAP_CATS/guidance/-/blob/master/koalas_setup.md) for more details.\n",
    "\n",
    "This document is not intended as a recommendation to learn one implementation over another.\n",
    "\n",
    "Throughout this document, to clearly distinguish between different types of DataFrames (DFs), pandas DFs are prefixed with `pdf`, Koalas DFs with `kdf`, and PySpark DFs with `sdf`. Derived scalar values are suffixed `_pandas`. `_koalas` and `_spark`. Prefixing or suffixing variable names with their data type in this way is generally not necessary when writing your own code; instead, choose short sensible names for your DFs. For more information on variable naming, please consult [Core Programming: Naming Variables](https://best-practice-and-impact.github.io/qa-of-code-guidance/core_programming.html#naming-variables) from the [QA of Code for Analysis and Research](https://best-practice-and-impact.github.io/qa-of-code-guidance/intro.html).\n",
    "\n",
    "## Reading, creating and converting data\n",
    "\n",
    "### Import relevant packages and setup\n",
    "\n",
    "Load packages into CDSW Python session.\n",
    "\n",
    "````{tabs}\n",
    "```{code-tab} python pandas\n",
    "import pandas as pd\n",
    "```\n",
    "```{code-tab} python PySpark\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "```\n",
    "```{code-tab} python Koalas\n",
    "import databricks.koalas as ks # Note the databricks prefix here for Koalas\n",
    "```\n",
    "````\n",
    "\n",
    "If you haven't set the environment variables `ARROW_PRE_0_15_IPC_FORMAT` and `PYARROW_IGNORE_TIMEZONE` to `1` then you may get a warning when importing Koalas.\n",
    "\n",
    "Note that Koalas will use an existing Spark session if one exists, otherwise it will create one with the session name `Koalas` the first time a Koalas DataFrame is loaded.\n",
    "\n",
    "````{tabs}\n",
    "```{code-tab} python Koalas\n",
    "# Start a Spark session\n",
    "spark = (SparkSession.builder.master(\"local[2]\")\n",
    "         .appName(\"comparison\")\n",
    "         .getOrCreate())\n",
    "```         \n",
    "```{code-tab} python PySpark\n",
    "spark = (SparkSession.builder.master(\"local[2]\")\n",
    "         .appName(\"comparison\")\n",
    "         .getOrCreate())\n",
    "```\n",
    "````\n",
    "\n",
    "### Create DataFrame\n",
    "\n",
    "Create pandas, Koalas or PySpark DataFrame based on rows and columns\n",
    "````{tabs}\n",
    "```{code-tab} python pandas\n",
    "rows = [[\"SW1V\", 23153], [\"PO15\", 22526], [\"NP10\", 23424]]\n",
    "columns = [\"postcode_district\", \"population\"]\n",
    "\n",
    "pdf = pd.DataFrame(rows, columns=columns)\n",
    "```\n",
    "```{code-tab} python PySpark\n",
    "rows = [[\"SW1V\", 23153], [\"PO15\", 22526], [\"NP10\", 23424]]\n",
    "columns = [\"postcode_district\", \"population\"]\n",
    "\n",
    "sdf = spark.createDataFrame(rows, columns)\n",
    "```\n",
    "```{code-tab} python Koalas\n",
    "rows = [[\"SW1V\", 23153], [\"PO15\", 22526], [\"NP10\", 23424]]\n",
    "columns = [\"postcode_district\", \"population\"]\n",
    "\n",
    "kdf = ks.DataFrame(rows, columns=columns)\n",
    "```\n",
    "````\n",
    "\n",
    "### Convert DataFrame to pandas\n",
    "\n",
    "Convert from specified DataFrame type to pandas\n",
    "````{tabs}\n",
    "```{code-tab} python pandas\n",
    "pdf = kdf.to_pandas()\n",
    "\n",
    "```\n",
    "```{code-tab} python PySpark\n",
    "pdf = sdf.toPandas()\n",
    "```\n",
    "````\n",
    "\n",
    "### Convert DataFrame to Koalas\n",
    "\n",
    "Convert from specified DataFrame type to Koalas\n",
    "````{tabs}\n",
    "```{code-tab} python pandas\n",
    "kdf = ks.from_pandas(pdf)\n",
    "\n",
    "```\n",
    "```{code-tab} python PySpark\n",
    "kdf = ks.DataFrame(sdf)\n",
    "```\n",
    "````\n",
    "\n",
    "### Convert DataFrame to PySpark\n",
    "\n",
    "Convert from specified DataFrame type to PySpark\n",
    "````{tabs}\n",
    "```{code-tab} python pandas\n",
    "sdf = spark.createDataFrame(pdf)\n",
    "```\n",
    "```{code-tab} python pandas\n",
    "sdf = kdf.to_spark()\n",
    "```\n",
    "````\n",
    "\n",
    "### Read CSV from HDFS\n",
    "\n",
    "Read in a CSV from HDFS.\n",
    "\n",
    "For pandas you can read and write directly to HDFS with [Pydoop](http://np2rvlapxx507/DAP_CATS/troubleshooting/tip-of-the-week/-/blob/master/tip_12_pydoop.ipynb) or go via PySpark.\n",
    "\n",
    "file_path = \"/training/animal_rescue.csv\"\n",
    "````{tabs}\n",
    "```{code-tab} python pandas\n",
    "# Easiest to read in with Koalas/Pyspark then convert\n",
    "# Be careful of file size since all the data needs to be able to fit on the driver (CDSW or notebook session)\n",
    "sdf = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "pdf = sdf.toPandas()\n",
    "\n",
    "```\n",
    "```{code-tab} python pandas\n",
    "kdf = ks.read_csv(file_path, header=0)\n",
    "\n",
    "```\n",
    "```{code-tab} python PySpark\n",
    "sdf = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "```\n",
    "````\n",
    "\n",
    "## Selecting and renaming columns\n",
    "\n",
    "### Rename one column\n",
    "\n",
    "Rename just one column\n",
    "\n",
    "old_col = \"IncidentNumber\"\n",
    "new_col = \"incident_number\"\n",
    "````{tabs}\n",
    "```{code-tab} python pandas\n",
    "pdf = pdf.rename({old_col: new_col}, axis=1)\n",
    "\n",
    "```\n",
    "```{code-tab} python pandas\n",
    "kdf = kdf.rename({old_col: new_col}, axis=1)\n",
    "\n",
    "```\n",
    "```{code-tab} python PySpark\n",
    "sdf = sdf.withColumnRenamed(old_col, new_col)\n",
    "```\n",
    "````\n",
    "\n",
    "### Rename multiple columns\n",
    "\n",
    "Use a dictionary to rename multiple columns\n",
    "\n",
    "columns_dict = {\"HourlyNotionalCost(Â£)\": \"total_cost\",\n",
    "                \"AnimalGroupParent\": \"animal_group\",\n",
    "                \"CalYear\": \"cal_year\",\n",
    "                \"PumpHoursTotal\": \"job_hours\",\n",
    "                \"PumpCount\": \"engine_count\",\n",
    "                \"OriginofCall\": \"origin_of_call\",\n",
    "                \"PropertyType\": \"property_type\",\n",
    "                \"PropertyCategory\": \"property_category\"\n",
    "}\n",
    "````{tabs}\n",
    "```{code-tab} python pandas\n",
    "pdf = pdf.rename(columns_dict, axis=1)\n",
    "\n",
    "```\n",
    "```{code-tab} python pandas\n",
    "kdf = kdf.rename(columns_dict, axis=1)\n",
    "\n",
    "```\n",
    "```{code-tab} python PySpark\n",
    "for old_col, new_col in columns_dict.items():\n",
    "    sdf = sdf.withColumnRenamed(old_col, new_col)\n",
    "    ```\n",
    "````\n",
    "\n",
    "### Select columns\n",
    "\n",
    "Select a subset of columns\n",
    "````{tabs}\n",
    "```{code-tab} python pandas\n",
    "columns = [\"incident_number\"] + [new_col[1] for new_col in columns_dict.items()]\n",
    "\n",
    "pdf = pdf[columns]\n",
    "\n",
    "```\n",
    "```{code-tab} python pandas\n",
    "columns = [\"incident_number\"] + [new_col[1] for new_col in columns_dict.items()]\n",
    "\n",
    "kdf = kdf[columns]\n",
    "\n",
    "```\n",
    "```{code-tab} python PySpark\n",
    "columns = [\"incident_number\"] + [new_col[1] for new_col in columns_dict.items()]\n",
    "\n",
    "sdf = sdf.select(columns)\n",
    "```\n",
    "````\n",
    "\n",
    "### Drop columns\n",
    "\n",
    "Remove columns\n",
    "\n",
    "columns = [\"property_type\", \"property_category\"]\n",
    "````{tabs}\n",
    "```{code-tab} python pandas\n",
    "pdf.drop(columns, axis=1, inplace=True)\n",
    "\n",
    "```\n",
    "```{code-tab} python pandas\n",
    "# axis=1 is the default value\n",
    "kdf = kdf.drop(columns)\n",
    "\n",
    "```\n",
    "```{code-tab} python PySpark\n",
    "# If using a list, need to unpack the list with *\n",
    "sdf = sdf.drop(*columns)\n",
    "# This is not needed if passing the column names directly\n",
    "sdf = sdf.drop(columns[0], columns[1])\n",
    "```\n",
    "````\n",
    "\n",
    "### Derive new column\n",
    "\n",
    "Calculate a new column based on other columns. This will overwrite an existing column if one already exists with the same name.\n",
    "\n",
    "new_col = \"incident_duration\"\n",
    "numerator_col = \"job_hours\"\n",
    "denominator_col = \"engine_count\"\n",
    "````{tabs}\n",
    "```{code-tab} python pandas\n",
    "pdf[new_col] = pdf[numerator_col] / pdf[denominator_col]\n",
    "\n",
    "```\n",
    "```{code-tab} python pandas\n",
    "kdf[new_col] = kdf[numerator_col] / kdf[denominator_col]\n",
    "\n",
    "```\n",
    "```{code-tab} python PySpark\n",
    "sdf = sdf.withColumn(new_col, F.col(numerator_col) / F.col(denominator_col))\n",
    "```\n",
    "````\n",
    "\n",
    "## Preview data and structure\n",
    "\n",
    "### Preview data\n",
    "\n",
    "Display first n rows. PySpark and Koalas DataFrames are not ordered in the same way as pandas DFs, and so this may return different results unless explicitly ordered with `sdf.sort()`/`sdf.orderBy()` or `kdf.sort_values()`.\n",
    "\n",
    "`tail()` does not exist in Spark 2.4.0, even on an ordered DF, and so cannot be used on PySpark or Koalas DFs.\n",
    "\n",
    "\n",
    "````{tabs}\n",
    "```{code-tab} python pandas - default 5\n",
    "n = 5\n",
    "pdf.head(n)\n",
    "```\n",
    "\n",
    "```{code-tab} python pandas - default 5\n",
    "n = 5\n",
    "kdf.head(n)\n",
    "```\n",
    "\n",
    "```{code-tab} python PySpark - default 20\n",
    "n = 5\n",
    "sdf.show(n)\n",
    "```\n",
    "````\n",
    "\n",
    "### Data types\n",
    "\n",
    "Get data types\n",
    "\n",
    "````{tabs}\n",
    "```{code-tab} python pandas\n",
    "types_pandas = pdf.dtypes\n",
    "\n",
    "```\n",
    "```{code-tab} python pandas\n",
    "types_koalas = kdf.dtypes\n",
    "\n",
    "```\n",
    "```{code-tab} python PySpark\n",
    "types_spark = sdf.dtypes\n",
    "```\n",
    "````\n",
    "\n",
    "### Row count\n",
    "\n",
    "Get number of rows\n",
    "\n",
    "````{tabs}\n",
    "```{code-tab} python pandas\n",
    "row_ct_pandas = pdf.shape[0]\n",
    "```\n",
    "```{code-tab} python pandas\n",
    "row_ct_koalas = kdf.shape[0]\n",
    "```\n",
    "```{code-tab} python PySpark\n",
    "row_ct_spark = sdf.count()\n",
    "```\n",
    "````\n",
    "\n",
    "### Column count\n",
    "\n",
    "Get number of columns\n",
    "\n",
    "````{tabs}\n",
    "```{code-tab} python pandas\n",
    "col_ct_pandas = pdf.shape[1]\n",
    "\n",
    "```\n",
    "```{code-tab} python pandas\n",
    "col_ct_koalas = pdf.shape[1]\n",
    "\n",
    "```\n",
    "```{code-tab} python PySpark\n",
    "# This is not a function, so no () after columns\n",
    "col_ct_spark = len(sdf.columns)\n",
    "```\n",
    "````\n",
    "\n",
    "### Count distinct\n",
    "\n",
    "Count distinct number of entries\n",
    "\n",
    "````{tabs}\n",
    "```{code-tab} python pandas: two possible methods\n",
    "distinct_ct_pandas = len(pdf[\"animal_group\"].unique())\n",
    "distinct_ct_pandas = len(set(pdf[\"animal_group\"]))\n",
    "\n",
    "```\n",
    "```{code-tab} python pandas\n",
    "distinct_ct_koalas = len(kdf[\"animal_group\"].unique())\n",
    "\n",
    "```\n",
    "```{code-tab} python PySpark\n",
    "distinct_ct_spark = sdf.select(\"animal_group\").distinct().count()\n",
    "```\n",
    "````\n",
    "\n",
    "## Filter rows\n",
    "\n",
    "Be careful not to confuse the `.filter()` operation; in pandas it will filter on a row index (which doesn't exist in Spark), whereas in PySpark it will filter on values, the same as `.loc` in pandas.\n",
    "\n",
    "### Filter rows on values from one column\n",
    "\n",
    "Filter rows on values from one column\n",
    "\n",
    "````{tabs}\n",
    "```{code-tab} python pandas\n",
    "pdf_dogs = pdf.loc[pdf[\"animal_group\"] == \"Dog\"]\n",
    "\n",
    "```\n",
    "```{code-tab} python pandas\n",
    "kdf_dogs = kdf.loc[kdf[\"animal_group\"] == \"Dog\"]\n",
    "\n",
    "```\n",
    "```{code-tab} python PySpark\n",
    "sdf_dogs = sdf.filter(F.col(\"animal_group\") == \"Dog\")\n",
    "```\n",
    "````\n",
    "\n",
    "### Filter rows on values from multiple columns\n",
    "\n",
    "Filter rows on values from multiple columns; ensure you have each condition wrapped in brackets\n",
    "\n",
    "````{tabs}\n",
    "```{code-tab} python pandas\n",
    "pdf_dogs_recent = pdf.loc[(pdf[\"animal_group\"] == \"Dog\") & (pdf[\"cal_year\"] >= 2017)]\n",
    "\n",
    "```\n",
    "```{code-tab} python pandas\n",
    "kdf_dogs_recent = kdf.loc[(kdf[\"animal_group\"] == \"Dog\") & (kdf[\"cal_year\"] >= 2017)]\n",
    "\n",
    "```\n",
    "```{code-tab} python PySpark\n",
    "sdf_dogs_recent = sdf.filter((F.col(\"animal_group\") == \"Dog\") & ((F.col(\"cal_year\") >= 2017)))\n",
    "```\n",
    "````\n",
    "\n",
    "## Handling missing values\n",
    "\n",
    "### Filter nulls\n",
    "\n",
    "Only return rows where specified column is null\n",
    "\n",
    "col_name = \"job_hours\"\n",
    "````{tabs}\n",
    "```{code-tab} python pandas\n",
    "pdf_null_rows = pdf[pdf[col_name].isnull()]\n",
    "\n",
    "```\n",
    "```{code-tab} python pandas\n",
    "kdf_null_rows = kdf[kdf[col_name].isnull()]\n",
    "\n",
    "```\n",
    "```{code-tab} python PySpark\n",
    "sdf_null_rows = sdf.filter(F.col(col_name).isNull())\n",
    "```\n",
    "````\n",
    "\n",
    "### Filter non-nulls\n",
    "\n",
    "Only return rows where specified column is not null\n",
    "\n",
    "col_name = \"job_hours\"\n",
    "````{tabs}\n",
    "```{code-tab} python pandas\n",
    "pdf = pdf[~pdf[col_name].isnull()]\n",
    "\n",
    "```\n",
    "```{code-tab} python pandas\n",
    "kdf = kdf[~kdf[col_name].isnull()]\n",
    "\n",
    "```\n",
    "```{code-tab} python PySpark\n",
    "sdf = sdf.filter(F.col(col_name).isNotNull())\n",
    "```\n",
    "````\n",
    "\n",
    "### Fill nulls\n",
    "\n",
    "Fill nulls with 0 (or other value)\n",
    "\n",
    "fill_value = 0\n",
    "````{tabs}\n",
    "```{code-tab} python pandas\n",
    "pdf_nato0 = pdf.fillna(fill_value)\n",
    "\n",
    "```\n",
    "```{code-tab} python pandas\n",
    "kdf_nato0 = kdf.fillna(fill_value)\n",
    "\n",
    "```\n",
    "```{code-tab} python PySpark\n",
    "sdf_nato0 = sdf.fillna(fill_value)\n",
    "```\n",
    "````\n",
    "\n",
    "## Grouping and aggregating\n",
    "\n",
    "### Sum a column\n",
    "\n",
    "Returns sum of one column as a scalar value\n",
    "\n",
    "col_name = \"total_cost\"\n",
    "````{tabs}\n",
    "```{code-tab} python pandas\n",
    "col_sum_pandas = pdf[col_name].sum()\n",
    "\n",
    "```\n",
    "```{code-tab} python pandas\n",
    "col_sum_koalas = kdf[col_name].sum()\n",
    "\n",
    "```\n",
    "```{code-tab} python PySpark: sdf.agg() with one column will return a one row DF, use .collect()[0][0] to get a scalar\n",
    "col_sum_spark = sdf.agg(F.sum(col_name)).collect()[0][0]\n",
    "```\n",
    "````\n",
    "\n",
    "### Get maximum of a column\n",
    "\n",
    "Returns maximum of one column as a scalar value\n",
    "\n",
    "col_name = \"total_cost\"\n",
    "````{tabs}\n",
    "```{code-tab} python pandas\n",
    "col_max_pandas = pdf[col_name].max()\n",
    "\n",
    "```\n",
    "```{code-tab} python pandas\n",
    "col_max_koalas = kdf[col_name].max()\n",
    "\n",
    "```\n",
    "```{code-tab} python PySpark: sdf.agg() with one column will return a one row DF, use .collect()[0][0] to get a scalar\n",
    "col_max_spark = sdf.agg(F.max(col_name)).collect()[0][0]\n",
    "```\n",
    "````\n",
    "\n",
    "### Get minimum of a column\n",
    "\n",
    "Returns minimum of one column as a scalar value\n",
    "\n",
    "col_name = \"total_cost\"\n",
    "````{tabs}\n",
    "```{code-tab} python pandas\n",
    "col_min_pandas = pdf[col_name].min()\n",
    "\n",
    "```\n",
    "```{code-tab} python pandas\n",
    "col_min_koalas = kdf[col_name].min()\n",
    "\n",
    "```\n",
    "```{code-tab} python PySpark: sdf.agg() with one column will return a one row DF, use .collect()[0][0] to get a scalar\n",
    "col_min_spark = sdf.agg(F.min(col_name)).collect()[0][0]\n",
    "```\n",
    "````\n",
    "\n",
    "### Multiple aggregations to every column\n",
    "\n",
    "Multiple aggregations, applied to every non-grouped column\n",
    "\n",
    "group_col = \"animal_group\"\n",
    "````{tabs}\n",
    "```{code-tab} python pandas\n",
    "pdf_agg = pdf.groupby([group_col]).agg([\"min\", \"max\"])\n",
    "\n",
    "```\n",
    "```{code-tab} python pandas\n",
    "kdf_agg = kdf.groupby([group_col]).agg([\"min\", \"max\"])\n",
    "\n",
    "```\n",
    "```{code-tab} python PySpark\n",
    "cols = sdf.columns\n",
    "cols.remove(group_col)\n",
    "sdf_agg = sdf.groupBy(group_col).agg(*[F.min(x) for x in cols], *[F.max(x) for x in cols])\n",
    "```\n",
    "````\n",
    "\n",
    "### Multiple aggregations to specific columns\n",
    "\n",
    "Multiple aggregations, applied to specific columns\n",
    "\n",
    "group_col = \"animal_group\"\n",
    "num_col_1 = \"total_cost\"\n",
    "num_col_2 = \"job_hours\"\n",
    "````{tabs}\n",
    "```{code-tab} python pandas\n",
    "pdf_agg = pdf.groupby([group_col]).agg({num_col_1: [\"sum\"], num_col_2: [\"max\"]})\n",
    "\n",
    "```\n",
    "```{code-tab} python pandas\n",
    "kdf_agg = kdf.groupby([group_col]).agg({num_col_1: [\"sum\"], num_col_2: [\"max\"]})\n",
    "\n",
    "```\n",
    "```{code-tab} python PySpark\n",
    "sdf_agg = sdf.groupBy(group_col).agg(F.sum(num_col_1), F.max(num_col_2))\n",
    "```\n",
    "````\n",
    "\n",
    "### Multiple renamed aggregations\n",
    "\n",
    "Multiple aggregations, specify aggregate column names\n",
    "\n",
    "````{tabs}\n",
    "```{code-tab} python pandas\n",
    "group_col = \"animal_group\"\n",
    "num_col_1 = \"total_cost\"\n",
    "num_col_2 = \"job_hours\"\n",
    "num_col_1_alias = \"total_cost_sum\"\n",
    "num_col_2_alias = \"job_hours_max\"\n",
    "pdf_agg = pdf.groupby([group_col]).agg(num_col_1_alias=(num_col_1, \"sum\"), num_col_2_alias=(num_col_2, \"max\"))\n",
    "\n",
    "```\n",
    "```{code-tab} python pandas\n",
    "group_col = \"animal_group\"\n",
    "num_col_1 = \"total_cost\"\n",
    "num_col_2 = \"job_hours\"\n",
    "num_col_1_alias = \"total_cost_sum\"\n",
    "num_col_2_alias = \"job_hours_max\"\n",
    "kdf_agg = kdf.groupby([group_col]).agg(num_col_1_alias=(num_col_1, \"sum\"), num_col_2_alias=(num_col_2, \"max\"))\n",
    "\n",
    "```\n",
    "```{code-tab} python PySpark\n",
    "group_col = \"animal_group\"\n",
    "num_col_1 = \"total_cost\"\n",
    "num_col_2 = \"job_hours\"\n",
    "num_col_1_alias = \"total_cost_sum\"\n",
    "num_col_2_alias = \"job_hours_max\"\n",
    "sdf_agg = sdf.groupBy(group_col).agg(F.sum(num_col_1).alias(num_col_1_alias), F.max(num_col_2).alias(num_col_2_alias))\n",
    "```\n",
    "````\n",
    "\n",
    "### Pivot table\n",
    "\n",
    "Create an Excel style pivot table\n",
    "\n",
    "index_col = [\"animal_group\", \"origin_of_call\"]\n",
    "pivot_col = \"cal_year\"\n",
    "value_col = [\"total_cost\"]\n",
    "````{tabs}\n",
    "```{code-tab} python pandas\n",
    "# Two methods: on the DataFrame or using pd.pivot_table\n",
    "pdf_pivot = pdf.pivot_table(index=index_col, columns=pivot_col, values=value_col, aggfunc=sum)\n",
    "pdf_pivot = pd.pivot_table(pdf, index=index_col, columns=pivot_col, values=value_col, aggfunc=sum)\n",
    "\n",
    "`````{code-tab} python pandas\n",
    "# Note that ks.pivot_table doesnât exist\n",
    "# aggfunc must be a string\n",
    "kdf_pivot = kdf.pivot_table(index=index_col, columns=pivot_col, values=value_col, aggfunc=\"sum\")\n",
    "\n",
    "``````{code-tab} python PySpark\n",
    "# The list inputs have been unpacked with *\n",
    "sdf_pivot = sdf.groupBy(*index_col).pivot(pivot_col).sum(*value_col)\n",
    "# Unpacking not needed if passing the column names directly\n",
    "sdf_pivot = sdf.groupBy(index_col[0], index_col[1]).pivot(pivot_col).sum(value_col[0])\n",
    "```\n",
    "````\n",
    "\n",
    "## Sorting\n",
    "\n",
    "### Sorting by columns\n",
    "\n",
    "Sort by specified columns. Note that in pandas and Koalas you can use `inplace=True` rather than having to reassign to the variable name.\n",
    "\n",
    "sort_col_1 = \"total_cost\"\n",
    "sort_col_2 = \"incident_number\"\n",
    "````{tabs}\n",
    "```{code-tab} python pandas\n",
    "pdf.sort_values([sort_col_1, sort_col_2], ascending=[False, True], inplace=True)\n",
    "\n",
    "`````{code-tab} python pandas\n",
    "kdf.sort_values([sort_col_1, sort_col_2], ascending=[False, True], inplace=True)\n",
    "\n",
    "``````{code-tab} python PySpark\n",
    "sdf = sdf.sort([sort_col_1, sort_col_2], ascending=[False, True])\n",
    "sdf = sdf.orderBy([sort_col_1, sort_col_2], ascending=[False, True])\n",
    "```\n",
    "````\n",
    "\n",
    "## Combining two DataFrames\n",
    "\n",
    "### Join\n",
    "\n",
    "Join two DataFrames; must be same DataFrame type, e.g. you can't join a PySpark DF and pandas DF without converting one of them first.\n",
    "\n",
    "rows = [[\"Cat\", \"Meow\"],\n",
    "        [\"Dog\", \"Woof\"],\n",
    "        [\"Cow\", \"Moo\"]]\n",
    "\n",
    "columns = [\"animal_group\", \"animal_noise\"]\n",
    "\n",
    "join_col = \"animal_group\"\n",
    "````{tabs}\n",
    "```{code-tab} python pandas\n",
    "pdf_desc = pd.DataFrame(rows, columns=columns)\n",
    "pdf_joined = pdf.merge(pdf_desc, on=[join_col], how=\"left\")\n",
    "\n",
    "`````{code-tab} python pandas\n",
    "kdf_desc = ks.DataFrame(rows, columns=columns)\n",
    "kdf_joined = kdf.merge(kdf_desc, on=[join_col], how=\"left\")\n",
    "\n",
    "``````{code-tab} python PySpark\n",
    "sdf_desc = spark.createDataFrame(rows, columns)\n",
    "sdf_joined = sdf.join(sdf_desc, on=[join_col], how=\"left\")\n",
    "```\n",
    "````\n",
    "\n",
    "### Append/Union All\n",
    "\n",
    "Append a DataFrame to another. Ensure the schema (column names and types) is identical and in the same order in both DataFrames first.\n",
    "\n",
    "# Create another DF for pandas, Koalas and PySpark; note that the _dogs DFs were already created earlier\n",
    "pdf_cats = pdf.loc[pdf[\"animal_group\"] == \"Cat\"]\n",
    "kdf_cats = kdf.loc[kdf[\"animal_group\"] == \"Cat\"]\n",
    "sdf_cats = sdf.filter(F.col(\"animal_group\") == \"Cat\")\n",
    "````{tabs}\n",
    "```{code-tab} python pandas: two possible methods\n",
    "pdf_dogs_and_cats = pdf_dogs.append(pdf_cats)\n",
    "pdf_dogs_and_cats = pd.concat([pdf_dogs, pdf_cats])\n",
    "\n",
    "`````{code-tab} python pandas: two possible methods\n",
    "kdf_dogs_and_cats = kdf_dogs.append(kdf_cats)\n",
    "kdf_dogs_and_cats = ks.concat([kdf_dogs, kdf_cats])\n",
    "\n",
    "``````{code-tab} python PySpark\n",
    "# union will keep duplicates, equivalent to UNION ALL in SQL\n",
    "sdf_dogs_and_cats = sdf_dogs.union(sdf_cats)\n",
    "```\n",
    "````\n",
    "\n",
    "## Comparing DataFrames\n",
    "\n",
    "### DataFrame element-wise equality\n",
    "\n",
    "Compare elements between two DataFrames, return True if identical, False otherwise.\n",
    "\n",
    "This cannot be done in PySpark without a custom function. For whole rows consider `sdf_v2.exceptAll(sdf)`.\n",
    "\n",
    "````{tabs}\n",
    "```{code-tab} python pandas\n",
    "pdf_v2 = pdf.copy()\n",
    "pdf_equal = pdf_v2.eq(pdf)\n",
    "\n",
    "```\n",
    "```{code-tab} python Koalas\n",
    "kdf_v2 = kdf\n",
    "kdf_equal = kdf_v2.eq(kdf)\n",
    "```\n",
    "````\n",
    "\n",
    "### Whole DataFrame equality\n",
    "\n",
    "Compare equality of two DataFrames.\n",
    "\n",
    "This cannot be done in PySpark; see the [Pytest for PySpark repository](http://np2rvlapxx507/DAP_CATS/Training/pytest-for-pyspark) for unit testing examples.\n",
    "\n",
    "Note that the Koalas behaviour for `kdf_equal = kdf_v2.equals(kdf)` gives the same result as `koalas.eq`. Consider converting to PySpark and using the unit testing examples.\n",
    "\n",
    "pdf_v2 = pdf.copy()\n",
    "````{tabs}\n",
    "```{code-tab} python pandas\n",
    "equality_pandas = pdf_v2.equals(pdf)\n",
    "```\n",
    "````\n",
    "\n",
    "## Custom functions\n",
    "\n",
    "### Lambda functions/UDF\n",
    "\n",
    "Apply custom functions.\n",
    "\n",
    "In general try and avoid these and use the in-built functionality wherever possible as it will be far more efficient.\n",
    "\n",
    "Lambda functions should work in Koalas but currently do not at the time of writing.\n",
    "\n",
    "# Define function\n",
    "def job_minutes(job_hours):\n",
    "    return job_hours * 60\n",
    "````{tabs}\n",
    "```{code-tab} python pandas\n",
    "pdf[\"job_minutes\"] = pdf.apply(lambda x: job_minutes(x[\"job_hours\"]), axis=1)\n",
    "\n",
    "``````{code-tab} python PySpark\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Register UDF\n",
    "spark.udf.register(\"job_minutes_udf_reg\", job_minutes)\n",
    "job_minutes_udf = udf(job_minutes, DoubleType())\n",
    "                       \n",
    "sdf = sdf.withColumn(\"job_minutes\", job_minutes_udf(F.col(\"job_hours\")))\n",
    "```\n",
    "````\n",
    "\n",
    "## Other useful operations\n",
    "\n",
    "### Rounding\n",
    "\n",
    "Note that PySpark and pandas round numbers ending in `.5` differently; PySpark and Koalas will round away from zero whereas pandas rounds to the nearest even integer. This is explained in more detail in the [Tip: Rounding differences in Python, R and Spark](http://np2rvlapxx507/DAP_CATS/troubleshooting/tip-of-the-week/-/blob/master/tip_32_rounding.ipynb).\n",
    "\n",
    "````{tabs}\n",
    "```{code-tab} python pandas\n",
    "# Rounds .5 to nearest even integer\n",
    "pdf[\"incident_duration_round\"] = pdf[\"incident_duration\"].round()\n",
    "\n",
    "`````{code-tab} python pandas\n",
    "# Uses the Spark method of rounding (.5 is away from zero)\n",
    "kdf[\"incident_duration_round\"] = kdf[\"incident_duration\"].round()\n",
    "\n",
    "``````{code-tab} python PySpark\n",
    "# Default round for .5 is away from zero, F.bround is idential to pandas\n",
    "sdf = sdf.withColumn(\"incident_duration_round\", F.round(\"incident_duration\"))\n",
    "sdf = sdf.withColumn(\"incident_duration_round\", F.bround(\"incident_duration\"))\n",
    "```\n",
    "````\n",
    "\n",
    "### Close Spark session\n",
    "\n",
    "Disconnect from the Spark session. This is good practice to free up resources on the Spark cluster for other users.\n",
    "\n",
    "````{tabs}\n",
    "```{code-tab} python PySpark and Koalas\n",
    "spark.stop()\n",
    "```\n",
    "````\n",
    "\n",
    "### Further Resources\n",
    "\n",
    "- [10 Minutes to Koalas](https://docs.databricks.com/languages/koalas.html): Introduction to Koalas from Databricks\n",
    "- [Artifactory setup](http://np2rvlapxx507/DAP_CATS/guidance/-/blob/master/Artifactory.md)\n",
    "- [Koalas setup instructions](http://np2rvlapxx507/DAP_CATS/guidance/-/blob/master/koalas_setup.md)\n",
    "- [Pytest for PySpark repository](http://np2rvlapxx507/DAP_CATS/Training/pytest-for-pyspark)\n",
    "- [QA of Code for Analysis and Research](https://best-practice-and-impact.github.io/qa-of-code-guidance/intro.html):\n",
    "    - [Core Programming: Naming Variables](https://best-practice-and-impact.github.io/qa-of-code-guidance/core_programming.html#naming-variables)\n",
    "- [Tip of the Week](http://np2rvlapxx507/DAP_CATS/troubleshooting/tip-of-the-week):\n",
    "    - [Rounding differences in Python, R and Spark](http://np2rvlapxx507/DAP_CATS/troubleshooting/tip-of-the-week/-/blob/master/tip_32_rounding.ipynb)\n",
    "    - [Pydoop](http://np2rvlapxx507/DAP_CATS/troubleshooting/tip-of-the-week/-/blob/master/tip_12_pydoop.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
