

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Shuffling &#8212; Spark at the ONS</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/accessibility.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="../../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-95MGHSRD0S"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-95MGHSRD0S');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'raw-notebooks/shuffling/shuffling';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/logo.png" class="logo__image only-light" alt="Spark at the ONS - Home"/>
    <script>document.write(`<img src="../../_static/logo.png" class="logo__image only-dark" alt="Spark at the ONS - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro.html">
                    Introduction
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Spark overview</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../spark-overview/spark-start.html">Getting Started with Spark</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../spark-overview/when-to-use-spark.html">When To Use Spark</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../spark-overview/spark-session-guidance.html">Guidance on Spark Sessions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../spark-overview/example-spark-sessions.html">Example Spark Sessions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../spark-overview/spark-defaults.html">Configuration Hierarchy and <code class="docutils literal notranslate"><span class="pre">spark-defaults.conf</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../spark-overview/data-types.html">Data Types in Spark</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../spark-overview/creating-dataframes.html">Creating DataFrames Manually</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../spark-overview/reading-and-writing-data-spark.html">Reading and Writing Data in Spark</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../spark-overview/data-storage.html">Data Storage</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction to PySpark</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../pyspark-intro/pyspark-intro.html">Introduction to PySpark</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../pyspark-intro/returning-data.html">Returning Data from Cluster to Driver</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../pyspark-intro/f-col.html">Reference columns by name: <code class="docutils literal notranslate"><span class="pre">F.col()</span></code></a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction to sparklyr</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../sparklyr-intro/sparklyr-intro.html">Introduction to sparklyr</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../sparklyr-intro/sparklyr-functions.html">Using Spark functions in sparklyr</a></li>

</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Spark functions</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../spark-functions/union-dataframes-with-different-columns.html">Union two DataFrames with different columns</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../spark-functions/padding.html">Add leading zeros with <code class="docutils literal notranslate"><span class="pre">lpad()</span></code></a></li>

<li class="toctree-l1"><a class="reference internal" href="../../spark-functions/sampling.html">Sampling: an overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../spark-functions/pivot-tables.html">Pivot tables in Spark</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../spark-functions/rounding.html">Rounding differences in Python, R and Spark</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../spark-functions/window-functions.html">Window Functions in Spark</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../spark-functions/cross-joins.html">Cross Joins</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../spark-functions/arrays.html">Arrays Functions in PySpark</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../spark-functions/writing-data.html">Writing Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../spark-functions/job-description-notebook.html">Set Spark Job Description</a></li>




</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Understanding and Optimising Spark</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../spark-concepts/optimisation-tips.html">Ideas for optimising Spark code</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../spark-concepts/spark-application-and-ui.html">Spark Application and UI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../spark-concepts/shuffling.html">Shuffling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../spark-concepts/join-concepts.html">Optimising Joins</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../spark-concepts/salted-joins.html">Salted Joins</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../spark-concepts/partitions.html">Managing Partitions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../spark-concepts/persistence.html">Persisting in Spark</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../spark-concepts/cache.html">Caching</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../spark-concepts/checkpoint-staging.html">Checkpoints and Staging Tables</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../spark-concepts/garbage-collection.html">Garbage Collection</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../spark-concepts/df-order.html">Spark DataFrames Are Not Ordered</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../spark-concepts/exercises.html">Exercises</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Analysis in Spark</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../spark-analysis/logistic-regression.html">Logistic Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../spark-analysis/visualisation.html">Spark and Visualisation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../spark-analysis/bin-continuous-variable.html">Data binning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../spark-analysis/cramer_v.html">Calculating Cramér’s V from a Spark DataFrame</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Testing and Debugging</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../testing-debugging/spark-errors.html">Understanding and Handling Spark Errors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../testing-debugging/unit-testing.html">Unit Testing in Spark</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Ancillary Topics</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../ancillary-topics/module-imports.html">Naming Conflicts in Module Imports</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ancillary-topics/pydoop.html">Pydoop: HDFS to pandas</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ancillary-topics/creating-a-SQL-view-in-HDFS.html">Creating a SQL view in HDFS</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ancillary-topics/pandas-udfs.html">Pandas UDFs</a></li>
</ul>

        </div>
    </nav></div>
            <div class="bd-sidebar__bottom">
                <!-- To handle the deprecated key -->
                
                <div class="navbar_extra_footer">
                <div>
        <p>Book version 2022.6</p>
    </div>
    
                </div>
                
            </div>
        </div>
        <div id="rtd-footer-container"></div>
    </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/best-practice-and-impact/ons-spark" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/best-practice-and-impact/ons-spark/issues/new?title=Issue%20on%20page%20%2Fraw-notebooks/shuffling/shuffling.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/raw-notebooks/shuffling/shuffling.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Shuffling</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#shuffling-concepts">Shuffling Concepts</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-shuffle">What is a shuffle?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-partition">What is a partition?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-transformations-cause-a-shuffle">What transformations cause a shuffle?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#a-quick-note-on-rdds">A quick note on RDDs</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#examples">Examples</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-1-one-shuffle">Example 1: One Shuffle</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-2-lots-of-shuffles">Example 2: Lots of shuffles</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optimising-and-avoiding-shuffles">Optimising and Avoiding Shuffles</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#minimise-actions-and-caching">Minimise actions and caching</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#converting-to-pandas-or-base-r-df">Converting to pandas or base R DF</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reduce-size-of-dataframe">Reduce size of DataFrame</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#use-a-broadcast-join">Use a broadcast join</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#replace-joins-with-conditional-statements">Replace joins with conditional statements</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#avoiding-unnecessary-sorting">Avoiding unnecessary sorting</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#use-window-functions">Use Window functions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#other-things-matter-too">Other things matter too!</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#good-shuffles">Good Shuffles</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#further-resources">Further Resources</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section id="shuffling">
<h1>Shuffling<a class="headerlink" href="#shuffling" title="Permalink to this heading">#</a></h1>
<p>This article introduces the concept of a <em>shuffle</em>, also referred to as an <em>exchange</em>. Shuffles occur when processing <em>wide transformations</em> such as joins and aggregations. Shuffles can be a bottleneck when processing, so it is good to understand when they occur and how to make them more efficient or minimise their use.</p>
<p>It covers both the theory of shuffles and practical implementations. Several topics are explored in more detail elsewhere and is recommended to read this alongside the <span class="xref myst">Spark Application and UI</span>, <span class="xref myst">Partitions</span>, <span class="xref myst">Optimising Joins</span> and <span class="xref myst">Persisting</span> articles. Some practical suggestions to improve the efficiency of shuffles include <span class="xref myst">broadcast joins</span>, <span class="xref myst">salted joins</span> and <span class="xref myst">window functions</span>.</p>
<section id="shuffling-concepts">
<h2>Shuffling Concepts<a class="headerlink" href="#shuffling-concepts" title="Permalink to this heading">#</a></h2>
<p>First, we will explain a few of the key concepts of shuffles, before moving on to some examples.</p>
<section id="what-is-a-shuffle">
<h3>What is a shuffle?<a class="headerlink" href="#what-is-a-shuffle" title="Permalink to this heading">#</a></h3>
<p>Shuffles in Spark occur when data are moved between different partitions. This is a simultaneous read and write operation and has to be fully processed before parallel processing can resume.</p>
<p>To fully understand this, we need to explore what we mean by <em>partitions</em>, and what <em>transformations</em> cause data to be moved between them.</p>
</section>
<section id="what-is-a-partition">
<h3>What is a partition?<a class="headerlink" href="#what-is-a-partition" title="Permalink to this heading">#</a></h3>
<p>Partitions are just separate chunks of data. Whereas a pandas or base R DataFrame will be stored as one object in the driver memory, a Spark DataFrame will be distributed over many different partitions on the Spark cluster. These partitions are processed in parallel wherever possible. This structure is one of the reasons that Spark is so powerful for handling large data. The Partitions article explores this topic in more detail.</p>
<p>In the diagram below, the solid boxes represent partitions and the arrows are transformations. If the data stays on the same partition that is referred to as as <em>narrow transformation</em>. Shuffles occur between the stages, where the data moves between partitions; this happens when a <em>wide transformation</em> is processed.</p>
<figure class="align-default" id="sparkapplication">
<a class="reference internal image-reference" href="raw-notebooks/images/application_and_ui.png"><img alt="Diagram showing wide and narrow transformations" src="raw-notebooks/images/application_and_ui.png" style="width: 100%;" /></a>
<figcaption>
<p><span class="caption-text">Spark application</span><a class="headerlink" href="#sparkapplication" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>This is covered in more detail in the <span class="xref myst">Spark Application and UI</span> article.</p>
</section>
<section id="what-transformations-cause-a-shuffle">
<h3>What transformations cause a shuffle?<a class="headerlink" href="#what-transformations-cause-a-shuffle" title="Permalink to this heading">#</a></h3>
<p><em>A shuffle takes place when the value of one row depends on another in a different partition</em>, as the partitions of the DataFrame cannot then be processed in parallel. All the previous operations need to have been completed on every partition before a shuffle can take place, and then the shuffle needs to finish before anything else can happen. A good example is sorting, since the data will need to move between partitions to be in the specified order. A few examples of <em>wide transformations</em>:</p>
<ul class="simple">
<li><p>Grouping</p></li>
<li><p>Aggregating</p></li>
<li><p>Sorting</p></li>
<li><p>Joining</p></li>
<li><p>Repartitioning</p></li>
</ul>
</section>
<section id="a-quick-note-on-rdds">
<h3>A quick note on RDDs<a class="headerlink" href="#a-quick-note-on-rdds" title="Permalink to this heading">#</a></h3>
<p>Although in PySpark and sparklyr we work with DataFrames, they are actually stored in memory as a <a class="reference external" href="https://spark.apache.org/docs/latest/rdd-programming-guide.html">Resilient Distributed Dataset (RDD)</a> and this is what you will see in the Spark UI.</p>
<p>In PySpark, a DataFrame is effectively an API built on top of the RDD, allowing it to be manipulated in a more user-friendly manner than the native RDD syntax. We can access RDD specific functionality with <code class="docutils literal notranslate"><span class="pre">df.rdd</span></code>; an example of this is <a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.RDD.getNumPartitions.html"><code class="docutils literal notranslate"><span class="pre">df.rdd.getNumPartitions()</span></code></a> to get the number of partitions of the DataFrame.</p>
<p>In sparklyr we simply have <code class="docutils literal notranslate"><span class="pre">tbl_spark</span></code> objects rather than two APIs, e.g. to get the number of partitions use <a class="reference external" href="https://spark.rstudio.com/packages/sparklyr/latest/reference/sdf_num_partitions.html"><code class="docutils literal notranslate"><span class="pre">sdf_num_partitions()</span></code></a> on your sparklyr DataFrame.</p>
<p>From a practical perspective when coding, do not worry too much about RDDs, the concept of a shuffle works exactly the same way on a DataFrame.</p>
</section>
</section>
<section id="examples">
<h2>Examples<a class="headerlink" href="#examples" title="Permalink to this heading">#</a></h2>
<p>To demonstrate shuffles we will run some Spark code, and see data being transferred between partitions.</p>
<section id="example-1-one-shuffle">
<h3>Example 1: One Shuffle<a class="headerlink" href="#example-1-one-shuffle" title="Permalink to this heading">#</a></h3>
<p>Start with a small example to demonstrate the concept of shuffling and what actually happens to the data and partitions. First, create a Spark session. We are setting the number of partitions returned from a shuffled DataFrame to be just <code class="docutils literal notranslate"><span class="pre">2</span></code> with the <code class="docutils literal notranslate"><span class="pre">spark.sql.shuffle.partitions</span></code> option (the default is <code class="docutils literal notranslate"><span class="pre">200</span></code>).</p>
<p>Note that in sparklyr with a local session <code class="docutils literal notranslate"><span class="pre">spark.sql.local.partitions</span></code> needs to be set; if running on a Spark cluster you can ignore this setting.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span><span class="p">,</span> <span class="n">functions</span> <span class="k">as</span> <span class="n">F</span>

<span class="n">spark</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">SparkSession</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">master</span><span class="p">(</span><span class="s2">&quot;local[2]&quot;</span><span class="p">)</span>
    <span class="o">.</span><span class="n">appName</span><span class="p">(</span><span class="s2">&quot;shuffling&quot;</span><span class="p">)</span>
    <span class="c1"># Set number of partitions after a shuffle to 2</span>
    <span class="o">.</span><span class="n">config</span><span class="p">(</span><span class="s2">&quot;spark.sql.shuffle.partitions&quot;</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">library</span><span class="p">(</span><span class="n">sparklyr</span><span class="p">)</span>
<span class="nf">library</span><span class="p">(</span><span class="n">dplyr</span><span class="p">)</span>

<span class="n">shuffle_config</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">spark_config</span><span class="p">()</span>
<span class="c1"># Set number of partitions after a shuffle to 2</span>
<span class="n">shuffle_config</span><span class="o">$</span><span class="n">spark.sql.shuffle.partitions</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">2</span>
<span class="c1"># Set number of local partitions to 2</span>
<span class="n">shuffle_config</span><span class="o">$</span><span class="n">spark.sql.local.partitions</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">2</span>

<span class="n">sc</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">spark_connect</span><span class="p">(</span>
<span class="w">    </span><span class="n">master</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;local[2]&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="n">app_name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;shuffling&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="n">config</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">shuffle_config</span><span class="p">)</span>
</pre></div>
</div>
<p>As we will be using random numbers in this example we set the seed, so that that repeated runs of the code will not change the results.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">seed_no</span> <span class="o">=</span> <span class="mi">999</span>
</pre></div>
</div>
</div>
</div>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">seed_no</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">999L</span>
</pre></div>
</div>
<p>For this example, create a DataFrame with one column, <code class="docutils literal notranslate"><span class="pre">id</span></code>, 20 rows and two partitions using <a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.SparkSession.range.html"><code class="docutils literal notranslate"><span class="pre">spark.range()</span></code></a>/<a class="reference external" href="https://spark.rstudio.com/packages/sparklyr/latest/reference/sdf_seq.html"><code class="docutils literal notranslate"><span class="pre">sdf_seq()</span></code></a>.</p>
<p>Now look at the DF:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">example_1</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="n">numPartitions</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">example_1</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>+---+
| id|
+---+
|  0|
|  1|
|  2|
|  3|
|  4|
|  5|
|  6|
|  7|
|  8|
|  9|
| 10|
| 11|
| 12|
| 13|
| 14|
| 15|
| 16|
| 17|
| 18|
| 19|
+---+
</pre></div>
</div>
</div>
</div>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">example_1</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">sdf_seq</span><span class="p">(</span><span class="n">sc</span><span class="p">,</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">19</span><span class="p">,</span><span class="w"> </span><span class="n">repartition</span><span class="o">=</span><span class="m">2</span><span class="p">)</span>

<span class="n">example_1</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">    </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">collect</span><span class="p">()</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">    </span><span class="nf">print</span><span class="p">()</span>
</pre></div>
</div>
<p>In order to print out the DataFrame we have to transfer the data back to the driver as one object, so we cannot see the partitioning. Indeed when we write our code we often do not pay attention to how the DataFrame might be distributed in memory.</p>
<p>To see how it is partitioned in Spark add another column using <a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.spark_partition_id.html"><code class="docutils literal notranslate"><span class="pre">F.spark_partition_id()</span></code></a>/<a class="reference external" href="https://spark.apache.org/docs/latest/api/sql/index.html#spark_partition_id"><code class="docutils literal notranslate"><span class="pre">spark_partition_id()</span></code></a>. In PySpark this is from the <code class="docutils literal notranslate"><span class="pre">functions</span></code> module; in sparklyr this is Spark function called inside <code class="docutils literal notranslate"><span class="pre">mutate</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">example_1</span> <span class="o">=</span> <span class="n">example_1</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s2">&quot;partition_id&quot;</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">spark_partition_id</span><span class="p">())</span>
<span class="n">example_1</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>+---+------------+
| id|partition_id|
+---+------------+
|  0|           0|
|  1|           0|
|  2|           0|
|  3|           0|
|  4|           0|
|  5|           0|
|  6|           0|
|  7|           0|
|  8|           0|
|  9|           0|
| 10|           1|
| 11|           1|
| 12|           1|
| 13|           1|
| 14|           1|
| 15|           1|
| 16|           1|
| 17|           1|
| 18|           1|
| 19|           1|
+---+------------+
</pre></div>
</div>
</div>
</div>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">example_1</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">example_1</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">    </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">mutate</span><span class="p">(</span><span class="n">partition_id</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">spark_partition_id</span><span class="p">())</span>

<span class="n">example_1</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">    </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">collect</span><span class="p">()</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">    </span><span class="nf">print</span><span class="p">()</span>
</pre></div>
</div>
<p>We can see that this DataFrame has two partitions: <code class="docutils literal notranslate"><span class="pre">id</span></code> from <code class="docutils literal notranslate"><span class="pre">0</span></code> to <code class="docutils literal notranslate"><span class="pre">9</span></code> are in partition <code class="docutils literal notranslate"><span class="pre">0</span></code>, and <code class="docutils literal notranslate"><span class="pre">10</span></code> to <code class="docutils literal notranslate"><span class="pre">19</span></code> in partition <code class="docutils literal notranslate"><span class="pre">1</span></code>.</p>
<p>Try a transformation on this DataFrame: adding a column of random numbers, <code class="docutils literal notranslate"><span class="pre">rand1</span></code>, between <code class="docutils literal notranslate"><span class="pre">1</span></code> and <code class="docutils literal notranslate"><span class="pre">10</span></code> with - <a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.rand.html"><code class="docutils literal notranslate"><span class="pre">F.rand()</span></code></a>/<a class="reference external" href="https://spark.apache.org/docs/latest/api/sql/index.html#rand"><code class="docutils literal notranslate"><span class="pre">rand()</span></code></a> and <a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.ceil.html"><code class="docutils literal notranslate"><span class="pre">F.ceil()</span></code></a>/<a class="reference external" href="https://spark.apache.org/docs/latest/api/sql/index.html#ceil"><code class="docutils literal notranslate"><span class="pre">ceil()</span></code></a> (which rounds numbers up to the nearest integer), then recalculate the <code class="docutils literal notranslate"><span class="pre">partition_id</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">example_1</span> <span class="o">=</span> <span class="p">(</span><span class="n">example_1</span>
             <span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s2">&quot;rand1&quot;</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">seed_no</span><span class="p">)</span> <span class="o">*</span> <span class="mi">10</span><span class="p">))</span>
             <span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s2">&quot;partition_id&quot;</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">spark_partition_id</span><span class="p">()))</span>

<span class="n">example_1</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>+---+------------+-----+
| id|partition_id|rand1|
+---+------------+-----+
|  0|           0|    9|
|  1|           0|    4|
|  2|           0|    4|
|  3|           0|    8|
|  4|           0|    6|
|  5|           0|    2|
|  6|           0|    2|
|  7|           0|   10|
|  8|           0|    4|
|  9|           0|    3|
| 10|           1|    1|
| 11|           1|    4|
| 12|           1|    9|
| 13|           1|    6|
| 14|           1|    4|
| 15|           1|    9|
| 16|           1|    1|
| 17|           1|    6|
| 18|           1|    9|
| 19|           1|    8|
+---+------------+-----+
</pre></div>
</div>
</div>
</div>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">example_1</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">example_1</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">    </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">mutate</span><span class="p">(</span>
<span class="w">        </span><span class="n">rand1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">ceil</span><span class="p">(</span><span class="nf">rand</span><span class="p">(</span><span class="n">seed_no</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="m">10</span><span class="p">),</span>
<span class="w">        </span><span class="n">partition_id</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">spark_partition_id</span><span class="p">())</span>

<span class="n">example_1</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">    </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">collect</span><span class="p">()</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">    </span><span class="nf">print</span><span class="p">()</span>
</pre></div>
</div>
<p>Comparing this to the previous output, we see that the <code class="docutils literal notranslate"><span class="pre">partition_id</span></code> has not changed. Why? This is because the data did not need to be shuffled between the partitions, as the creation of a column of random numbers is a <em>narrow transformation</em>. This calculation can be done independently on each row and so all partitions can be processed in parallel.</p>
<p>Now try sorting the data by <code class="docutils literal notranslate"><span class="pre">rand1</span></code> in ascending order. Sorting will cause a shuffle, as the data needs to be moved between partitions to get it in the correct order. Then create a column for the <code class="docutils literal notranslate"><span class="pre">partition_id</span></code> after the sort, called <code class="docutils literal notranslate"><span class="pre">partition_id_new</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">example_1</span> <span class="o">=</span> <span class="p">(</span><span class="n">example_1</span>
             <span class="o">.</span><span class="n">orderBy</span><span class="p">(</span><span class="s2">&quot;rand1&quot;</span><span class="p">)</span>
             <span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s2">&quot;partition_id_new&quot;</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">spark_partition_id</span><span class="p">()))</span>

<span class="n">example_1</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>+---+------------+-----+----------------+
| id|partition_id|rand1|partition_id_new|
+---+------------+-----+----------------+
| 10|           1|    1|               0|
| 16|           1|    1|               0|
|  5|           0|    2|               0|
|  6|           0|    2|               0|
|  9|           0|    3|               0|
|  1|           0|    4|               0|
|  2|           0|    4|               0|
|  8|           0|    4|               0|
| 11|           1|    4|               0|
| 14|           1|    4|               0|
|  4|           0|    6|               1|
| 13|           1|    6|               1|
| 17|           1|    6|               1|
|  3|           0|    8|               1|
| 19|           1|    8|               1|
|  0|           0|    9|               1|
| 12|           1|    9|               1|
| 15|           1|    9|               1|
| 18|           1|    9|               1|
|  7|           0|   10|               1|
+---+------------+-----+----------------+
</pre></div>
</div>
</div>
</div>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">example_1</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">example_1</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">    </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">sdf_sort</span><span class="p">(</span><span class="s">&quot;rand1&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">    </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">mutate</span><span class="p">(</span><span class="n">partition_id_new</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">spark_partition_id</span><span class="p">())</span>

<span class="n">example_1</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">    </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">collect</span><span class="p">()</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">    </span><span class="nf">print</span><span class="p">()</span>
</pre></div>
</div>
<p>We can see that some data has moved between the partitions. Specifically, any row where <code class="docutils literal notranslate"><span class="pre">partition_id</span></code> <span class="math notranslate nohighlight">\( \neq\)</span> <code class="docutils literal notranslate"><span class="pre">partition_id_new</span></code>. Note that not all the data has changed partitions; some are already on the correct partition prior to the sorting.</p>
<p>As we specified <code class="docutils literal notranslate"><span class="pre">spark.sql.shuffle.partitions</span></code> to be  <code class="docutils literal notranslate"><span class="pre">2</span></code> in the config, the shuffle has returned two partitions. In practical usage you will almost never only use two partitions, but this principle applies regardless of the size of the DF or number of partitions.</p>
<p>Another way to see when a shuffle has occurred is to check the Spark UI, where a shuffle is referred to as an <strong>Exchange</strong>. This is covered in more detail in the <span class="xref myst">Spark Application and UI</span> article. In a local Spark session, the URL is <a class="reference external" href="http://localhost:4040/">http://localhost:4040/</a>.</p>
<p>There are different visualisations available in the Spark UI; here we will choose SQL. There should be four completed queries, which relate to the four actions we have called to this point in the Spark session.</p>
<figure class="align-default" id="completedsqlqueries">
<a class="reference internal image-reference" href="raw-notebooks/images/shuffling_example1_ui_list.png"><img alt="List of completed SQL queries in Spark UI" src="raw-notebooks/images/shuffling_example1_ui_list.png" style="width: 60%;" /></a>
<figcaption>
<p><span class="caption-text">Completed SQL queries</span><a class="headerlink" href="#completedsqlqueries" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Clicking on the link where <code class="docutils literal notranslate"><span class="pre">ID</span></code> is <code class="docutils literal notranslate"><span class="pre">3</span></code> displays the plan:</p>
<figure class="align-default" id="exchangeexample">
<a class="reference internal image-reference" href="raw-notebooks/images/shuffling_example1_sql_ui.png"><img alt="Spark UI showing an exchange when DF is sorted" src="raw-notebooks/images/shuffling_example1_sql_ui.png" style="width: 50%;" /></a>
<figcaption>
<p><span class="caption-text">Exchange in Spark UI representing a shuffle</span><a class="headerlink" href="#exchangeexample" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>We can see from this that the shuffle occurred when we sorted the DataFrame by <code class="docutils literal notranslate"><span class="pre">rand1</span></code>, as represented by <strong>Exchange</strong> in the diagram.</p>
</section>
<section id="example-2-lots-of-shuffles">
<h3>Example 2: Lots of shuffles<a class="headerlink" href="#example-2-lots-of-shuffles" title="Permalink to this heading">#</a></h3>
<p>The previous example just had one shuffle on a tiny DF; this one has several shuffles on a larger DF, although still small by Spark standards. We will close the existing Spark session and start a new one, as the previous session had <code class="docutils literal notranslate"><span class="pre">spark.sql.shuffle.partitions</span></code> set to <code class="docutils literal notranslate"><span class="pre">2</span></code>; here, we want <code class="docutils literal notranslate"><span class="pre">100</span></code>. Be careful when closing and starting Spark sessions as existing values will be used if not explicitly defined in the new configuration.</p>
<p>Note that this only takes effect once we have shuffled the DataFrame, for more information on how newly created DataFrames are partitioned please refer to the Partitions article.</p>
<p>By default, Spark will try and use the broadcast method for joins. We want to ensure that we demonstrate the behaviour of the regular join (sort merge join) and so are deactivating automatic broadcast joins in the configuration by setting <code class="docutils literal notranslate"><span class="pre">spark.sql.autoBroadcastJoinThreshold</span></code> to <code class="docutils literal notranslate"><span class="pre">-1</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">spark</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>

<span class="n">spark</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">SparkSession</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">master</span><span class="p">(</span><span class="s2">&quot;local[2]&quot;</span><span class="p">)</span>
    <span class="o">.</span><span class="n">appName</span><span class="p">(</span><span class="s2">&quot;shuffling&quot;</span><span class="p">)</span>
    <span class="c1"># Set number of partitions after a shuffle to 100</span>
    <span class="o">.</span><span class="n">config</span><span class="p">(</span><span class="s2">&quot;spark.sql.shuffle.partitions&quot;</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
    <span class="c1"># Disable broadcast join by default</span>
    <span class="o">.</span><span class="n">config</span><span class="p">(</span><span class="s2">&quot;spark.sql.autoBroadcastJoinThreshold&quot;</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">spark_disconnect</span><span class="p">(</span><span class="n">sc</span><span class="p">)</span>

<span class="n">shuffle_config</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">spark_config</span><span class="p">()</span>
<span class="c1"># Set number of partitions after a shuffle to 100</span>
<span class="n">shuffle_config</span><span class="o">$</span><span class="n">spark.sql.shuffle.partitions</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">100</span>
<span class="c1"># Set number of local partitions to 100</span>
<span class="n">shuffle_config</span><span class="o">$</span><span class="n">spark.sql.local.partitions</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">100</span>
<span class="c1"># Disable broadcast join by default</span>
<span class="n">shuffle_config</span><span class="o">$</span><span class="n">spark.sql.autoBroadcastJoinThreshold</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">-1</span>

<span class="n">sc</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">spark_connect</span><span class="p">(</span>
<span class="w">    </span><span class="n">master</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;local[2]&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="n">app_name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;shuffling&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="n">config</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">shuffle_config</span><span class="p">)</span>
</pre></div>
</div>
<p>Here we create two DataFrames of <span class="math notranslate nohighlight">\(100,000\)</span> rows and add a random number between <code class="docutils literal notranslate"><span class="pre">1</span></code> to <code class="docutils literal notranslate"><span class="pre">10</span> </code>for each called <code class="docutils literal notranslate"><span class="pre">rand1</span></code> and <code class="docutils literal notranslate"><span class="pre">rand2</span></code> respectively, using the same method as previously.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create two DataFrames</span>
<span class="n">example_2A</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="mi">10</span><span class="o">**</span><span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s2">&quot;rand1&quot;</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">seed_no</span><span class="p">)</span> <span class="o">*</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">example_2B</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="mi">10</span><span class="o">**</span><span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s2">&quot;rand2&quot;</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">seed_no</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="mi">10</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">example_2A</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">sdf_seq</span><span class="p">(</span><span class="n">sc</span><span class="p">,</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">10</span><span class="o">**</span><span class="m">5</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="m">1</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">    </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">mutate</span><span class="p">(</span><span class="n">rand1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">ceil</span><span class="p">(</span><span class="nf">rand</span><span class="p">(</span><span class="n">seed_no</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="m">10</span><span class="p">))</span>
<span class="w">           </span>
<span class="n">example_2B</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">sdf_seq</span><span class="p">(</span><span class="n">sc</span><span class="p">,</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">10</span><span class="o">**</span><span class="m">5</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="m">1</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">    </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">mutate</span><span class="p">(</span><span class="n">rand2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">ceil</span><span class="p">(</span><span class="nf">rand</span><span class="p">(</span><span class="n">seed_no</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="m">1L</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="m">10</span><span class="p">))</span>
</pre></div>
</div>
<p>We now want to perform some transformations which involve a shuffle. Let’s create a new DataFrame, <code class="docutils literal notranslate"><span class="pre">joined_df</span></code>, by joining <code class="docutils literal notranslate"><span class="pre">example_2B</span></code> to <code class="docutils literal notranslate"><span class="pre">example_2A</span></code>. We then group by <code class="docutils literal notranslate"><span class="pre">rand1</span></code> and <code class="docutils literal notranslate"><span class="pre">rand2</span></code>, then sort by <code class="docutils literal notranslate"><span class="pre">rand1</span></code> and <code class="docutils literal notranslate"><span class="pre">rand2</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">joined_df</span> <span class="o">=</span> <span class="p">(</span><span class="n">example_2A</span><span class="o">.</span>
             <span class="n">join</span><span class="p">(</span><span class="n">example_2B</span><span class="p">,</span> <span class="n">on</span><span class="o">=</span><span class="s2">&quot;id&quot;</span><span class="p">,</span> <span class="n">how</span><span class="o">=</span><span class="s2">&quot;left&quot;</span><span class="p">)</span>
             <span class="o">.</span><span class="n">groupBy</span><span class="p">(</span><span class="s2">&quot;rand1&quot;</span><span class="p">,</span> <span class="s2">&quot;rand2&quot;</span><span class="p">)</span>
             <span class="o">.</span><span class="n">agg</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">count</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">col</span><span class="p">(</span><span class="s2">&quot;id&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&quot;row_count&quot;</span><span class="p">))</span>
             <span class="o">.</span><span class="n">orderBy</span><span class="p">(</span><span class="s2">&quot;rand1&quot;</span><span class="p">,</span> <span class="s2">&quot;rand2&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">joined_df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">example_2A</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">    </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">left_join</span><span class="p">(</span><span class="n">example_2B</span><span class="p">,</span><span class="w"> </span><span class="n">by</span><span class="o">=</span><span class="s">&quot;id&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">    </span><span class="n">dplyr</span><span class="o">::</span><span class="nf">group_by</span><span class="p">(</span><span class="n">rand1</span><span class="p">,</span><span class="w"> </span><span class="n">rand2</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">    </span><span class="n">dplyr</span><span class="o">::</span><span class="nf">summarise</span><span class="p">(</span><span class="n">row_count</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">n</span><span class="p">())</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">    </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">sdf_sort</span><span class="p">(</span><span class="nf">c</span><span class="p">(</span><span class="s">&quot;rand1&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;rand2&quot;</span><span class="p">))</span>
</pre></div>
</div>
<p>For the action, we are calling a <code class="docutils literal notranslate"><span class="pre">collect</span></code>-style operation (<code class="docutils literal notranslate"><span class="pre">.toPandas()</span></code>/<code class="docutils literal notranslate"><span class="pre">collect()</span></code>) rather than <code class="docutils literal notranslate"><span class="pre">.show(n)</span></code> (PySpark) or <code class="docutils literal notranslate"><span class="pre">head(n)</span> <span class="pre">%&gt;%</span> <span class="pre">collect()</span></code> (sparklyr). The reason is <code class="docutils literal notranslate"><span class="pre">.show(n)</span></code>/<code class="docutils literal notranslate"><span class="pre">head(n)</span></code> will only do the minimum required to return <code class="docutils literal notranslate"><span class="pre">n</span></code> rows, whereas <code class="docutils literal notranslate"><span class="pre">.toPandas()</span></code>/<code class="docutils literal notranslate"><span class="pre">collect()</span></code> causes all the data to be collected to the driver from the Spark cluster, forcing a full calculation.</p>
<p>Then sanity check the result, by getting the row count and the top and bottom 5 rows:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">collected_df</span> <span class="o">=</span> <span class="n">joined_df</span><span class="o">.</span><span class="n">toPandas</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Row count: &quot;</span><span class="p">,</span> <span class="n">collected_df</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Top 5 rows:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">collected_df</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Bottom 5 rows:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">collected_df</span><span class="o">.</span><span class="n">tail</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Row count:  100

Top 5 rows:
    rand1  rand2  row_count
0      1      1        969
1      1      2        988
2      1      3       1023
3      1      4       1033
4      1      5        976

Bottom 5 rows:
     rand1  rand2  row_count
95     10      6        963
96     10      7       1004
97     10      8       1023
98     10      9        989
99     10     10        959
</pre></div>
</div>
</div>
</div>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">collected_df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">joined_df</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">    </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">collect</span><span class="p">()</span>

<span class="nf">print</span><span class="p">(</span><span class="s">&quot;Row count: &quot;</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="nf">nrow</span><span class="p">(</span><span class="n">collected_df</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="s">&quot;Top 5 rows:&quot;</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="nf">head</span><span class="p">(</span><span class="n">collected_df</span><span class="p">,</span><span class="w"> </span><span class="m">5</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="s">&quot;Bottom 5 rows:&quot;</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="nf">tail</span><span class="p">(</span><span class="n">collected_df</span><span class="p">,</span><span class="w"> </span><span class="m">5</span><span class="p">))</span>
</pre></div>
</div>
<p>The result is as expected.</p>
<p>Rather than try and print out the <code class="docutils literal notranslate"><span class="pre">partition_id</span></code> like we did in the first example, here the Spark UI is much more informative:</p>
<figure class="align-default" id="multipleexchanges">
<a class="reference internal image-reference" href="raw-notebooks/images/shuffling_example2_sql_ui.png"><img alt="Spark UI showing exchanges when a DF is joined, grouped and sorted" src="raw-notebooks/images/shuffling_example2_sql_ui.png" style="width: 100%;" /></a>
<figcaption>
<p><span class="caption-text">Multiple exchanges</span><a class="headerlink" href="#multipleexchanges" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>We can visually see where a shuffle takes place, in this case, on both DataFrames prior to the join, then when grouping, and finally one more when sorting at the end.</p>
<p>A quick note on the shuffles prior to the join: Spark uses sort merge join, which requires a shuffle of the DataFrames before performing the join, hence the initial <strong>Exchange</strong> and <strong>Sort</strong> in the diagram. The other joining algorithm is the broadcast join, briefly covered later and in full in the Join Concepts article.</p>
</section>
</section>
<section id="optimising-and-avoiding-shuffles">
<h2>Optimising and Avoiding Shuffles<a class="headerlink" href="#optimising-and-avoiding-shuffles" title="Permalink to this heading">#</a></h2>
<p>Now we have seen some examples of shuffles, let us explain properly why we generally want to minimise unnecessary use of them.</p>
<p>The shuffle itself is an expensive operation, by which we mean it takes a long time to run compared to most other operations. In addition, when the data are being shuffled, all prior operations have to complete first. This is why the steps in the Spark UI are referred to as <em>stages</em>; all the processing in one stage has to be completed before the next one can start.</p>
<p>As such shuffles can cause bottlenecks in our code and slow it down.</p>
<p>It is very unlikely that you will be able to avoid using shuffles completely. Grouping, aggregations, joining and ordering are almost essential when writing Spark codes. So the important thing is to <em>understand</em> shuffles and use them sensibly, rather than being afraid of them!</p>
<section id="minimise-actions-and-caching">
<h3>Minimise actions and caching<a class="headerlink" href="#minimise-actions-and-caching" title="Permalink to this heading">#</a></h3>
<p>Remember how Spark works: it is a set of transformations, which are triggered by an action. The action will process the entire Spark plan. You could have written your code very efficiently to minimise the number of shuffles, but if you are calling repeated actions then these shuffles will also be repeated.</p>
<p>During development you may have actions to preview the data (e.g. <code class="docutils literal notranslate"><span class="pre">.show()</span></code>/<code class="docutils literal notranslate"><span class="pre">glimpse()</span></code>) or to get the row count (<code class="docutils literal notranslate"><span class="pre">.count()</span></code>/<code class="docutils literal notranslate"><span class="pre">sdf_nrow()</span></code>) at interim parts of the code, to understand how your DataFrame is changing and to help debug the code. Before repeatedly running analysis or releasing production code you will want to eliminate any of these superfluous actions.</p>
<p>You can make careful use of persisting in these situations, such as <span class="xref myst">caching</span>, <span class="xref myst">checkpointing or using staging tables</span>; see the <span class="xref myst">persisting</span> article for more detail.</p>
<p>In addition, Spark does have some efficiencies in the way repeated actions are handled but in general, try and minimise the number of actions that you call if they are not needed.</p>
</section>
<section id="converting-to-pandas-or-base-r-df">
<h3>Converting to pandas or base R DF<a class="headerlink" href="#converting-to-pandas-or-base-r-df" title="Permalink to this heading">#</a></h3>
<p>If your DataFrame is small, you can discard the concept of distributed computing and shuffling entirely by converting it into a pandas or base R DataFrame and process it in the driver memory rather than in Spark. Operations which would normally trigger a shuffle in Spark will be instead processed in the driver memory.</p>
<p>Converting PySpark DataFrames to pandas is very easy as they have a <a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.toPandas.html"><code class="docutils literal notranslate"><span class="pre">.toPandas()</span></code></a> method. In sparklyr, use <a class="reference external" href="https://dplyr.tidyverse.org/reference/compute.html"><code class="docutils literal notranslate"><span class="pre">sparklyr::collect()</span></code></a>; this will convert to the standard dplyr-style tibble, which are mostly interchangeable with base R DFs.</p>
<p>In PySpark you can even skip Spark entirely by reading in from HDFS to pandas with <span class="xref myst">Pydoop</span>.</p>
<p>Be careful; if your DF is large, there may not be enough driver memory to process it and you will get an error. Choosing between Spark and pandas/base R is something that should be done at the start of a project and should be carefully considered; if you choose to use the driver memory and find that there is insufficient capacity then you will have to re-write your code in Spark, which is often not a trivial task. See the <span class="xref myst">When To Use Spark</span> article for more information.</p>
</section>
<section id="reduce-size-of-dataframe">
<h3>Reduce size of DataFrame<a class="headerlink" href="#reduce-size-of-dataframe" title="Permalink to this heading">#</a></h3>
<p>The larger the DataFrame that is supplied to Spark the longer the Spark job will take. If it is possible to reduce the size of the DataFrame then most operations on the DF should be more efficient, including shuffling. An example could be moving a filter operation to earlier in the code. Note that Spark does have some optimisation in terms of orders of operations, but where possible try and work with smaller DFs if possible.</p>
</section>
<section id="use-a-broadcast-join">
<h3>Use a broadcast join<a class="headerlink" href="#use-a-broadcast-join" title="Permalink to this heading">#</a></h3>
<p>As we saw in the second example, a join will cause a shuffle on the DataFrames to be joined. We saw that this was referred to as a <span class="xref myst"><strong>SortMergeJoin</strong></span> in the visualised plan. There is however another type of join: the <span class="xref myst">broadcast join</span>. A broadcast join can be used when one of your DataFrames in the join is small; a copy is created on each partition. This can then be processed separately on each partition in parallel, avoiding the need for a full shuffle.</p>
<p>Broadcast joins are one of the easiest ways to improve the efficiency of your code, as you just have to wrap the second DF in <a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.broadcast.html"><code class="docutils literal notranslate"><span class="pre">F.broadcast()</span></code></a> in PySpark or <a class="reference external" href="https://spark.rstudio.com/packages/sparklyr/latest/reference/sdf_broadcast.html"><code class="docutils literal notranslate"><span class="pre">sdf_broadcast()</span></code></a>
in sparklyr.</p>
<p>The <span class="xref myst">Optimising Joins</span> article has full details; this is an example of the Spark UI from that article, showing that there are fewer shuffles involved when broadcasting:</p>
<figure class="align-default" id="broadcastexchange">
<a class="reference internal image-reference" href="raw-notebooks/images/broadcast_join_ui.png"><img alt="Spark UI for broadcast join, showing a broadcast exchange" src="raw-notebooks/images/broadcast_join_ui.png" style="width: 100%;" /></a>
<figcaption>
<p><span class="caption-text">Boradcast exchange</span><a class="headerlink" href="#broadcastexchange" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="replace-joins-with-conditional-statements">
<h3>Replace joins with conditional statements<a class="headerlink" href="#replace-joins-with-conditional-statements" title="Permalink to this heading">#</a></h3>
<p>If you are joining a tiny DataFrame of only a few rows, a shuffle can potentially be avoided entirely be re-writing the join as a set of conditional statements with <a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.when.html"><code class="docutils literal notranslate"><span class="pre">F.when()</span></code></a> in PySpark or <a class="reference external" href="https://dplyr.tidyverse.org/reference/case_when.html"><code class="docutils literal notranslate"><span class="pre">case_when()</span></code></a>
in sparklyr; although this method can be more prone to errors when coding and harder to maintain. See the <span class="xref myst">Optimising Joins: Replacing a join with a narrow transformation</span> section for more information.</p>
</section>
<section id="avoiding-unnecessary-sorting">
<h3>Avoiding unnecessary sorting<a class="headerlink" href="#avoiding-unnecessary-sorting" title="Permalink to this heading">#</a></h3>
<p>There can be the temptation to put a lot of <a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.orderBy.html"><code class="docutils literal notranslate"><span class="pre">.orderBy()</span></code></a>/<a class="reference external" href="https://spark.rstudio.com/packages/sparklyr/latest/reference/sdf_sort.html"><code class="docutils literal notranslate"><span class="pre">sdf_sort()</span></code></a> statements in your code when developing, especially if you are previewing the data at various points (e.g. with <code class="docutils literal notranslate"><span class="pre">show(n)</span></code>/<code class="docutils literal notranslate"><span class="pre">head(n)</span> <span class="pre">%&gt;%</span> <span class="pre">collect()</span></code>). Remember that our Spark DataFrames are distributed into partitions and so the concept of order is different to when we have these stored as one object (e.g. a pandas or R DataFrame) in the driver memory. Generally the order of the rows in the DataFrame should not matter; you can often just sort the data once right at the end of the code before writing out the results if the order of the output is important.</p>
<p>This does come with a caveat in that Spark sometimes can be cleverer than we expect, due to the <em>catalyst optimizer</em>. As we know, the Spark plan is only triggered when an action is called, such as <code class="docutils literal notranslate"><span class="pre">.show()</span></code>, <code class="docutils literal notranslate"><span class="pre">collect()</span></code> etc. Spark will sometimes however detect unnecessary sorting and optimise the plan. We can demonstrate this by sorting our DataFrame several times, then looking at the UI.</p>
<p>In sparklyr, <code class="docutils literal notranslate"><span class="pre">sdf_sort()</span></code> can only sort ascending, so a temporary column of negative values is created, then dropped at the end. <a class="reference external" href="https://dplyr.tidyverse.org/reference/arrange.html"><code class="docutils literal notranslate"><span class="pre">arrange()</span></code></a> can sort descending, although this is only triggered if collected immediately or used in conjunction with <code class="docutils literal notranslate"><span class="pre">head()</span></code>.</p>
<p>Note that the code below is being re-assigned on each line rather than being chained; this is to demonstrate that it is not the chaining which causes Spark to optimise the code.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">unsorted_df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="mi">10</span> <span class="o">**</span> <span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s2">&quot;rand1&quot;</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">seed_no</span><span class="p">)</span> <span class="o">*</span> <span class="mi">10</span><span class="p">))</span>

<span class="n">sorted_df</span> <span class="o">=</span> <span class="n">unsorted_df</span><span class="o">.</span><span class="n">orderBy</span><span class="p">(</span><span class="s2">&quot;rand1&quot;</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">sorted_df</span> <span class="o">=</span> <span class="n">sorted_df</span><span class="o">.</span><span class="n">orderBy</span><span class="p">(</span><span class="s2">&quot;rand1&quot;</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">sorted_df</span> <span class="o">=</span> <span class="n">sorted_df</span><span class="o">.</span><span class="n">orderBy</span><span class="p">(</span><span class="s2">&quot;rand1&quot;</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">sorted_df</span> <span class="o">=</span> <span class="n">sorted_df</span><span class="o">.</span><span class="n">orderBy</span><span class="p">(</span><span class="s2">&quot;rand1&quot;</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">sorted_collected_df</span> <span class="o">=</span> <span class="n">sorted_df</span><span class="o">.</span><span class="n">toPandas</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Row count: &quot;</span><span class="p">,</span> <span class="n">sorted_collected_df</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Top 5 rows:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">sorted_collected_df</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Bottom 5 rows:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">sorted_collected_df</span><span class="o">.</span><span class="n">tail</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Row count:  100000

Top 5 rows:
    id  rand1
0   7     10
1  19     10
2  21     10
3  24     10
4  58     10

Bottom 5 rows:
           id  rand1
99995  99930      1
99996  99936      1
99997  99947      1
99998  99963      1
99999  99975      1
</pre></div>
</div>
</div>
</div>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">unsorted_df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">sdf_seq</span><span class="p">(</span><span class="n">sc</span><span class="p">,</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">10</span><span class="o">**</span><span class="m">5</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="m">1</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">    </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">mutate</span><span class="p">(</span><span class="n">rand1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">ceil</span><span class="p">(</span><span class="nf">rand</span><span class="p">(</span><span class="n">seed_no</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="m">10</span><span class="p">),</span>
<span class="w">                     </span><span class="c1"># Create temporary column for sorting descending</span>
<span class="w">                     </span><span class="n">rand2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">rand1</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="m">-1</span><span class="p">)</span>

<span class="n">sorted_df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">unsorted_df</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">sdf_sort</span><span class="p">(</span><span class="s">&quot;rand1&quot;</span><span class="p">)</span>
<span class="n">sorted_df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sorted_df</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">sdf_sort</span><span class="p">(</span><span class="s">&quot;rand2&quot;</span><span class="p">)</span>
<span class="n">sorted_df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sorted_df</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">sdf_sort</span><span class="p">(</span><span class="s">&quot;rand1&quot;</span><span class="p">)</span>
<span class="n">sorted_df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sorted_df</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">sdf_sort</span><span class="p">(</span><span class="s">&quot;rand2&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">    </span><span class="c1"># Remove temporary column used to sort descending</span>
<span class="w">    </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">select</span><span class="p">(</span><span class="o">-</span><span class="n">rand2</span><span class="p">)</span>

<span class="n">sorted_collected_df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sorted_df</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">    </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">collect</span><span class="p">()</span>

<span class="nf">print</span><span class="p">(</span><span class="s">&quot;Row count: &quot;</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="nf">nrow</span><span class="p">(</span><span class="n">sorted_collected_df</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="s">&quot;Top 5 rows:&quot;</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="nf">head</span><span class="p">(</span><span class="n">sorted_collected_df</span><span class="p">,</span><span class="w"> </span><span class="m">5</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="s">&quot;Bottom 5 rows:&quot;</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="nf">tail</span><span class="p">(</span><span class="n">sorted_collected_df</span><span class="p">,</span><span class="w"> </span><span class="m">5</span><span class="p">))</span>
</pre></div>
</div>
<figure class="align-default" id="multiplesortssingleexchange">
<a class="reference internal image-reference" href="raw-notebooks/images/shuffling_catalyst_ui.png"><img alt="Spark UI showing one exchange for multiple DF sorting operations" src="raw-notebooks/images/shuffling_catalyst_ui.png" style="width: 50%;" /></a>
<figcaption>
<p><span class="caption-text">Multiple sorts in single exchange</span><a class="headerlink" href="#multiplesortssingleexchange" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>On the plan, we only have one exchange, but we gave it four operations which should cause a shuffle. What has happened is that Spark has optimised the plan to only include one shuffle. This is a feature of Spark called the <em>catalyst optimizer</em> and is explained further in the <span class="xref myst">Persisting</span> article.</p>
<p>We can demonstrate this by looking at the full plan with <a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.explain.html"><code class="docutils literal notranslate"><span class="pre">.explain(True)</span></code></a>
in PySpark and with <a class="reference external" href="https://spark.rstudio.com/packages/sparklyr/latest/reference/invoke.html"><code class="docutils literal notranslate"><span class="pre">invoke(&quot;queryExecution&quot;)</span></code></a> in sparklyr:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sorted_df</span><span class="o">.</span><span class="n">explain</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>== Parsed Logical Plan ==
&#39;Sort [&#39;rand1 DESC NULLS LAST], true
+- Sort [rand1#77L ASC NULLS FIRST], true
   +- Sort [rand1#77L DESC NULLS LAST], true
      +- Sort [rand1#77L ASC NULLS FIRST], true
         +- Project [id#75L, CEIL((rand(999) * cast(10 as double))) AS rand1#77L]
            +- Range (0, 100000, step=1, splits=Some(2))

== Analyzed Logical Plan ==
id: bigint, rand1: bigint
Sort [rand1#77L DESC NULLS LAST], true
+- Sort [rand1#77L ASC NULLS FIRST], true
   +- Sort [rand1#77L DESC NULLS LAST], true
      +- Sort [rand1#77L ASC NULLS FIRST], true
         +- Project [id#75L, CEIL((rand(999) * cast(10 as double))) AS rand1#77L]
            +- Range (0, 100000, step=1, splits=Some(2))

== Optimized Logical Plan ==
Sort [rand1#77L DESC NULLS LAST], true
+- Project [id#75L, CEIL((rand(999) * 10.0)) AS rand1#77L]
   +- Range (0, 100000, step=1, splits=Some(2))

== Physical Plan ==
*(2) Sort [rand1#77L DESC NULLS LAST], true, 0
+- Exchange rangepartitioning(rand1#77L DESC NULLS LAST, 100)
   +- *(1) Project [id#75L, CEIL((rand(999) * 10.0)) AS rand1#77L]
      +- *(1) Range (0, 100000, step=1, splits=2)
</pre></div>
</div>
</div>
</div>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">sorted_df</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">    </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">spark_dataframe</span><span class="p">()</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">    </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">invoke</span><span class="p">(</span><span class="s">&quot;queryExecution&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>The key output here is the difference between the <strong>Parsed</strong> and <strong>Analyzed Logical Plans</strong>, and the <strong>Optimized Logical Plan</strong>. We can see in the first two that all our sorting operations are present, but in the <strong>Optimized</strong>, only the final sort is. Hence we only have one shuffle.</p>
</section>
<section id="use-window-functions">
<h3>Use Window functions<a class="headerlink" href="#use-window-functions" title="Permalink to this heading">#</a></h3>
<p>One way to reduce the number of shuffles is to ensure that you are using window functions where possible. One example is deriving a new column which depends on the total of a group. One way is to create a new DataFrame with the group totals, then join that back to the original DataFrame, then use this in the calculation. This will involve shuffles for the grouping/aggregation and the join, whereas a window function will only need one.</p>
<p>Another benefit is that the code is neater and easier to read with a window function. See the article on <span class="xref myst">Window Functions</span> for more information.</p>
</section>
<section id="other-things-matter-too">
<h3>Other things matter too!<a class="headerlink" href="#other-things-matter-too" title="Permalink to this heading">#</a></h3>
<p>A final word on avoiding shuffles: do not let this distract you from everything else that you need to consider when writing good code! While it is important to understand shuffles and to try and implement the methods above where you can, it is better to have code which works, then optimise it afterwards. Also remember that with code there is a never a <em>final</em> version: provided it is well written, appropriately commented and has good unit tests, you or a colleague can always re-visit it to optimise it at a later date.</p>
<p>Some shuffles can actually be relatively quick, depending on factors including the total size of the DataFrame, the number of partitions, and how it is partitioned. It can be worth checking the Spark UI to see exactly how much time your shuffle is taking.</p>
<p>Although removing or optimising shuffles is one of the most obvious ways to improve the efficiency of your code, when looking at ways to improve the performance of your code make sure you look at other elements too.</p>
</section>
</section>
<section id="good-shuffles">
<h2>Good Shuffles<a class="headerlink" href="#good-shuffles" title="Permalink to this heading">#</a></h2>
<p>We’ve seen how shuffles take time to process and can cause a bottleneck in performance. However, this is not always the case.</p>
<p>Sometimes shuffling our data into more logical partitions can actually cause performance to increase. For instance, if our data are skewed so that one partition contains significantly more data than another, the parallelism will not be as efficient as it could be. By shuffling the data into partitions of similar sizes, the efficiency will actually increase. An example is using a salted join to reduce skew when joining DataFrames.</p>
<p>You can also change the number of partitions. By default, this is <code class="docutils literal notranslate"><span class="pre">200</span></code>, but repartitioning into a greater or smaller number depending on your data and what you are doing with it can help performance. Note that when repartitioning a DataFrame, <a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.repartition.html"><code class="docutils literal notranslate"><span class="pre">.repartition()</span></code></a>/<a class="reference external" href="https://spark.rstudio.com/packages/sparklyr/latest/reference/sdf_repartition.html"><code class="docutils literal notranslate"><span class="pre">sdf_repartition()</span></code></a> will cause a full shuffle into roughly equal partition sizes, whereas <a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.coalesce.html"><code class="docutils literal notranslate"><span class="pre">.coalesce()</span></code></a>/<a class="reference external" href="https://spark.rstudio.com/packages/sparklyr/latest/reference/sdf_coalesce.html"><code class="docutils literal notranslate"><span class="pre">sdf_coalesce()</span></code></a>  will essentially combine partitions (and so they may vary in size), so the data will still move but be processed more efficiently. This topic is explored more in the <span class="xref myst">Partitions</span> article.</p>
</section>
<section id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Permalink to this heading">#</a></h2>
<p>In this article we have demonstrated:</p>
<ul class="simple">
<li><p>What a shuffle is and why they are essential in a distributed environment</p></li>
<li><p>When they occur</p></li>
<li><p>Why they can be a bottleneck on performance</p></li>
<li><p>How to optimise and minimise them in your code</p></li>
<li><p>Why they can sometimes be beneficial</p></li>
</ul>
</section>
<section id="further-resources">
<h2>Further Resources<a class="headerlink" href="#further-resources" title="Permalink to this heading">#</a></h2>
<p>Spark at the ONS Articles:</p>
<ul class="simple">
<li><p><span class="xref myst">Spark Application and UI</span></p></li>
<li><p><span class="xref myst">Partitions</span></p></li>
<li><p><span class="xref myst">Optimising Joins</span></p>
<ul>
<li><p><span class="xref myst">Broadcast Join</span></p></li>
<li><p><span class="xref myst">Sort Merge Join</span></p></li>
<li><p><span class="xref myst">Replacing a join with a narrow transformation</span></p></li>
</ul>
</li>
<li><p><span class="xref myst">Persisting</span></p></li>
<li><p><span class="xref myst">Salted Joins</span></p></li>
<li><p><span class="xref myst">Window Functions in Spark</span></p></li>
<li><p><span class="xref myst">Caching</span></p></li>
<li><p><span class="xref myst">Checkpoint and Staging Tables</span></p></li>
<li><p><span class="xref myst">Pydoop</span></p></li>
<li><p><span class="xref myst">When To Use Spark</span></p></li>
</ul>
<p>PySpark Documentation:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.RDD.getNumPartitions.html"><code class="docutils literal notranslate"><span class="pre">.rdd.getNumPartitions()</span></code></a></p></li>
<li><p><a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.SparkSession.range.html"><code class="docutils literal notranslate"><span class="pre">spark.range()</span></code></a></p></li>
<li><p><a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.spark_partition_id.html"><code class="docutils literal notranslate"><span class="pre">F.spark_partition_id()</span></code></a></p></li>
<li><p><a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.rand.html"><code class="docutils literal notranslate"><span class="pre">F.rand()</span></code></a></p></li>
<li><p><a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.ceil.html"><code class="docutils literal notranslate"><span class="pre">F.ceil()</span></code></a></p></li>
<li><p><a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.toPandas.html"><code class="docutils literal notranslate"><span class="pre">.toPandas()</span></code></a></p></li>
<li><p><a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.broadcast.html"><code class="docutils literal notranslate"><span class="pre">F.broadcast()</span></code></a></p></li>
<li><p><a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.when.html"><code class="docutils literal notranslate"><span class="pre">F.when()</span></code></a></p></li>
<li><p><a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.orderBy.html"><code class="docutils literal notranslate"><span class="pre">.orderBy()</span></code></a></p></li>
<li><p><a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.explain.html"><code class="docutils literal notranslate"><span class="pre">.explain()</span></code></a></p></li>
<li><p><a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.repartition.html"><code class="docutils literal notranslate"><span class="pre">.repartition()</span></code></a></p></li>
<li><p><a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.coalesce.html"><code class="docutils literal notranslate"><span class="pre">.coalesce()</span></code></a></p></li>
</ul>
<p>sparklyr and tidyverse Documentation:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://spark.rstudio.com/packages/sparklyr/latest/reference/sdf_num_partitions.html"><code class="docutils literal notranslate"><span class="pre">sdf_num_partitions()</span></code></a></p></li>
<li><p><a class="reference external" href="https://spark.rstudio.com/packages/sparklyr/latest/reference/sdf_seq.html"><code class="docutils literal notranslate"><span class="pre">sdf_seq()</span></code></a></p></li>
<li><p><a class="reference external" href="https://dplyr.tidyverse.org/reference/compute.html"><code class="docutils literal notranslate"><span class="pre">collect()</span></code></a></p></li>
<li><p><a class="reference external" href="https://spark.rstudio.com/packages/sparklyr/latest/reference/sdf_broadcast.html"><code class="docutils literal notranslate"><span class="pre">sdf_broadcast()</span></code></a></p></li>
<li><p><a class="reference external" href="https://dplyr.tidyverse.org/reference/case_when.html"><code class="docutils literal notranslate"><span class="pre">case_when()</span></code></a></p></li>
<li><p><a class="reference external" href="https://spark.rstudio.com/packages/sparklyr/latest/reference/sdf_sort.html"><code class="docutils literal notranslate"><span class="pre">sdf_sort()</span></code></a></p></li>
<li><p><a class="reference external" href="https://dplyr.tidyverse.org/reference/arrange.html"><code class="docutils literal notranslate"><span class="pre">arrange()</span></code></a></p></li>
<li><p><a class="reference external" href="https://spark.rstudio.com/packages/sparklyr/latest/reference/invoke.html"><code class="docutils literal notranslate"><span class="pre">invoke()</span></code></a></p></li>
<li><p><a class="reference external" href="https://spark.rstudio.com/packages/sparklyr/latest/reference/sdf_repartition.html"><code class="docutils literal notranslate"><span class="pre">sdf_repartition()</span></code></a></p></li>
<li><p><a class="reference external" href="https://spark.rstudio.com/packages/sparklyr/latest/reference/sdf_coalesce.html"><code class="docutils literal notranslate"><span class="pre">sdf_coalesce()</span></code></a></p></li>
</ul>
<p>Spark SQL Documentation:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://spark.apache.org/docs/latest/api/sql/index.html#spark_partition_id"><code class="docutils literal notranslate"><span class="pre">spark_partition_id</span></code></a></p></li>
<li><p><a class="reference external" href="https://spark.apache.org/docs/latest/api/sql/index.html#rand"><code class="docutils literal notranslate"><span class="pre">rand</span></code></a></p></li>
<li><p><a class="reference external" href="https://spark.apache.org/docs/latest/api/sql/index.html#ceil"><code class="docutils literal notranslate"><span class="pre">ceil</span></code></a></p></li>
</ul>
<p>Spark documentation:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://spark.apache.org/docs/latest/rdd-programming-guide.html">RDD Programming Guide</a></p></li>
<li><p><a class="reference external" href="https://spark.apache.org/docs/latest/sql-performance-tuning.html">Performance tuning</a>: details of <code class="docutils literal notranslate"><span class="pre">spark.sql.shuffle.partitions</span></code> and <code class="docutils literal notranslate"><span class="pre">spark.sql.autoBroadcastJoinThreshold</span></code></p></li>
</ul>
<p>Other Links:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://best-practice-and-impact.github.io/qa-of-code-guidance/intro.html">QA of Code for Analysis and Research</a></p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./raw-notebooks/shuffling"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#shuffling-concepts">Shuffling Concepts</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-shuffle">What is a shuffle?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-partition">What is a partition?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-transformations-cause-a-shuffle">What transformations cause a shuffle?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#a-quick-note-on-rdds">A quick note on RDDs</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#examples">Examples</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-1-one-shuffle">Example 1: One Shuffle</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-2-lots-of-shuffles">Example 2: Lots of shuffles</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optimising-and-avoiding-shuffles">Optimising and Avoiding Shuffles</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#minimise-actions-and-caching">Minimise actions and caching</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#converting-to-pandas-or-base-r-df">Converting to pandas or base R DF</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reduce-size-of-dataframe">Reduce size of DataFrame</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#use-a-broadcast-join">Use a broadcast join</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#replace-joins-with-conditional-statements">Replace joins with conditional statements</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#avoiding-unnecessary-sorting">Avoiding unnecessary sorting</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#use-window-functions">Use Window functions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#other-things-matter-too">Other things matter too!</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#good-shuffles">Good Shuffles</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#further-resources">Further Resources</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Analysis Standards and Pipelines in Quality and Improvement, Office for National Statistics
</p>

  </div>
  
  <div class="footer-item">
    

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>