

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Managing Partitions &#8212; Spark at the ONS</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/accessibility.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="../../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-95MGHSRD0S"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-95MGHSRD0S');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'raw-notebooks/partitions/partitions';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/logo.png" class="logo__image only-light" alt="Spark at the ONS - Home"/>
    <script>document.write(`<img src="../../_static/logo.png" class="logo__image only-dark" alt="Spark at the ONS - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro.html">
                    Introduction
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Spark overview</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../spark-overview/spark-start.html">Getting Started with Spark</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../spark-overview/when-to-use-spark.html">When To Use Spark</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../spark-overview/spark-session-guidance.html">Guidance on Spark Sessions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../spark-overview/example-spark-sessions.html">Example Spark Sessions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../spark-overview/spark-defaults.html">Configuration Hierarchy and <code class="docutils literal notranslate"><span class="pre">spark-defaults.conf</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../spark-overview/data-types.html">Data Types in Spark</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../spark-overview/creating-dataframes.html">Creating DataFrames Manually</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../spark-overview/reading-and-writing-data-spark.html">Reading and Writing Data in Spark</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../spark-overview/data-storage.html">Data Storage</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction to PySpark</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../pyspark-intro/pyspark-intro.html">Introduction to PySpark</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../pyspark-intro/returning-data.html">Returning Data from Cluster to Driver</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../pyspark-intro/f-col.html">Reference columns by name: <code class="docutils literal notranslate"><span class="pre">F.col()</span></code></a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction to sparklyr</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../sparklyr-intro/sparklyr-intro.html">Introduction to sparklyr</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../sparklyr-intro/sparklyr-functions.html">Using Spark functions in sparklyr</a></li>

</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Spark functions</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../spark-functions/union-dataframes-with-different-columns.html">Union two DataFrames with different columns</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../spark-functions/padding.html">Add leading zeros with <code class="docutils literal notranslate"><span class="pre">lpad()</span></code></a></li>

<li class="toctree-l1"><a class="reference internal" href="../../spark-functions/sampling.html">Sampling: an overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../spark-functions/pivot-tables.html">Pivot tables in Spark</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../spark-functions/rounding.html">Rounding differences in Python, R and Spark</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../spark-functions/window-functions.html">Window Functions in Spark</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../spark-functions/cross-joins.html">Cross Joins</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../spark-functions/arrays.html">Arrays Functions in PySpark</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../spark-functions/date-functions.html">Date functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../spark-functions/writing-data.html">Writing Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../spark-functions/job-description-notebook.html">Set Spark Job Description</a></li>




<li class="toctree-l1"><a class="reference internal" href="../../spark-functions/median.html">Median in Spark</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Understanding and Optimising Spark</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../spark-concepts/optimisation-tips.html">Ideas for optimising Spark code</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../spark-concepts/spark-application-and-ui.html">Spark Application and UI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../spark-concepts/groups-not-loops.html">Groups not Loops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../spark-concepts/shuffling.html">Shuffling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../spark-concepts/join-concepts.html">Optimising Joins</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../spark-concepts/salted-joins.html">Salted Joins</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../spark-concepts/partitions.html">Managing Partitions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../spark-concepts/persistence.html">Persisting in Spark</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../spark-concepts/cache.html">Caching</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../spark-concepts/checkpoint-staging.html">Checkpoints and Staging Tables</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../spark-concepts/garbage-collection.html">Garbage Collection</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../spark-concepts/df-order.html">Spark DataFrames Are Not Ordered</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../spark-concepts/exercises.html">Exercises</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Analysis in Spark</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../spark-analysis/big-data-workflow.html">Big data workflow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../spark-analysis/interpolation.html">Interpolation in Spark</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../spark-analysis/logistic-regression.html">Logistic Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../spark-analysis/visualisation.html">Spark and Visualisation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../spark-analysis/flags.html">Flags in Spark</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../spark-analysis/bin-continuous-variable.html">Data binning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../spark-analysis/cramer_v.html">Calculating Cramér’s V from a Spark DataFrame</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Testing and Debugging</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../testing-debugging/spark-errors.html">Understanding and Handling Spark Errors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../testing-debugging/unit-testing.html">Unit Testing in Spark</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Ancillary Topics</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../ancillary-topics/module-imports.html">Naming Conflicts in Module Imports</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ancillary-topics/pydoop.html">Pydoop: HDFS to pandas</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ancillary-topics/creating-a-SQL-view-in-HDFS.html">Creating a SQL view in HDFS</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ancillary-topics/pandas-udfs.html">Pandas UDFs</a></li>
</ul>

        </div>
    </nav></div>
            <div class="bd-sidebar__bottom">
                <!-- To handle the deprecated key -->
                
                <div class="navbar_extra_footer">
                <div>
        <p>Book version 2022.6</p>
    </div>
    
                </div>
                
            </div>
        </div>
        <div id="rtd-footer-container"></div>
    </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/best-practice-and-impact/ons-spark" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/best-practice-and-impact/ons-spark/issues/new?title=Issue%20on%20page%20%2Fraw-notebooks/partitions/partitions.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/raw-notebooks/partitions/partitions.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Managing Partitions</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#investigating-partitions">Investigating partitions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#newly-created-dataframe">Newly created DataFrame</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#imported-dataframe">Imported DataFrame</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#processed-dataframe">Processed DataFrame</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-change-partitioning">How to change partitioning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#motivation">Motivation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#change-partitioning">Change partitioning</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#partition-sizes">Partition sizes</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#avoiding-the-small-files-issue-on-hdfs">Avoiding the small files issue on HDFS</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#more-efficient-processing-with-spark">More efficient processing with Spark</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#intermediate-partitions-in-wide-operations">Intermediate partitions in wide operations</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#set-up-the-problem">Set up the problem</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#run-the-jobs-and-investigate-ui">Run the jobs and investigate UI</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#discussion">Discussion</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-does-this-mean-in-practice">What does this mean in practice?</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-should-i-partition-my-data">How should I partition my data?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#partitions-when-unioning-or-binding-dataframes">Partitions when Unioning or Binding DataFrames</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#further-resources">Further resources</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section id="managing-partitions">
<h1>Managing Partitions<a class="headerlink" href="#managing-partitions" title="Permalink to this heading">#</a></h1>
<p>DataFrames in Spark are <em>distributed</em>, so although we treat them as one object they might be split up into multiple partitions over many machines on the cluster. The benefit of having multiple partitions is that some operations can be performed on the partitions in parallel.</p>
<p>More partitions means greater <em>parallelisation</em>. However, there is a cost associated with multiple partitions, for example scheduling delay and data serialisation. In the <span class="xref myst">Spark Application and UI</span> article we saw that putting a small DataFrame into one or two partitions can lead to more efficient processing.</p>
<p>In this notebook we will explore these partitions and how the partitioning of a DataFrame is affected by importing, processing and writing data in Spark. The goal is not to find the optimum number of partitions for a given DataFrame, the goal is to have greater awareness of partitioning so we can avoid the extreme cases of over-partitioning, under-partitioning and highly skewed partitioning.</p>
<section id="investigating-partitions">
<h2>Investigating partitions<a class="headerlink" href="#investigating-partitions" title="Permalink to this heading">#</a></h2>
<p>We will start by investigating the partitions of some DataFrames. We will look at the partitions of a newly created DataFrame, an imported DataFrame and a processed DataFrame. But first, we’ll need to do some imports and create a local application in the usual way.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">yaml</span>
<span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span><span class="p">,</span> <span class="n">Window</span><span class="p">,</span> <span class="n">functions</span> <span class="k">as</span> <span class="n">F</span>

<span class="n">spark</span> <span class="o">=</span> <span class="p">(</span><span class="n">SparkSession</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">master</span><span class="p">(</span><span class="s2">&quot;local[2]&quot;</span><span class="p">)</span>
         <span class="o">.</span><span class="n">appName</span><span class="p">(</span><span class="s2">&quot;partitions&quot;</span><span class="p">)</span>
         <span class="o">.</span><span class="n">getOrCreate</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">library</span><span class="p">(</span><span class="n">sparklyr</span><span class="p">)</span>

<span class="n">sc</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">spark_connect</span><span class="p">(</span>
<span class="w">    </span><span class="n">master</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;local[2]&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="n">app_name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;partitions&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="n">config</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">spark_config</span><span class="p">())</span>
</pre></div>
</div>
<section id="newly-created-dataframe">
<h3>Newly created DataFrame<a class="headerlink" href="#newly-created-dataframe" title="Permalink to this heading">#</a></h3>
<p>Let’s create a DataFrame called <code class="docutils literal notranslate"><span class="pre">rand_df</span></code> with an <code class="docutils literal notranslate"><span class="pre">id</span></code> column from 0 to 4,999 and a <code class="docutils literal notranslate"><span class="pre">rand_val</span></code> column containing random numbers from 1 to 10.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">row_ct</span> <span class="o">=</span> <span class="mi">5000</span>
<span class="n">seed_no</span> <span class="o">=</span> <span class="mi">42</span> <span class="c1">#this is used to create the pseudo-random numbers</span>

<span class="n">rand_df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">row_ct</span><span class="p">)</span>
<span class="n">rand_df</span> <span class="o">=</span> <span class="n">rand_df</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s2">&quot;rand_val&quot;</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="n">seed_no</span><span class="p">)</span><span class="o">*</span><span class="mi">10</span><span class="p">))</span>

<span class="n">rand_df</span><span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>+---+--------+
| id|rand_val|
+---+--------+
|  0|       7|
|  1|       9|
|  2|      10|
|  3|       9|
|  4|       5|
|  5|       6|
|  6|       1|
|  7|       2|
|  8|       4|
|  9|       7|
+---+--------+
only showing top 10 rows
</pre></div>
</div>
</div>
</div>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">row_ct</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">5000</span>
<span class="n">seed_no</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">42L</span><span class="w"> </span><span class="c1">#this is used to create the pseudo-random numbers</span>

<span class="n">rand_df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">sdf_seq</span><span class="p">(</span><span class="n">sc</span><span class="p">,</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="n">row_ct</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="m">1</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">    </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">mutate</span><span class="p">(</span><span class="n">rand_val</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">ceiling</span><span class="p">(</span><span class="nf">rand</span><span class="p">(</span><span class="n">seed_no</span><span class="p">)</span><span class="o">*</span><span class="m">10</span><span class="p">))</span>

<span class="n">rand_df</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="nf">head</span><span class="p">(</span><span class="m">10</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">collect</span><span class="p">()</span>
</pre></div>
</div>
<p>To find the number of partitions of a DataFrame in PySpark we need to access the underlying RDD structures that make up the DataFrame by using <code class="docutils literal notranslate"><span class="pre">.rdd</span></code> after referencing the DataFrame. Then we can use the <a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.RDD.getNumPartitions.html"><code class="docutils literal notranslate"><span class="pre">.getNumPartitions()</span></code></a> method to return the number of partitions. In sparklyr we can just use the function <a class="reference external" href="https://spark.rstudio.com/packages/sparklyr/latest/reference/sdf_num_partitions.html"><code class="docutils literal notranslate"><span class="pre">sdf_num_partitions()</span></code></a>.</p>
<p>We can also find out how many rows are in each partition. There is more than one way of doing this, one method was introduced in the <span class="xref myst">Shuffling</span> article and will be used later in this article, a second method is shown below. Again, in PySpark we will need to access the underlying RDDs then map on a <code class="docutils literal notranslate"><span class="pre">lambda</span></code> function that will loop over the rows in each partition and return a count of these rows in a list. In sparklyr we can use <a class="reference external" href="https://spark.rstudio.com/packages/sparklyr/latest/reference/spark_apply.html"><code class="docutils literal notranslate"><span class="pre">spark_apply()</span></code></a> to give us a row count for each partition.</p>
<p>Don’t worry too much about understanding this line of code, it’s the result which is important here.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Number of partitions:</span><span class="se">\t\t</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">rand_df</span><span class="o">.</span><span class="n">rdd</span><span class="o">.</span><span class="n">getNumPartitions</span><span class="p">())</span>

<span class="n">rows_in_part</span> <span class="o">=</span> <span class="n">rand_df</span><span class="o">.</span><span class="n">rdd</span><span class="o">.</span><span class="n">mapPartitions</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">[</span><span class="nb">sum</span><span class="p">(</span><span class="mi">1</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">x</span><span class="p">)])</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Number of rows per partition:</span><span class="se">\t</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">rows_in_part</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Number of partitions:		 2
Number of rows per partition:	 [2500, 2500]
</pre></div>
</div>
</div>
</div>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">print</span><span class="p">(</span><span class="nf">paste0</span><span class="p">(</span><span class="s">&#39;Number of partitions: &#39;</span><span class="p">,</span><span class="w"> </span><span class="n">rand_df</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">sdf_num_partitions</span><span class="p">()))</span>

<span class="n">rows_in_part</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">spark_apply</span><span class="p">(</span><span class="n">rand_df</span><span class="p">,</span><span class="w"> </span><span class="nf">function</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="nf">nrow</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">collect</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="nf">paste0</span><span class="p">(</span><span class="s">&#39;Number of rows per partition: &#39;</span><span class="p">,</span><span class="w"> </span><span class="n">rows_in_part</span><span class="p">))</span><span class="w"> </span>
</pre></div>
</div>
<p>Note that the size of the partitions is the same. We will see later, if there was skew in partition sizes we would always have to wait for the largest partition to finish processing before moving on to the next task, this is commonly referred to as a bottleneck. So Spark understandably puts a similar number of rows in each partition.</p>
<p>The number of partitions was set by default. The property that controls this number is <code class="docutils literal notranslate"><span class="pre">spark.default.parallelism</span></code>, and to modify the default we must override it in the Spark session. See the <span class="xref myst">Guidance on Spark Sessions</span> for more information on how to do this. To see what the default is we can look at <a class="reference external" href="https://spark.apache.org/docs/latest/configuration.html#execution-behavior">the Spark documentation</a> (if you follow the link use Ctrl+F to search for the property name).</p>
<blockquote>
<div><p>For operations like parallelize with no parent RDDs, it depends on the cluster manager:</p>
<ul class="simple">
<li><p>Local mode: number of cores on the local machine</p></li>
<li><p>Mesos fine grained mode: 8</p></li>
<li><p>Others: total number of cores on all executor nodes or 2, whichever is larger</p></li>
</ul>
</div></blockquote>
<p>We’re running Spark in local mode with 2 cores, hence without overriding this property it will default to 2.</p>
<p>Modifying the <code class="docutils literal notranslate"><span class="pre">spark.default.parallelism</span></code> property will set the number of partitions for all newly created DataFrames in this session. But what if we want to create a DataFrame that is unusually large or small for our session? For this case we can specify how many partitions we want when creating the DataFrame, using an extra argument <code class="docutils literal notranslate"><span class="pre">numPartitions</span></code>/<code class="docutils literal notranslate"><span class="pre">repartition</span></code> for PySpark/sparklyr.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">small_rand</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">numPartitions</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1">#just 10 rows, so we&#39;ll put them into one partition</span>
<span class="n">small_rand</span> <span class="o">=</span> <span class="n">small_rand</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s2">&quot;rand_value&quot;</span><span class="p">,</span>  <span class="n">F</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="n">seed_no</span><span class="p">)</span><span class="o">*</span><span class="mi">10</span><span class="p">))</span>

<span class="n">small_rand</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">small_rand</span><span class="o">.</span><span class="n">rdd</span><span class="o">.</span><span class="n">getNumPartitions</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>+---+----------+
| id|rand_value|
+---+----------+
|  0|         7|
|  1|         9|
|  2|        10|
|  3|         9|
|  4|         5|
|  5|         6|
|  6|         1|
|  7|         2|
|  8|         4|
|  9|         7|
+---+----------+
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1
</pre></div>
</div>
</div>
</div>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">small_rand</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">sdf_seq</span><span class="p">(</span><span class="n">sc</span><span class="p">,</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">9</span><span class="p">,</span><span class="w"> </span><span class="n">repartition</span><span class="o">=</span><span class="m">1</span><span class="p">)</span><span class="w"> </span><span class="c1">#just 10 rows, so we&#39;ll put them into one partition</span>
<span class="n">small_rand</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">small_rand</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span>
<span class="w">    </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">mutate</span><span class="p">(</span><span class="n">rand_val</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">ceiling</span><span class="p">(</span><span class="nf">rand</span><span class="p">(</span><span class="n">seed_no</span><span class="p">)</span><span class="o">*</span><span class="m">10</span><span class="p">))</span>
<span class="w">    </span>
<span class="n">small_rand</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">collect</span><span class="p">()</span>
<span class="n">small_rand</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">sdf_num_partitions</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="imported-dataframe">
<h3>Imported DataFrame<a class="headerlink" href="#imported-dataframe" title="Permalink to this heading">#</a></h3>
<p>Next, let’s import the <code class="docutils literal notranslate"><span class="pre">animal_rescue.csv</span></code> file and see how many partitions we have.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;../../../config.yaml&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">config</span> <span class="o">=</span> <span class="n">yaml</span><span class="o">.</span><span class="n">safe_load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
    
<span class="n">rescue_path</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="s2">&quot;rescue_path_csv&quot;</span><span class="p">]</span>

<span class="n">rescue</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">csv</span><span class="p">(</span><span class="n">rescue_path</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">inferSchema</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">rescue</span><span class="o">.</span><span class="n">rdd</span><span class="o">.</span><span class="n">getNumPartitions</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1
</pre></div>
</div>
</div>
</div>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">config</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">yaml</span><span class="o">::</span><span class="nf">yaml.load_file</span><span class="p">(</span><span class="s">&quot;ons-spark/config.yaml&quot;</span><span class="p">)</span>

<span class="n">rescue</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">spark_read_csv</span><span class="p">(</span><span class="n">sc</span><span class="p">,</span><span class="w"> </span><span class="n">config</span><span class="o">$</span><span class="n">rescue_path_csv</span><span class="p">,</span><span class="w"> </span><span class="n">header</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">,</span><span class="w"> </span><span class="n">inferSchema</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">)</span><span class="w"> </span>

<span class="n">rescue</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">sdf_num_partitions</span><span class="p">()</span>
</pre></div>
</div>
<p>Again, we didn’t set this number anywhere, and it obviously wasn’t set by <code class="docutils literal notranslate"><span class="pre">spark.default.parallelism</span></code> either.</p>
<p>How many rows and columns do each DataFrame have?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">rescue_rows</span> <span class="o">=</span> <span class="n">rescue</span><span class="o">.</span><span class="n">count</span><span class="p">()</span>
<span class="n">rescue_columns</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">rescue</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>

<span class="n">rand_df_columns</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">rand_df</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Number of rows in rescue DataFrame:</span><span class="se">\t\t</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">rescue_rows</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Number of columns in rescue DataFrame:</span><span class="se">\t\t</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">rescue_columns</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Total number of cells in rescue DataFrame:</span><span class="se">\t</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">rescue_rows</span><span class="o">*</span><span class="n">rescue_columns</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Number of rows in random_df DataFrame:</span><span class="se">\t\t</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">row_ct</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Number of columns in random_df DataFrame:</span><span class="se">\t</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">rand_df_columns</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Total number of cells in random_df DataFrame:</span><span class="se">\t</span><span class="s1">&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">rand_df</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span><span class="o">*</span><span class="n">row_ct</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Number of rows in rescue DataFrame:		 5898
Number of columns in rescue DataFrame:		 26
Total number of cells in rescue DataFrame:	 153348

Number of rows in random_df DataFrame:		 5000
Number of columns in random_df DataFrame:	 2
Total number of cells in random_df DataFrame:	 10000
</pre></div>
</div>
</div>
</div>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">rescue_rows</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">rescue</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">sdf_nrow</span><span class="p">()</span>
<span class="n">rescue_columns</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">rescue</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">sdf_ncol</span><span class="p">()</span>
<span class="w">    </span>
<span class="n">rand_df_columns</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">rand_df</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">sdf_ncol</span><span class="p">()</span>
<span class="w"> </span>
<span class="nf">print</span><span class="p">(</span><span class="nf">paste0</span><span class="p">(</span><span class="s">&#39;Number of rows in rescue DataFrame: &#39;</span><span class="p">,</span><span class="w"> </span><span class="n">rescue_rows</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="nf">paste0</span><span class="p">(</span><span class="s">&#39;Number of columns in rescue DataFrame:&#39;</span><span class="p">,</span><span class="w"> </span><span class="n">rescue_columns</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="nf">paste0</span><span class="p">(</span><span class="s">&#39;Total number of cells in rescue DataFrame:&#39;</span><span class="p">,</span><span class="w"> </span><span class="n">rescue_rows</span><span class="o">*</span><span class="n">rescue_columns</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="nf">paste0</span><span class="p">(</span><span class="s">&#39;Number of rows in random_df DataFrame:&#39;</span><span class="p">,</span><span class="w"> </span><span class="n">row_ct</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="nf">paste0</span><span class="p">(</span><span class="s">&#39;Number of columns in random_df DataFrame:&#39;</span><span class="p">,</span><span class="w"> </span><span class="n">rand_df_columns</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="nf">paste0</span><span class="p">(</span><span class="s">&#39;Total number of cells in random_df DataFrame:&#39;</span><span class="p">,</span><span class="w"> </span><span class="n">rand_df_columns</span><span class="o">*</span><span class="n">row_ct</span><span class="p">))</span>
</pre></div>
</div>
<p>There are more cells in the rescue DataFrame, but Spark put it into just one partition.</p>
<p>The reason for this is that the original csv file is stored as a single file on disk, which means that Spark will read it in as one partition. Data on HDFS can be stored in multiple files. In general, when a Spark DataFrame with <span class="math notranslate nohighlight">\(P\)</span> partitions is written onto disk, it will be stored in <span class="math notranslate nohighlight">\(P\)</span> files, and whenever we read that dataset into a Spark session it will again have <span class="math notranslate nohighlight">\(P\)</span> partitions. We will revisit writing partitions to disk later.</p>
</section>
<section id="processed-dataframe">
<h3>Processed DataFrame<a class="headerlink" href="#processed-dataframe" title="Permalink to this heading">#</a></h3>
<p>Let’s see how applying <em>narrow</em> and <em>wide</em> transformations to the DataFrame affects the number of partitions. Firstly we’ll filter the data, which is a <em>narrow</em> operation.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">filtered_rescue</span> <span class="o">=</span> <span class="n">rescue</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">col</span><span class="p">(</span><span class="s1">&#39;AnimalGroupParent&#39;</span><span class="p">)</span> <span class="o">==</span> <span class="s1">&#39;Cat&#39;</span><span class="p">)</span>
<span class="n">filtered_rescue</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s1">&#39;AnimalGroupParent&#39;</span><span class="p">,</span> <span class="s1">&#39;FinalDescription&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">truncate</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Number of partitions in filtered_rescue DataFrame:</span><span class="se">\t</span><span class="s1">&#39;</span><span class="p">,</span><span class="n">filtered_rescue</span><span class="o">.</span><span class="n">rdd</span><span class="o">.</span><span class="n">getNumPartitions</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>+-----------------+--------------------------------------------+
|AnimalGroupParent|FinalDescription                            |
+-----------------+--------------------------------------------+
|Cat              |TO ASSIST RSPCA WITH CAT TRAPPED UP TREE,B15|
|Cat              |ASSIST RSPCA WITH CAT STUCK UP TREE, B15    |
|Cat              |CAT STUCK UP TREE,B15                       |
+-----------------+--------------------------------------------+
only showing top 3 rows

Number of partitions in filtered_rescue DataFrame:	 1
</pre></div>
</div>
</div>
</div>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">filtered_rescue</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">rescue</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">filter</span><span class="p">(</span><span class="n">AnimalGroupParent</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="s">&quot;Cat&quot;</span><span class="p">)</span>
<span class="n">filtered_rescue</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">select</span><span class="p">(</span><span class="n">AnimalGroupParent</span><span class="p">,</span><span class="w"> </span><span class="n">FinalDescription</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span>
<span class="w">    </span><span class="nf">head</span><span class="p">(</span><span class="m">3</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span>
<span class="w">    </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">collect</span><span class="p">()</span>

<span class="nf">print</span><span class="p">(</span><span class="nf">paste0</span><span class="p">(</span><span class="s">&#39;Number of partitions in filtered_rescue DataFrame: &#39;</span><span class="p">,</span><span class="n">filtered_rescue</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">sdf_num_partitions</span><span class="p">()))</span>
</pre></div>
</div>
<p>No change. In general, for a narrow transformations the contents of the partitions will change according to the transformation applied, but no data will move from one partition to another and the total number of partitions will remain the same.</p>
<p>Now let’s group the data, which is a wide operation, by <code class="docutils literal notranslate"><span class="pre">PostcodeDistrict</span></code> and then aggregate to find a count of incidents by area. How many partitions will we have in the new DataFrame?</p>
<p>*Note PySpark and sparklyr will give different results here; we will explain why later.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">count_by_area</span> <span class="o">=</span> <span class="p">(</span><span class="n">rescue</span><span class="o">.</span><span class="n">groupBy</span><span class="p">(</span><span class="s1">&#39;PostcodeDistrict&#39;</span><span class="p">)</span>
    <span class="o">.</span><span class="n">agg</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">count</span><span class="p">(</span><span class="s1">&#39;IncidentNumber&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;Count&#39;</span><span class="p">)))</span>

<span class="n">count_by_area</span><span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Number of partitions in rescue_by_area DataFrame:</span><span class="se">\t</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">count_by_area</span><span class="o">.</span><span class="n">rdd</span><span class="o">.</span><span class="n">getNumPartitions</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>+----------------+-----+
|PostcodeDistrict|Count|
+----------------+-----+
|            SE17|   20|
|            EC1Y|    1|
|             CR0|   98|
|             EN3|   45|
|            SW14|   12|
+----------------+-----+
only showing top 5 rows

Number of partitions in rescue_by_area DataFrame:	 200
</pre></div>
</div>
</div>
</div>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">count_by_area</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">rescue</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="n">dplyr</span><span class="o">::</span><span class="nf">group_by</span><span class="p">(</span><span class="n">PostcodeDistrict</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">    </span><span class="n">dplyr</span><span class="o">::</span><span class="nf">summarise</span><span class="p">(</span><span class="n">count</span><span class="o">=</span><span class="nf">n</span><span class="p">())</span>
<span class="w">    </span>
<span class="n">count_by_area</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="nf">head</span><span class="p">(</span><span class="m">5</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">collect</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="nf">paste0</span><span class="p">(</span><span class="s">&#39;Number of partitions in count_by_area DataFrame:&#39;</span><span class="p">,</span><span class="w"> </span><span class="n">count_by_area</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">sdf_num_partitions</span><span class="p">()))</span>
</pre></div>
</div>
<p><strong>How many?!?</strong></p>
<p><font size="6"> 😵 </font></p>
<p>Let’s check how many rows are in each partition.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">rows_in_part</span> <span class="o">=</span> <span class="n">count_by_area</span><span class="o">.</span><span class="n">rdd</span><span class="o">.</span><span class="n">mapPartitions</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">[</span><span class="nb">sum</span><span class="p">(</span><span class="mi">1</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">x</span><span class="p">)])</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Number of rows per partition:</span><span class="se">\t</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">rows_in_part</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Number of rows per partition:	 [1, 1, 1, 0, 0, 0, 1, 1, 2, 1, 2, 1, 0, 1, 1, 0, 1, 2, 2, 4, 1, 2, 1, 0, 2, 3, 2, 3, 1, 2, 0, 1, 1, 3, 0, 0, 2, 3, 0, 1, 1, 2, 1, 1, 3, 0, 2, 2, 1, 2, 1, 2, 0, 3, 5, 4, 1, 1, 1, 2, 0, 2, 2, 1, 0, 1, 1, 0, 0, 0, 1, 1, 4, 3, 1, 2, 3, 3, 1, 4, 0, 2, 2, 5, 1, 1, 0, 0, 3, 1, 3, 2, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 2, 3, 3, 0, 2, 0, 1, 2, 2, 0, 1, 6, 0, 2, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 2, 2, 1, 3, 1, 3, 1, 3, 0, 0, 1, 4, 1, 4, 1, 0, 0, 0, 1, 1, 3, 1, 2, 3, 1, 0, 1, 0, 2, 1, 1, 2, 1, 0, 2, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 2, 4, 2, 0, 1, 0, 0, 1, 2, 3, 2, 1, 0, 1, 0, 3, 2, 0, 2, 1, 0, 4, 0, 2, 0, 0, 0, 3]
</pre></div>
</div>
</div>
</div>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># Note that this code takes a long time to run in sparklyr</span>
<span class="n">rows_in_part</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">spark_apply</span><span class="p">(</span><span class="n">count_by_area</span><span class="p">,</span><span class="w"> </span><span class="nf">function</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="nf">nrow</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">collect</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="nf">paste0</span><span class="p">(</span><span class="s">&#39;Number of rows per partition: &#39;</span><span class="p">,</span><span class="w"> </span><span class="n">rows_in_part</span><span class="p">))</span><span class="w"> </span>
</pre></div>
</div>
<p>Quite a few empty partitions- this is a clear case of overpartitioning. In the <span class="xref myst">Spark Application and UI</span> article it was shown that overpartitioning leads to slower processing as a larger proportion of time is spent on scheduling tasks and (de)serialising data instead of processing the task.</p>
<p>Again, there is a property we can modify in the Spark session configuration to change this behaviour. The property to override is <code class="docutils literal notranslate"><span class="pre">spark.sql.shuffle.partitions</span></code>. <em>Shuffle partitions</em> means when a shuffle occurs, for example a <em>wide</em> operation. So this property says “give the resulting DataFrame this many partitions”. See the article on <span class="xref myst">Shuffling</span> for more information on shuffles.</p>
<p>The <a class="reference external" href="https://spark.apache.org/docs/latest/configuration.html#runtime-sql-configuration">Spark documentation</a> shows that the default value for <code class="docutils literal notranslate"><span class="pre">spark.sql.shuffle.partitions</span></code> is 200. This default is obviously too high for our case of grouping a small dataset with a small number of groups i.e. <code class="docutils literal notranslate"><span class="pre">PostcodeDistrict</span></code>. This is one of the more useful properties to consider changing depending on the size of the data you want to process with Spark.</p>
<p>We noted above that sparklyr would give different results. Using a local sparklyr session there is another property that is used for partitioning after a wide operation, <code class="docutils literal notranslate"><span class="pre">spark.sql.shuffle.partitions.local</span></code>, which is set to 16 by default.</p>
</section>
</section>
<section id="how-to-change-partitioning">
<h2>How to change partitioning<a class="headerlink" href="#how-to-change-partitioning" title="Permalink to this heading">#</a></h2>
<p>Before we look at how to modify DataFrame partitioning, let’s revisit the motivation for wanting to do this.</p>
<section id="motivation">
<h3>Motivation<a class="headerlink" href="#motivation" title="Permalink to this heading">#</a></h3>
<ol class="arabic simple">
<li><p>We can change the number of partitions of a DataFrame to achieve less or more parallelisation. More parallel processing is great for large DataFrames, but there are some scheduling and data serialisation overheads involved in processing multiple partitions. So for small DataFrames it’s best to have just a small number of partition because the overheads involved in organising and writing multiple partitions will result in slower processing. However, we don’t always get to choose the partitioning, e.g. to do a join Spark must first put rows with the same join keys on the same partition. More on this later.</p></li>
<li><p>We can also partition by a specified column(s) in the DataFrame. This can be useful when writing to a Hive table or file becuase we can then read in single partitions. For example, say we had a DataFrame containing records from the past 20 years. We could partition this DataFrame by year when writing it to disk, then in future we could read in one year at a time or multiple years to speed up future tasks.</p></li>
</ol>
</section>
<section id="change-partitioning">
<h3>Change partitioning<a class="headerlink" href="#change-partitioning" title="Permalink to this heading">#</a></h3>
<p>Here are two ways of changing the number of partitions of a DataFrame. One is <a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.repartition.html"><code class="docutils literal notranslate"><span class="pre">.repartition()</span></code></a>/<a class="reference external" href="https://spark.rstudio.com/packages/sparklyr/latest/reference/sdf_repartition.html"><code class="docutils literal notranslate"><span class="pre">sdf_repartition()</span></code></a> and the other is <code class="docutils literal notranslate"><span class="pre">coalesce()</span></code>/<a class="reference external" href="https://spark.rstudio.com/packages/sparklyr/latest/reference/sdf_coalesce.html"><code class="docutils literal notranslate"><span class="pre">sdf_coalesce()</span></code></a>.</p>
<p>Let’s see how to use these functions.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">rand_df</span><span class="o">.</span><span class="n">rdd</span><span class="o">.</span><span class="n">getNumPartitions</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2
</pre></div>
</div>
</div>
</div>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">rand_df</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">sdf_num_partitions</span><span class="p">()</span>
</pre></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">rand_df</span> <span class="o">=</span> <span class="n">rand_df</span><span class="o">.</span><span class="n">coalesce</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">rand_df</span><span class="o">.</span><span class="n">rdd</span><span class="o">.</span><span class="n">getNumPartitions</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1
</pre></div>
</div>
</div>
</div>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">rand_df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">rand_df</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">sdf_coalesce</span><span class="p">(</span><span class="m">1</span><span class="p">)</span>
<span class="n">rand_df</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">sdf_num_partitions</span><span class="p">()</span>
</pre></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">rand_df</span> <span class="o">=</span> <span class="n">rand_df</span><span class="o">.</span><span class="n">coalesce</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="n">rand_df</span><span class="o">.</span><span class="n">rdd</span><span class="o">.</span><span class="n">getNumPartitions</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2
</pre></div>
</div>
</div>
</div>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">rand_df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">rand_df</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">sdf_coalesce</span><span class="p">(</span><span class="m">10</span><span class="p">)</span>
<span class="n">rand_df</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">sdf_num_partitions</span><span class="p">()</span>
</pre></div>
</div>
<p>Is that the answer we were expecting? Note that <a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.coalesce.html"><code class="docutils literal notranslate"><span class="pre">.coalesce()</span></code></a> should only be used to <em>decrease</em> the number of partitions. When increasing it can only increase to a previous state.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">rand_df</span> <span class="o">=</span> <span class="n">rand_df</span><span class="o">.</span><span class="n">repartition</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">rand_df</span><span class="o">.</span><span class="n">rdd</span><span class="o">.</span><span class="n">getNumPartitions</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1
</pre></div>
</div>
</div>
</div>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">rand_df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">rand_df</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">sdf_repartition</span><span class="p">(</span><span class="m">1</span><span class="p">)</span>
<span class="n">rand_df</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">sdf_num_partitions</span><span class="p">()</span>
</pre></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">rand_df</span> <span class="o">=</span> <span class="n">rand_df</span><span class="o">.</span><span class="n">repartition</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="n">rand_df</span><span class="o">.</span><span class="n">rdd</span><span class="o">.</span><span class="n">getNumPartitions</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>10
</pre></div>
</div>
</div>
</div>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">rand_df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">rand_df</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">sdf_repartition</span><span class="p">(</span><span class="m">10</span><span class="p">)</span>
<span class="n">rand_df</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">sdf_num_partitions</span><span class="p">()</span>
</pre></div>
</div>
<p>Repartition can be used to increase parallelisation.</p>
<p>The other important difference is that <code class="docutils literal notranslate"><span class="pre">.repartition()</span></code> incurs a <em>full</em> shuffle of the DataFrame, meaning it rewrites all the data into the new partitions. Shuffling takes time, especially for large amounts of data, so it’s best to avoid shuffling more data than needed. To learn more about shuffling have a look at the <span class="xref myst">Shuffling</span> article.</p>
<p>On the other hand <code class="docutils literal notranslate"><span class="pre">.coalesce()</span></code> involves moving just some of the data. Let’s demonstrate by an example.</p>
<p>Say we have a simple DataFrame where the partitions contain the following numbers:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Partition 1: 1, 2, 3
Partition 2: 4, 5, 6
Partition 3: 7, 8, 9
Partition 4: 10, 11, 12
</pre></div>
</div>
<p>We then decide we want two partitions as opposed to four so we apply <code class="docutils literal notranslate"><span class="pre">.coalesce(2)</span></code> to the DataFrame. The partitions would now contain the following:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Partition 1: 1, 2, 3 + (7, 8, 9)
Partition 2: 4, 5, 6 + (10, 11, 12)
</pre></div>
</div>
<p>So the data on Partitions 1 and 2 have not been shuffled. We have just moved the data on Partitions 3 to Partition 1 and Partition 4 to Partition 2, this is sometimes referred to as a partial shuffle. The gains from this simple example would be negligible, but scale this process up to millions of rows and then it becomes significant.</p>
<p>It’s also possible to use a column name in <code class="docutils literal notranslate"><span class="pre">.repartition()</span></code> or even a number and column name, see documentation for more information.</p>
</section>
</section>
<section id="partition-sizes">
<h2>Partition sizes<a class="headerlink" href="#partition-sizes" title="Permalink to this heading">#</a></h2>
<p>A key concept in both Hadoop and Spark is that files and partitions should optimally be <a class="reference external" href="https://spark.apache.org/docs/latest/sql-performance-tuning.html#other-configuration-options">around 128MB</a>. It is however common that data are stored as many small files rather than fewer large ones. This issue can be solved by reducing the number of partitions before writing to HDFS, using <a class="reference external" href="https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.coalesce"><code class="docutils literal notranslate"><span class="pre">.coalesce()</span></code></a> or <a class="reference external" href="https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.repartition"><code class="docutils literal notranslate"><span class="pre">.repartition()</span></code></a> in PySpark and <a class="reference external" href="https://spark.rstudio.com/reference/sdf_coalesce.html"><code class="docutils literal notranslate"><span class="pre">sdf_coalesce()</span></code></a> or <a class="reference external" href="https://spark.rstudio.com/reference/sdf_repartition.html"><code class="docutils literal notranslate"><span class="pre">sdf_repartition()</span></code></a> in sparklyr.</p>
<p>Below are two reasons why it’s a good idea to avoid writing out many small files.</p>
<section id="avoiding-the-small-files-issue-on-hdfs">
<h3>Avoiding the small files issue on HDFS<a class="headerlink" href="#avoiding-the-small-files-issue-on-hdfs" title="Permalink to this heading">#</a></h3>
<p>HDFS consists of many DataNodes, which store the files, and a single NameNode, which essentially works as an index for the file system. The small files issue occurs as every file path is loaded into the NameNode which leads to an inefficient use of the memory capacity, e.g. if a 600MB file is stored as 200 files of 3MB then the NameNode needs to load 200 file paths, instead of the 5 file paths it would have to load if stored as 5 files of 120MB each.</p>
</section>
<section id="more-efficient-processing-with-spark">
<h3>More efficient processing with Spark<a class="headerlink" href="#more-efficient-processing-with-spark" title="Permalink to this heading">#</a></h3>
<p>Spark will by default use one partition per file on disk when reading in the data. If there are many small files on disk then these will be converted to many small partitions, which leads to slower processing as proportionally more time is spent scheduling tasks and serialising data.</p>
</section>
</section>
<section id="intermediate-partitions-in-wide-operations">
<h2>Intermediate partitions in wide operations<a class="headerlink" href="#intermediate-partitions-in-wide-operations" title="Permalink to this heading">#</a></h2>
<p>Now onto a more complex topic of intermediate partitioining.</p>
<p>We saw earlier that for narrow transformations the number of partitions of input and output DataFrames is the same. We also saw that for wide transformations the number of partitions of the output DataFrame is changed to the value of the <code class="docutils literal notranslate"><span class="pre">spark.sql.shuffle.partitions</span></code> property set in the Spark session.</p>
<p>But what is inside the partitions of the output DataFrame after a wide operation? This depends on the operation, so in this section we will look at three wide transformations: join, group by and <span class="xref myst">window functions</span>. Understanding how these functions work is particularly useful when we deal with skewed data, or more specifically skew in the join key, group variable or windows. We will demonstrate this with some skewed data and show that knowing your data can help you make informed decisions on scaling jobs vertically or horizontally, or employing an alternative strategy like a <span class="xref myst">salted join</span>.</p>
<section id="set-up-the-problem">
<h3>Set up the problem<a class="headerlink" href="#set-up-the-problem" title="Permalink to this heading">#</a></h3>
<p>Let’s start with a DataFrame with a skewed variable <code class="docutils literal notranslate"><span class="pre">skew_col</span></code>, but uniform partition sizes. It also has a column <code class="docutils literal notranslate"><span class="pre">rand_val</span></code> containing some random numbers to perform some calculations.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">row_ct</span> <span class="o">=</span> <span class="mi">10</span><span class="o">**</span><span class="mi">7</span>
<span class="n">seed_no</span> <span class="o">=</span> <span class="mi">42</span>

<span class="n">skewed_df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">row_ct</span><span class="p">,</span> <span class="n">numPartitions</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">skewed_df</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">skewed_df</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s2">&quot;skew_col&quot;</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">when</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">col</span><span class="p">(</span><span class="s2">&quot;id&quot;</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">100</span><span class="p">,</span> <span class="s2">&quot;A&quot;</span><span class="p">)</span>
                               <span class="o">.</span><span class="n">when</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">col</span><span class="p">(</span><span class="s2">&quot;id&quot;</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">1000</span><span class="p">,</span> <span class="s2">&quot;B&quot;</span><span class="p">)</span>
                               <span class="o">.</span><span class="n">when</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">col</span><span class="p">(</span><span class="s2">&quot;id&quot;</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">10000</span><span class="p">,</span> <span class="s2">&quot;C&quot;</span><span class="p">)</span>
                               <span class="o">.</span><span class="n">when</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">col</span><span class="p">(</span><span class="s2">&quot;id&quot;</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">100000</span><span class="p">,</span> <span class="s2">&quot;D&quot;</span><span class="p">)</span>
                               <span class="o">.</span><span class="n">otherwise</span><span class="p">(</span><span class="s2">&quot;E&quot;</span><span class="p">))</span>
              <span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s2">&quot;rand_val&quot;</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">rint</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">seed_no</span><span class="p">)</span><span class="o">*</span><span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="s2">&quot;int&quot;</span><span class="p">))</span>
<span class="p">)</span>

<span class="n">skewed_df</span><span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>+---+--------+--------+
| id|skew_col|rand_val|
+---+--------+--------+
|  0|       A|       7|
|  1|       A|       9|
|  2|       A|       9|
|  3|       A|       9|
|  4|       A|       4|
+---+--------+--------+
only showing top 5 rows
</pre></div>
</div>
</div>
</div>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">row_ct</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">10</span><span class="o">**</span><span class="m">7</span>
<span class="n">seed_no</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">42L</span>

<span class="n">skewed_df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">sdf_seq</span><span class="p">(</span><span class="n">sc</span><span class="p">,</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="n">row_ct</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="n">repartition</span><span class="o">=</span><span class="m">2</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">    </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">mutate</span><span class="p">(</span>
<span class="w">        </span><span class="n">skew_col</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">dplyr</span><span class="o">::</span><span class="nf">case_when</span><span class="p">(</span>
<span class="w">            </span><span class="n">id</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="m">100</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="s">&quot;A&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="n">id</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="m">1000</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="s">&quot;B&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="n">id</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="m">10000</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="s">&quot;C&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="n">id</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="m">100000</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="s">&quot;D&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="kc">TRUE</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="s">&quot;E&quot;</span><span class="p">),</span>
<span class="w">        </span><span class="n">rand_val</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">ceiling</span><span class="p">(</span><span class="nf">rand</span><span class="p">(</span><span class="n">seed_no</span><span class="p">)</span><span class="o">*</span><span class="m">10</span><span class="p">))</span>

<span class="n">skewed_df</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="nf">head</span><span class="p">(</span><span class="m">5</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">collect</span><span class="p">()</span>
</pre></div>
</div>
<p>To confirm the details of the partitioning we will add a column with the partition ID, then group that column and count how many rows are in each partition. This method of counting how many rows are in each partition is very useful and much quicker than the method shown earlier, but it will not show any empty partitions like the previous method. Therefore, we will also show the total number of partitions.</p>
<p>We’ll put this into a function so we can run it again on other DataFrames later.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">print_partitioning_info</span><span class="p">(</span><span class="n">sdf</span><span class="p">):</span>
    <span class="n">sdf</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s2">&quot;part_id&quot;</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">spark_partition_id</span><span class="p">())</span><span class="o">.</span><span class="n">groupBy</span><span class="p">(</span><span class="s2">&quot;part_id&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">count</span><span class="p">()</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Number of partitions: </span><span class="si">{</span><span class="n">sdf</span><span class="o">.</span><span class="n">rdd</span><span class="o">.</span><span class="n">getNumPartitions</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">print_partitioning_info</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">function</span><span class="p">(</span><span class="n">sdf</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">sdf</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span>
<span class="w">        </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">mutate</span><span class="p">(</span>
<span class="w">            </span><span class="n">part_id</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">spark_partition_id</span><span class="p">())</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">        </span><span class="n">dplyr</span><span class="o">::</span><span class="nf">group_by</span><span class="p">(</span><span class="n">part_id</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">        </span><span class="n">dplyr</span><span class="o">::</span><span class="nf">summarise</span><span class="p">(</span><span class="n">count</span><span class="o">=</span><span class="nf">n</span><span class="p">())</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">        </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">collect</span><span class="p">()</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">        </span><span class="nf">print</span><span class="p">()</span>

<span class="w">    </span><span class="nf">print</span><span class="p">(</span><span class="nf">paste0</span><span class="p">(</span><span class="s">&quot;Number of partitions: &quot;</span><span class="p">,</span><span class="w"> </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">sdf_num_partitions</span><span class="p">(</span><span class="n">sdf</span><span class="p">)))</span>
<span class="p">}</span>
</pre></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">print_partitioning_info</span><span class="p">(</span><span class="n">skewed_df</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>+-------+-------+
|part_id|  count|
+-------+-------+
|      1|5000000|
|      0|5000000|
+-------+-------+

Number of partitions: 2
</pre></div>
</div>
</div>
</div>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">print_partitioning_info</span><span class="p">(</span><span class="n">skewed_df</span><span class="p">)</span>
</pre></div>
</div>
<p>To perform the join we will also need a second DataFrame</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">small_df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([</span>
    <span class="p">[</span><span class="s2">&quot;A&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
    <span class="p">[</span><span class="s2">&quot;B&quot;</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
    <span class="p">[</span><span class="s2">&quot;C&quot;</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
    <span class="p">[</span><span class="s2">&quot;D&quot;</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span>
    <span class="p">[</span><span class="s2">&quot;E&quot;</span><span class="p">,</span> <span class="mi">5</span><span class="p">]</span>
<span class="p">],</span> <span class="p">[</span><span class="s2">&quot;skew_col&quot;</span><span class="p">,</span> <span class="s2">&quot;number_col&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">small_df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">sdf_copy_to</span><span class="p">(</span><span class="n">sc</span><span class="p">,</span><span class="w"> </span><span class="nf">data.frame</span><span class="p">(</span>
<span class="w">    </span><span class="s">&quot;skew_col&quot;</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">LETTERS</span><span class="p">[</span><span class="m">1</span><span class="o">:</span><span class="m">5</span><span class="p">],</span><span class="w"> </span>
<span class="w">    </span><span class="s">&quot;number_col&quot;</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="m">5</span><span class="p">))</span><span class="w"> </span>
</pre></div>
</div>
<p>To see how Spark executes each plan in full we will write the output DataFrames to disk to initiate the Spark job, then delete the file afterwards. Of course, the information on the Spark job will remain in the UI for us to inspect after deleting the file. In Spark 3 there is a nicer way of doing this by using the <code class="docutils literal notranslate"><span class="pre">noop</span></code> argument, meaning <em>no operation</em>.</p>
<p>We will use the <code class="docutils literal notranslate"><span class="pre">checkpoint_path</span></code> from the config.yml file to write the data before deleting it. See the <span class="xref myst">Checkpoint and Staging Tables</span> article for more information.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">subprocess</span>

<span class="k">def</span> <span class="nf">write_delete</span><span class="p">(</span><span class="n">sdf</span><span class="p">):</span>
    <span class="n">path</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="s2">&quot;checkpoint_path&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="s2">&quot;/temp.parquet&quot;</span>
    <span class="n">sdf</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">mode</span><span class="p">(</span><span class="s2">&quot;overwrite&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
    <span class="n">cmd</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;rm -r -skipTrash </span><span class="si">{</span><span class="n">path</span><span class="si">}</span><span class="s1">&#39;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">subprocess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">cmd</span><span class="p">,</span> <span class="n">shell</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">write_delete</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">function</span><span class="p">(</span><span class="n">sdf</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">path</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">paste0</span><span class="p">(</span><span class="n">config</span><span class="o">$</span><span class="n">checkpoint_path</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;/temp.parquet&quot;</span><span class="p">)</span>
<span class="w">    </span><span class="n">sdf</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">spark_write_parquet</span><span class="p">(</span><span class="n">path</span><span class="o">=</span><span class="n">path</span><span class="p">,</span><span class="w"> </span><span class="n">mode</span><span class="o">=</span><span class="s">&quot;overwrite&quot;</span><span class="p">)</span>
<span class="w">    </span><span class="n">cmd</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">paste0</span><span class="p">(</span><span class="s">&quot;hdfs dfs -rm -r -skipTrash &quot;</span><span class="p">,</span><span class="w"> </span><span class="n">path</span><span class="p">)</span>
<span class="w">    </span><span class="nf">system</span><span class="p">(</span><span class="n">cmd</span><span class="p">)</span>
<span class="p">}</span>
</pre></div>
</div>
<p>We will need a link to the Spark UI to view the details of how Spark partitions the data. When using a local session we can access the Spark UI with this URL, <a class="reference external" href="http://localhost:4040/jobs/">http://localhost:4040/jobs/</a>.</p>
</section>
<section id="run-the-jobs-and-investigate-ui">
<h3>Run the jobs and investigate UI<a class="headerlink" href="#run-the-jobs-and-investigate-ui" title="Permalink to this heading">#</a></h3>
<p>Next we will carry out the wide transformations on the <code class="docutils literal notranslate"><span class="pre">skewed_df</span></code> and apply the above function to create jobs. We have also added custom job descriptions to make it easier to find the relevant stage details in the UI.</p>
<p>Note that the join key, group variable and windows are all set on the <code class="docutils literal notranslate"><span class="pre">skew_col</span></code> variable.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">spark</span><span class="o">.</span><span class="n">sparkContext</span><span class="o">.</span><span class="n">setJobDescription</span><span class="p">(</span><span class="s2">&quot;join&quot;</span><span class="p">)</span>
<span class="n">joined_df</span> <span class="o">=</span> <span class="n">skewed_df</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">small_df</span><span class="p">,</span> <span class="n">on</span><span class="o">=</span><span class="s2">&quot;skew_col&quot;</span><span class="p">,</span> <span class="n">how</span><span class="o">=</span><span class="s2">&quot;left&quot;</span><span class="p">)</span>
<span class="n">write_delete</span><span class="p">(</span><span class="n">joined_df</span><span class="p">)</span>

<span class="n">spark</span><span class="o">.</span><span class="n">sparkContext</span><span class="o">.</span><span class="n">setJobDescription</span><span class="p">(</span><span class="s2">&quot;groupby&quot;</span><span class="p">)</span>
<span class="n">grouped_df</span> <span class="o">=</span> <span class="n">skewed_df</span><span class="o">.</span><span class="n">groupBy</span><span class="p">(</span><span class="s2">&quot;skew_col&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">agg</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="s2">&quot;rand_val&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&quot;sum_rand&quot;</span><span class="p">))</span>
<span class="n">write_delete</span><span class="p">(</span><span class="n">grouped_df</span><span class="p">)</span>

<span class="n">spark</span><span class="o">.</span><span class="n">sparkContext</span><span class="o">.</span><span class="n">setJobDescription</span><span class="p">(</span><span class="s2">&quot;window&quot;</span><span class="p">)</span>
<span class="n">window_df</span> <span class="o">=</span> <span class="n">skewed_df</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s2">&quot;skew_window_sum&quot;</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="s2">&quot;rand_val&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">over</span><span class="p">(</span><span class="n">Window</span><span class="o">.</span><span class="n">partitionBy</span><span class="p">(</span><span class="s2">&quot;skew_col&quot;</span><span class="p">)))</span>
<span class="n">write_delete</span><span class="p">(</span><span class="n">window_df</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">sc</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">    </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">spark_context</span><span class="p">()</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">    </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">invoke</span><span class="p">(</span><span class="s">&quot;setJobDescription&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;join&quot;</span><span class="p">)</span>

<span class="n">joined_df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">skewed_df</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">left_join</span><span class="p">(</span><span class="n">small_df</span><span class="p">,</span><span class="w"> </span><span class="n">by</span><span class="o">=</span><span class="s">&quot;skew_col&quot;</span><span class="p">)</span>
<span class="nf">write_delete</span><span class="p">(</span><span class="n">joined_df</span><span class="p">)</span>

<span class="n">sc</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">    </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">spark_context</span><span class="p">()</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">    </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">invoke</span><span class="p">(</span><span class="s">&quot;setJobDescription&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;groupby&quot;</span><span class="p">)</span>
<span class="w">    </span>
<span class="n">grouped_df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">skewed_df</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span>
<span class="w">    </span><span class="n">dplyr</span><span class="o">::</span><span class="nf">group_by</span><span class="p">(</span><span class="n">skew_col</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span>
<span class="w">    </span><span class="n">dplyr</span><span class="o">::</span><span class="nf">summarise</span><span class="p">(</span><span class="n">sum_rand</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">sum</span><span class="p">(</span><span class="n">rand_val</span><span class="p">))</span>

<span class="nf">write_delete</span><span class="p">(</span><span class="n">grouped_df</span><span class="p">)</span>

<span class="n">sc</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">    </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">spark_context</span><span class="p">()</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">    </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">invoke</span><span class="p">(</span><span class="s">&quot;setJobDescription&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;window&quot;</span><span class="p">)</span>

<span class="n">window_df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">skewed_df</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">    </span><span class="n">dplyr</span><span class="o">::</span><span class="nf">group_by</span><span class="p">(</span><span class="n">skew_col</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">    </span><span class="n">dplyr</span><span class="o">::</span><span class="nf">mutate</span><span class="p">(</span><span class="n">skew_window_sum</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">sum</span><span class="p">(</span><span class="n">rand_val</span><span class="p">))</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">    </span><span class="n">dplyr</span><span class="o">::</span><span class="nf">ungroup</span><span class="p">()</span>

<span class="nf">write_delete</span><span class="p">(</span><span class="n">window_df</span><span class="p">)</span>
</pre></div>
</div>
<p>Below are images of the task timeline for the above jobs containing the wide transformations. Note that the images might look slightly different if you are running the source notebook. The processing times will also vary.</p>
<p>The main message in this set of images is that we see a clear bottleneck in the case of the join and window function, but there is no bottleneck in the group by.</p>
<figure class="align-default" id="jointimeline">
<a class="reference internal image-reference" href="raw-notebooks/images/partition_skew_join.PNG"><img alt="Task timeline for skewed join showing unevenly sized tasks" src="raw-notebooks/images/partition_skew_join.PNG" style="width: 100%;" /></a>
<figcaption>
<p><span class="caption-text">Task timeline for skewed join</span><a class="headerlink" href="#jointimeline" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-default" id="groupbytimeline">
<a class="reference internal image-reference" href="raw-notebooks/images/partition_skew_groupby.PNG"><img alt="Task timeline for skewed group by showing evenly sized tasks" src="raw-notebooks/images/partition_skew_groupby.PNG" style="width: 100%;" /></a>
<figcaption>
<p><span class="caption-text">Task timeline for skewed group by</span><a class="headerlink" href="#groupbytimeline" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-default" id="windowtimeline">
<a class="reference internal image-reference" href="raw-notebooks/images/partition_skew_window.PNG"><img alt="Task timeline for skewed windows showing unevenly sized tasks" src="raw-notebooks/images/partition_skew_window.PNG" style="width: 100%;" /></a>
<figcaption>
<p><span class="caption-text">Task timeline for skewed windows</span><a class="headerlink" href="#windowtimeline" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>It’s also useful to look at the Tasks table below the timeline to see how many records were processes in each of the 200 tasks. In the images below we have sorted the table by the <em>Output Size / Records</em> column (circled red) so that the largest tasks are at the top.</p>
<figure class="align-default" id="joindetails">
<a class="reference internal image-reference" href="raw-notebooks/images/partition_skew_join_table.PNG"><img alt="Task details for skewed join showing number of records in uneven partitions" src="raw-notebooks/images/partition_skew_join_table.PNG" style="width: 100%;" /></a>
<figcaption>
<p><span class="caption-text">Task details for skewed join</span><a class="headerlink" href="#joindetails" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-default" id="groupbydetails">
<a class="reference internal image-reference" href="raw-notebooks/images/partition_skew_groupby_table.PNG"><img alt="Task details for skewed group by showing number of records in even partitions" src="raw-notebooks/images/partition_skew_groupby_table.PNG" style="width: 100%;" /></a>
<figcaption>
<p><span class="caption-text">Task details for skewed group by</span><a class="headerlink" href="#groupbydetails" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-default" id="windowdetails">
<a class="reference internal image-reference" href="raw-notebooks/images/partition_skew_window_table.PNG"><img alt="Task details for skewed windows" src="raw-notebooks/images/partition_skew_window_table.PNG" style="width: 100%;" /></a>
<figcaption>
<p><span class="caption-text">Task details for skewed windows</span><a class="headerlink" href="#windowdetails" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>One last piece of information we will gather is the partitioning information of the output DataFrames using the <code class="docutils literal notranslate"><span class="pre">print_partitioning_info()</span></code> function we defined above.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">print_partitioning_info</span><span class="p">(</span><span class="n">joined_df</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>+-------+-------+
|part_id|  count|
+-------+-------+
|     78|  90000|
|     43|9900000|
|     49|    900|
|    106|    100|
|     89|   9000|
+-------+-------+

Number of partitions: 200
</pre></div>
</div>
</div>
</div>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">print_partitioning_info</span><span class="p">(</span><span class="n">joined_df</span><span class="p">)</span>
</pre></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">print_partitioning_info</span><span class="p">(</span><span class="n">grouped_df</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>+-------+-----+
|part_id|count|
+-------+-----+
|     78|    1|
|     43|    1|
|     49|    1|
|    106|    1|
|     89|    1|
+-------+-----+

Number of partitions: 200
</pre></div>
</div>
</div>
</div>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">print_partitioning_info</span><span class="p">(</span><span class="n">grouped_df</span><span class="p">)</span>
</pre></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">print_partitioning_info</span><span class="p">(</span><span class="n">window_df</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>+-------+-------+
|part_id|  count|
+-------+-------+
|      1|5000000|
|      0|5000000|
+-------+-------+

Number of partitions: 200
</pre></div>
</div>
</div>
</div>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">print_partitioning_info</span><span class="p">(</span><span class="n">window_df</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="discussion">
<h3>Discussion<a class="headerlink" href="#discussion" title="Permalink to this heading">#</a></h3>
<p>Now that we have all the information on how Spark performed these processes we can compare the three cases.</p>
<p>All three operations involve an intermediate shuffle where the join key/grouped variable/windows are moved to the same partitions. This is called <code class="docutils literal notranslate"><span class="pre">hashpartitioning()</span></code> in the execution plan. As we mentioned previously, these are all wide transformations so the number of partitions of the output DataFrames are the same, 200. Listed below are the differences in processing and output DataFrames for the three operations.</p>
<p><strong>Join</strong></p>
<p>There was much more work to be done on the larger partition than the smaller ones. The Spark UI shows there were more records to process on the large partition therefore the task takes much longer. Note the number of records processed in each task matches the number of rows in each join key. The output DataFrame is partitioned by the join key.</p>
<p><strong>Group by</strong></p>
<p>In the case of the group by the skewed variable isn’t too much of an issue because the aggregation is quite simple and is optimised, therefore it is easy to process each partition whether there is skew or not. Again, the output DataFrame is partitioned by the grouped variable, this time there is one row on each partition.</p>
<p><strong>Window</strong></p>
<p>Like the join, there is more work to do on the larger partitions. However, unlike the join, the rows of the output DataFrame have the same partition IDs as the input DataFrame.</p>
</section>
<section id="what-does-this-mean-in-practice">
<h3>What does this mean in practice?<a class="headerlink" href="#what-does-this-mean-in-practice" title="Permalink to this heading">#</a></h3>
<p>Highly skewed data is a common issue that causes slow processing with Spark. Above we have seen that skewed data causes skewed partitions when Spark processes wide transformations and can result in bottlenecks, where some tasks take much longer than others therefore not utilising the full potential of parallel processing. An even worse situation is where the skew causes spill, where the large partition cannot fit into its allocated memory on an executor and overflows temporarily onto disk. We cannot recreate the issue of spill in a local session but they are easy to spot in the Stage details page in the Spark UI.</p>
<p>This is where knowing your data can be useful. To help with the explanation we will refer to the join keys/groups/windows as groups. If a join/group by/window function is causing you issues, try to work out how many groups there are and the sizes of these groups. If there are lots on smaller groups it might help to increase <code class="docutils literal notranslate"><span class="pre">spark.sql.shuffle.partitions</span></code> and aim for greater parallel power with lots of smaller executors. If there are fewer groups you might want to scale more vertically so decrease <code class="docutils literal notranslate"><span class="pre">spark.sql.shuffle.partitions</span></code> and aim for a smaller number of bulkier executors.</p>
<p>If you are doing a join on a highly skewed DataFrame you might want to try a <span class="xref myst">salted join</span>. If you are dealing with skew in a window function you can apply a group by and join to achieve the same result. However, these are alternative solutions when dealing with highly skewed data and in most cases a regular join or window function are more efficient and makes the code more readable.</p>
</section>
</section>
<section id="how-should-i-partition-my-data">
<h2>How should I partition my data?<a class="headerlink" href="#how-should-i-partition-my-data" title="Permalink to this heading">#</a></h2>
<p>The short answer is- don’t worry about it too much!</p>
<p>Let’s flip the question around. What are the consequences of getting the number of partitions “wrong”? Most of the time- nothing. You could spend hours trying to optimise the number of partitions and perhaps speed up the processing time to be twice as quick, but sometimes there are simpler things you can do that might speed up the processing by ten times or a hundred times the processing speed.</p>
<p>As the computer scientist Donald Knuth once wrote:</p>
<blockquote>
<div><p>Premature optimization is the root of all evil (or at least most of it) in programming</p>
<p>-&gt; <em>Computer Programming as an Art (1974)</em></p>
</div></blockquote>
<p>More importantly it’s good practice to be aware of the size of the DataFrame and the number of partitions as this will help to avoid obvious issues or over-partitioning, under-partitioning and highly skewed processes.</p>
<p>A more accurate answer depends on a variety of factors including: the size of the data, data types, distributions within the data, type of processing and other properties defined in the <a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.SparkSession.html"><code class="docutils literal notranslate"><span class="pre">SparkSession</span></code></a>. Here are some tips for quick wins:</p>
<ul class="simple">
<li><p>As suggested above, the first step is getting the code right. Only look to optimise if you’re running into performance issues.</p></li>
<li><p>Decreasing the <code class="docutils literal notranslate"><span class="pre">spark.sql.shuffle.partitions</span></code> parameter is sensible for smaller datasets.</p></li>
<li><p>If you search for an answer on the web, you might find the optimum size for a Spark partitions is 100-200MB. In practice, this is impossible to keep track of in an analysis script, but it gives you an idea of what is considered big or small.</p></li>
<li><p>It makes sense for the number of partitions to be some multiple of the number of cores in the Spark session. This will help to avoid redundant cores.</p></li>
</ul>
<p>Remember, it’s only worth experimenting on the <em>optimum</em> number if you have a real performance issue.</p>
</section>
<section id="partitions-when-unioning-or-binding-dataframes">
<h2>Partitions when Unioning or Binding DataFrames<a class="headerlink" href="#partitions-when-unioning-or-binding-dataframes" title="Permalink to this heading">#</a></h2>
<p>You can append two DataFrames in PySpark that have the same schema with the <a class="reference external" href="https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.union"><code class="docutils literal notranslate"><span class="pre">.union()</span></code></a> operation. In sparklyr, the equivalent operation is <a class="reference external" href="https://spark.rstudio.com/reference/sdf_bind.html"><code class="docutils literal notranslate"><span class="pre">sdf_bind_rows()</span></code></a> and R users will often refer to appending two DataFrames as <em>binding</em>.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">.union()</span></code> function in PySpark and the <code class="docutils literal notranslate"><span class="pre">sdf_bind_rows()</span></code> function in sparklyr are both equivalent to <code class="docutils literal notranslate"><span class="pre">UNION</span> <span class="pre">ALL</span></code> in SQL. A regular <code class="docutils literal notranslate"><span class="pre">UNION</span></code> operation in SQL will remove duplicates, whereas <code class="docutils literal notranslate"><span class="pre">.union()</span></code> in PySpark and <code class="docutils literal notranslate"><span class="pre">sdf_bind_rows()</span></code> in sparklyr will not.</p>
<p>Unlike joining two DataFrames, <code class="docutils literal notranslate"><span class="pre">.union()</span></code>/<code class="docutils literal notranslate"><span class="pre">sdf_bind_rows()</span></code>  does not involve a full shuffle, as the data does not move between partitions. Instead, the number of partitions in the unioned DataFrame is equal to the sum of the number of partitions in the two source DataFrames, i.e. if you union a DataFrame consisting of 100 partitions and one consisting of 50 partitions, your unioned DataFrame will have 150 partitions.</p>
<p>To avoid excessive number of partitions, you can use <a class="reference external" href="https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.coalesce"><code class="docutils literal notranslate"><span class="pre">.coalesce()</span></code></a> or <a class="reference external" href="https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.repartition"><code class="docutils literal notranslate"><span class="pre">.repartition()</span></code></a> to reduce the number of partitions (use <a class="reference external" href="https://spark.rstudio.com/reference/sdf_coalesce.html"><code class="docutils literal notranslate"><span class="pre">sdf_coalesce()</span></code></a> or <a class="reference external" href="https://spark.rstudio.com/reference/sdf_repartition.html"><code class="docutils literal notranslate"><span class="pre">sdf_repartition()</span></code></a>
in sparklyr). The DataFrame will also get repartitioned when a <em>wide transformation</em> is applied to the DataFrame (also called a shuffle), e.g. with a <code class="docutils literal notranslate"><span class="pre">.groupBy()</span></code> or <code class="docutils literal notranslate"><span class="pre">.orderBy()</span></code>.</p>
<p>It is also worth being aware that storing data on HDFS as many small files is inefficient, both in terms of how the data is stored and in reading it in. Unioning many DataFrames and then writing straight to HDFS can be a cause of this issue. For more information on the small file issue, see the previous section on <span class="xref myst">Partition sizes</span>.</p>
<p>First we will stop the previous spark sessions and start a new Spark session which limits the number of partitions to 12, read the Animal Rescue data, group by animal and year, and then count. The grouping and aggregation will cause a shuffle, meaning that the DataFrame will have 12 partitions, as we set <code class="docutils literal notranslate"><span class="pre">spark.sql.shuffle.partitions</span></code> to <code class="docutils literal notranslate"><span class="pre">12</span></code> in the <code class="docutils literal notranslate"><span class="pre">SparkSession.builder</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span><span class="p">,</span> <span class="n">functions</span> <span class="k">as</span> <span class="n">F</span>

<span class="c1"># Stop previous spark session and create new spark session</span>

<span class="n">spark</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>

<span class="n">spark</span> <span class="o">=</span> <span class="p">(</span><span class="n">SparkSession</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">master</span><span class="p">(</span><span class="s2">&quot;local[2]&quot;</span><span class="p">)</span>
         <span class="o">.</span><span class="n">appName</span><span class="p">(</span><span class="s2">&quot;partitions&quot;</span><span class="p">)</span>
         <span class="o">.</span><span class="n">config</span><span class="p">(</span><span class="s2">&quot;spark.sql.shuffle.partitions&quot;</span><span class="p">,</span> <span class="mi">12</span><span class="p">)</span>
         <span class="o">.</span><span class="n">getOrCreate</span><span class="p">())</span>


<span class="n">rescue_path_csv</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="s2">&quot;rescue_path_csv&quot;</span><span class="p">]</span>

<span class="c1"># # Read in and shuffle data</span>
<span class="n">rescue</span> <span class="o">=</span> <span class="p">(</span><span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">csv</span><span class="p">(</span><span class="n">rescue_path_csv</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">inferSchema</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  
          <span class="o">.</span><span class="n">withColumnRenamed</span><span class="p">(</span><span class="s2">&quot;IncidentNumber&quot;</span><span class="p">,</span> <span class="s2">&quot;incident_number&quot;</span><span class="p">)</span>
          <span class="o">.</span><span class="n">withColumnRenamed</span><span class="p">(</span><span class="s2">&quot;AnimalGroupParent&quot;</span><span class="p">,</span> <span class="s2">&quot;animal_group&quot;</span><span class="p">)</span>
          <span class="o">.</span><span class="n">withColumnRenamed</span><span class="p">(</span><span class="s2">&quot;CalYear&quot;</span><span class="p">,</span> <span class="s2">&quot;cal_year&quot;</span><span class="p">)</span>
          <span class="o">.</span><span class="n">groupBy</span><span class="p">(</span><span class="s2">&quot;animal_group&quot;</span><span class="p">,</span> <span class="s2">&quot;cal_year&quot;</span><span class="p">)</span>
          <span class="o">.</span><span class="n">agg</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">count</span><span class="p">(</span><span class="s2">&quot;incident_number&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&quot;animal_count&quot;</span><span class="p">)))</span>

<span class="n">rescue</span><span class="o">.</span><span class="n">limit</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">toPandas</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>animal_group</th>
      <th>cal_year</th>
      <th>animal_count</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Bird</td>
      <td>2010</td>
      <td>99</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Hamster</td>
      <td>2011</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Unknown - Domestic Animal Or Pet</td>
      <td>2012</td>
      <td>18</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Unknown - Heavy Livestock Animal</td>
      <td>2012</td>
      <td>4</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Sheep</td>
      <td>2012</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># Stop previous spark session and create new spark session limiting number of partitions</span>
<span class="nf">spark_disconnect</span><span class="p">(</span><span class="n">sc</span><span class="p">)</span>

<span class="n">small_config</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">spark_config</span><span class="p">()</span>
<span class="n">small_config</span><span class="o">$</span><span class="n">spark.sql.shuffle.partitions</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">12</span>

<span class="n">sc</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">spark_connect</span><span class="p">(</span>
<span class="w">  </span><span class="n">master</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;local&quot;</span><span class="p">,</span>
<span class="w">  </span><span class="n">app_name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;partitions&quot;</span><span class="p">,</span>
<span class="w">  </span><span class="n">config</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">small_config</span><span class="p">)</span>

<span class="n">sparklyr</span><span class="o">::</span><span class="nf">spark_connection_is_open</span><span class="p">(</span><span class="n">sc</span><span class="p">)</span>

<span class="c1"># Set the data path</span>
<span class="n">config</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">yaml</span><span class="o">::</span><span class="nf">yaml.load_file</span><span class="p">(</span><span class="s">&quot;ons-spark/config.yaml&quot;</span><span class="p">)</span>
<span class="c1"># Read in and shuffle data</span>
<span class="n">rescue</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">spark_read_csv</span><span class="p">(</span><span class="n">sc</span><span class="p">,</span><span class="w"> </span><span class="n">config</span><span class="o">$</span><span class="n">rescue_path_csv</span><span class="p">,</span><span class="w"> </span><span class="n">header</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">,</span><span class="w"> </span><span class="n">infer_schema</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">)</span>

<span class="n">rescue</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">rescue</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">    </span><span class="n">dplyr</span><span class="o">::</span><span class="nf">rename</span><span class="p">(</span>
<span class="w">        </span><span class="n">incident_number</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">IncidentNumber</span><span class="p">,</span>
<span class="w">        </span><span class="n">animal_group</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">AnimalGroupParent</span><span class="p">,</span>
<span class="w">        </span><span class="n">cal_year</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">CalYear</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">        </span><span class="n">dplyr</span><span class="o">::</span><span class="nf">group_by</span><span class="p">(</span><span class="n">animal_group</span><span class="p">,</span><span class="n">cal_year</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">        </span><span class="n">dplyr</span><span class="o">::</span><span class="nf">count</span><span class="p">(</span><span class="n">animal_count</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">count</span><span class="p">())</span>

<span class="n">rescue</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="nf">head</span><span class="p">(</span><span class="m">5</span><span class="p">)</span>
</pre></div>
</div>
<p>We can confirm the number of partitions of the <code class="docutils literal notranslate"><span class="pre">rescue</span></code> DataFrame using <code class="docutils literal notranslate"><span class="pre">.rdd.getNumPartitions()</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Rescue partitions:&#39;</span><span class="p">,</span><span class="n">rescue</span><span class="o">.</span><span class="n">rdd</span><span class="o">.</span><span class="n">getNumPartitions</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Rescue partitions: 12
</pre></div>
</div>
</div>
</div>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">print</span><span class="p">(</span><span class="nf">paste0</span><span class="p">(</span><span class="s">&quot;Rescue partitions: &quot;</span><span class="p">,</span><span class="w"> </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">sdf_num_partitions</span><span class="p">(</span><span class="n">rescue</span><span class="p">)))</span>
</pre></div>
</div>
<p>Now create will create some smaller DataFrames, containing different animals, by filtering the rescue data. We will preview just the <code class="docutils literal notranslate"><span class="pre">dogs</span></code> data, as all others will be of a similar format:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dogs</span> <span class="o">=</span> <span class="n">rescue</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">col</span><span class="p">(</span><span class="s2">&quot;animal_group&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="s2">&quot;Dog&quot;</span><span class="p">)</span>
<span class="n">cats</span> <span class="o">=</span> <span class="n">rescue</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">col</span><span class="p">(</span><span class="s2">&quot;animal_group&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="s2">&quot;Cat&quot;</span><span class="p">)</span>
<span class="n">hamsters</span> <span class="o">=</span> <span class="n">rescue</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">col</span><span class="p">(</span><span class="s2">&quot;animal_group&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="s2">&quot;Hamster&quot;</span><span class="p">)</span>

<span class="n">dogs</span><span class="o">.</span><span class="n">limit</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">toPandas</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>animal_group</th>
      <th>cal_year</th>
      <th>animal_count</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Dog</td>
      <td>2011</td>
      <td>103</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Dog</td>
      <td>2017</td>
      <td>81</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Dog</td>
      <td>2009</td>
      <td>132</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Dog</td>
      <td>2012</td>
      <td>100</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Dog</td>
      <td>2014</td>
      <td>90</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">dogs</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">rescue</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">filter</span><span class="p">(</span><span class="n">animal_group</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="s">&#39;Dog&#39;</span><span class="p">)</span>
<span class="n">cats</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">rescue</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">filter</span><span class="p">(</span><span class="n">animal_group</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="s">&#39;Cat&#39;</span><span class="p">)</span>
<span class="n">hamsters</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">rescue</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">filter</span><span class="p">(</span><span class="n">animal_group</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="s">&#39;Hamster&#39;</span><span class="p">)</span>

<span class="n">dogs</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="nf">head</span><span class="p">(</span><span class="m">5</span><span class="p">)</span>
</pre></div>
</div>
<p>We can check that each of these DataFrames has 12 partitions:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39; Dog partitions:&#39;</span><span class="p">,</span> <span class="n">dogs</span><span class="o">.</span><span class="n">rdd</span><span class="o">.</span><span class="n">getNumPartitions</span><span class="p">(),</span>
      <span class="s1">&#39;</span><span class="se">\n</span><span class="s1"> Cat partitions:&#39;</span><span class="p">,</span> <span class="n">cats</span><span class="o">.</span><span class="n">rdd</span><span class="o">.</span><span class="n">getNumPartitions</span><span class="p">(),</span>
      <span class="s1">&#39;</span><span class="se">\n</span><span class="s1"> Hamster partitions:&#39;</span><span class="p">,</span> <span class="n">hamsters</span><span class="o">.</span><span class="n">rdd</span><span class="o">.</span><span class="n">getNumPartitions</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> Dog partitions: 12 
 Cat partitions: 12 
 Hamster partitions: 12
</pre></div>
</div>
</div>
</div>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">print</span><span class="p">(</span><span class="nf">paste0</span><span class="p">(</span><span class="s">&quot;Dogs partitions: &quot;</span><span class="p">,</span><span class="w"> </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">sdf_num_partitions</span><span class="p">(</span><span class="n">dogs</span><span class="p">)))</span>
<span class="nf">print</span><span class="p">(</span><span class="nf">paste0</span><span class="p">(</span><span class="s">&quot;Cats partitions: &quot;</span><span class="p">,</span><span class="w"> </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">sdf_num_partitions</span><span class="p">(</span><span class="n">cats</span><span class="p">)))</span>
<span class="nf">print</span><span class="p">(</span><span class="nf">paste0</span><span class="p">(</span><span class="s">&quot;Hamsters partitions: &quot;</span><span class="p">,</span><span class="w"> </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">sdf_num_partitions</span><span class="p">(</span><span class="n">hamsters</span><span class="p">)))</span>
</pre></div>
</div>
<p>When we apply the union function, the number of partitions in the unioned DataFrame will be the sum of partitions in each DataFrame. In this case we will now have 12 + 12 = 24 partitions:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dogs_and_cats</span> <span class="o">=</span> <span class="n">dogs</span><span class="o">.</span><span class="n">union</span><span class="p">(</span><span class="n">cats</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Dogs and Cats union partitions:&#39;</span><span class="p">,</span><span class="n">dogs_and_cats</span><span class="o">.</span><span class="n">rdd</span><span class="o">.</span><span class="n">getNumPartitions</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Dogs and Cats union partitions: 24
</pre></div>
</div>
</div>
</div>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">dogs_and_cats</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">sdf_bind_rows</span><span class="p">(</span><span class="n">dogs</span><span class="p">,</span><span class="n">cats</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="nf">paste0</span><span class="p">(</span><span class="s">&quot;Dogs and Cats union partitions: &quot;</span><span class="p">,</span><span class="w"> </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">sdf_num_partitions</span><span class="p">(</span><span class="n">dogs_and_cats</span><span class="p">)))</span>
</pre></div>
</div>
<p>Unioning another DataFrame adds another 12 partitions to make 36 (24 + 12):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dogs_cats_and_hamsters</span> <span class="o">=</span> <span class="n">dogs_and_cats</span><span class="o">.</span><span class="n">union</span><span class="p">(</span><span class="n">hamsters</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Dogs, Cats and Hamsters union partitions:&#39;</span><span class="p">,</span><span class="n">dogs_cats_and_hamsters</span><span class="o">.</span><span class="n">rdd</span><span class="o">.</span><span class="n">getNumPartitions</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Dogs, Cats and Hamsters union partitions: 36
</pre></div>
</div>
</div>
</div>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">dogs_cats_and_hamsters</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">sdf_bind_rows</span><span class="p">(</span><span class="n">dogs_and_cats</span><span class="p">,</span><span class="n">hamsters</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="nf">paste0</span><span class="p">(</span><span class="s">&quot;Dogs, Cats and Hamsters union partitions: &quot;</span><span class="p">,</span><span class="w"> </span><span class="nf">sdf_num_partitions</span><span class="p">(</span><span class="n">dogs_cats_and_hamsters</span><span class="p">)))</span>
</pre></div>
</div>
<p>Although we only have 36 partitions here it is easy to see how this might get excessive with too many <code class="docutils literal notranslate"><span class="pre">.union()</span></code> statements. This can also become a bigger problem if the number of partitons is left to its default value of 200, where we would end up with 600 partitions!</p>
<p>A subsequent shuffle (e.g. sorting the DataFrame) will reset the number of partitions to that specified in <code class="docutils literal notranslate"><span class="pre">spark.sql.shuffle.partitions</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dogs_cats_and_hamsters</span><span class="o">.</span><span class="n">orderBy</span><span class="p">(</span><span class="s2">&quot;animal_group&quot;</span><span class="p">,</span> <span class="s2">&quot;cal_year&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">rdd</span><span class="o">.</span><span class="n">getNumPartitions</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>12
</pre></div>
</div>
</div>
</div>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">sdf_num_partitions</span><span class="p">(</span><span class="n">dogs_cats_and_hamsters</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">sdf_sort</span><span class="p">(</span><span class="nf">c</span><span class="p">(</span><span class="s">&#39;animal_group&#39;</span><span class="p">,</span><span class="s">&#39;cal_year&#39;</span><span class="p">)))</span>
</pre></div>
</div>
<p>You can also use <code class="docutils literal notranslate"><span class="pre">.repartition()</span></code> or <code class="docutils literal notranslate"><span class="pre">.coalesce()</span></code>. <code class="docutils literal notranslate"><span class="pre">.repartition()</span></code> involves a shuffle of the DataFrame and puts the data into roughly equal partition sizes, whereas <code class="docutils literal notranslate"><span class="pre">.coalesce()</span></code> combines partitions without a full shuffle, and so is more efficient, although at the potential cost of less equal partition sizes and therefore potential skew in the data. For more information see the previous section on <span class="xref myst">Partition sizes</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dogs_cats_and_hamsters</span><span class="o">.</span><span class="n">repartition</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span><span class="o">.</span><span class="n">rdd</span><span class="o">.</span><span class="n">getNumPartitions</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>20
</pre></div>
</div>
</div>
</div>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">sdf_num_partitions</span><span class="p">(</span><span class="n">dogs_cats_and_hamsters</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="n">sparklyr</span><span class="o">::</span><span class="nf">sdf_repartition</span><span class="p">(</span><span class="m">20</span><span class="p">))</span>
</pre></div>
</div>
<p>Further information on managing partitions while writing data can now be found in the <span class="xref myst">Reading and Writing Data in Spark</span> page.</p>
</section>
<section id="further-resources">
<h2>Further resources<a class="headerlink" href="#further-resources" title="Permalink to this heading">#</a></h2>
<p>Spark at the ONS Articles:</p>
<ul class="simple">
<li><p><span class="xref myst">Spark Application and UI</span></p></li>
<li><p><span class="xref myst">Shuffling</span></p></li>
<li><p><span class="xref myst">Guidance on Spark Sessions</span></p></li>
<li><p><span class="xref myst">Window Functions in Spark</span></p></li>
<li><p><span class="xref myst">Salted Joins</span></p></li>
<li><p><span class="xref myst">Checkpoint and Staging Tables</span></p></li>
<li><p><span class="xref myst">Reading and Writing Data in Spark</span></p></li>
</ul>
<p>PySpark Documentation:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.coalesce.html"><code class="docutils literal notranslate"><span class="pre">.coalesce()</span></code></a></p></li>
<li><p><a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.SparkSession.html"><code class="docutils literal notranslate"><span class="pre">SparkSession</span></code></a></p></li>
<li><p><a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.RDD.getNumPartitions.html"><code class="docutils literal notranslate"><span class="pre">.getNumPartitions()</span></code></a></p></li>
<li><p><a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.repartition.html"><code class="docutils literal notranslate"><span class="pre">.repartition()</span></code></a></p></li>
<li><p><a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.coalesce.html"><code class="docutils literal notranslate"><span class="pre">.coalesce()</span></code></a></p></li>
<li><p><a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.RDD.mapPartitions.html"><code class="docutils literal notranslate"><span class="pre">.rdd.mapPartitions()</span></code></a></p></li>
<li><p><a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.partitionBy.html"><code class="docutils literal notranslate"><span class="pre">DataFrameWriter.partitionBy()</span></code></a></p></li>
<li><p><a class="reference external" href="https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.union"><code class="docutils literal notranslate"><span class="pre">.union()</span></code></a></p></li>
</ul>
<p>Python Documentation:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://docs.python.org/3/library/subprocess.html"><code class="docutils literal notranslate"><span class="pre">subprocess</span></code></a></p></li>
</ul>
<p>sparklyr and tidyverse Documentation:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://spark.rstudio.com/packages/sparklyr/latest/reference/sdf_coalesce.html"><code class="docutils literal notranslate"><span class="pre">sdf_coalesce()</span></code></a></p></li>
<li><p><a class="reference external" href="https://spark.rstudio.com/packages/sparklyr/latest/reference/sdf_repartition.html"><code class="docutils literal notranslate"><span class="pre">sdf_repartition()</span></code></a></p></li>
<li><p><a class="reference external" href="https://spark.rstudio.com/packages/sparklyr/latest/reference/sdf_num_partitions.html"><code class="docutils literal notranslate"><span class="pre">sdf_num_partitions()</span></code></a></p></li>
<li><p><a class="reference external" href="https://spark.rstudio.com/packages/sparklyr/latest/reference/spark_apply.html"><code class="docutils literal notranslate"><span class="pre">spark_apply()</span></code></a></p></li>
<li><p><a class="reference external" href="https://spark.rstudio.com/reference/sdf_bind.html"><code class="docutils literal notranslate"><span class="pre">sdf_bind_rows()</span></code></a></p></li>
</ul>
<p>Spark Documentation:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://spark.apache.org/docs/latest/configuration.html">Spark Configuration</a>:</p>
<ul>
<li><p><a class="reference external" href="https://spark.apache.org/docs/latest/configuration.html#execution-behavior">Execution Behaviour</a></p></li>
<li><p><a class="reference external" href="https://spark.apache.org/docs/latest/configuration.html#runtime-sql-configuration">Runtime SQL Configuration</a></p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://spark.apache.org/docs/latest/api/sql/index.html#round"><code class="docutils literal notranslate"><span class="pre">round</span></code></a></p></li>
<li><p><a class="reference external" href="https://spark.apache.org/docs/latest/api/sql/index.html#bround"><code class="docutils literal notranslate"><span class="pre">bround</span></code></a></p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./raw-notebooks/partitions"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#investigating-partitions">Investigating partitions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#newly-created-dataframe">Newly created DataFrame</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#imported-dataframe">Imported DataFrame</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#processed-dataframe">Processed DataFrame</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-change-partitioning">How to change partitioning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#motivation">Motivation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#change-partitioning">Change partitioning</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#partition-sizes">Partition sizes</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#avoiding-the-small-files-issue-on-hdfs">Avoiding the small files issue on HDFS</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#more-efficient-processing-with-spark">More efficient processing with Spark</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#intermediate-partitions-in-wide-operations">Intermediate partitions in wide operations</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#set-up-the-problem">Set up the problem</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#run-the-jobs-and-investigate-ui">Run the jobs and investigate UI</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#discussion">Discussion</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-does-this-mean-in-practice">What does this mean in practice?</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-should-i-partition-my-data">How should I partition my data?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#partitions-when-unioning-or-binding-dataframes">Partitions when Unioning or Binding DataFrames</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#further-resources">Further resources</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Analysis Standards and Pipelines in Quality and Improvement, Office for National Statistics
</p>

  </div>
  
  <div class="footer-item">
    

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>