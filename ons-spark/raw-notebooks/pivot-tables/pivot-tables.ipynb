{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pivot tables in Spark\n",
    "\n",
    "A pivot table is a way of displaying the result of grouped and aggregated data as a two dimensional table, rather than in the list form that you get from regular grouping and aggregating. You might be familiar with them from Excel.\n",
    "\n",
    "The principles are the same in PySpark and sparklyr, although unlike some Spark functions that are used in both PySpark and sparklyr the syntax is very different.\n",
    "\n",
    "<details>\n",
    "<summary><b>Python Explanation</b></summary>\n",
    "    \n",
    "You can create pivot tables in PySpark by using [`.pivot()`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.GroupedData.pivot.html) with [`.groupBy()`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.groupBy.html). If you group your data by two or more columns then you may find it easier to view the data in this way.\n",
    "\n",
    "`.pivot()` has two arguments. `pivot_col` is the column used to create the output columns, and has to be a single column; it cannot accept a list of multiple columns. The second argument, `values`, is optional but recommended. You can specify the exact columns that you want returned. If left blank, Spark will automatically use all possible values as output columns; calculating this can be inefficient and the output will look untidy if there are a large number of columns.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><b>R Explanation</b></summary>\n",
    "\n",
    "You can create pivot tables in sparklyr with [`sdf_pivot()`](https://spark.rstudio.com/packages/sparklyr/latest/reference/sdf_pivot.html). This is a sparklyr specific function and so it cannot be used on base R DataFrames or tibbles. An example of pivoting on a tibble is given at the end for comparison.\n",
    "\n",
    "`sdf_pivot(x, formula, fun.aggregate)` has three arguments. The first, `x` is the sparklyr DataFrame, the second, formula is an R formula with grouped columns on the left and pivot column on the right, separated by a tilde (e.g. `col1 + col2 ~ pivot_col`), and the third, `fun.aggregate`, is the functions used for aggregation; by default it will count the rows if left blank. Be careful with pivoting data where your pivot column has a large number of distinct values; it will return a very wide DataFrame that will be untidy to view. It is recommended to `filter()` the data first to only include the values you want in the output columns. The second example uses `filter()`.\n",
    "</details>\n",
    "\n",
    "### Example 1: Group by one column and count\n",
    "\n",
    "Create a new Spark session and read the Animal Rescue data. To make the example easier to read, just filter on a few animal groups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[2]\").appName(\"sampling\").getOrCreate()\n",
    "\n",
    "with open(\"../../../config.yaml\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "    \n",
    "rescue_path = config[\"rescue_path\"]\n",
    "rescue = (spark.read.parquet(rescue_path)\n",
    "          .select(\"incident_number\", \"animal_group\", \"cal_year\", \"total_cost\", \"origin_of_call\")\n",
    "          .filter(F.col(\"animal_group\").isin(\"Cat\", \"Dog\", \"Hamster\", \"Sheep\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "library(sparklyr)\n",
    "library(dplyr)\n",
    "options(pillar.print_max = Inf, pillar.width=Inf)\n",
    "\n",
    "sc <- sparklyr::spark_connect(\n",
    "    master = \"local[2]\",\n",
    "    app_name = \"sampling\",\n",
    "    config = sparklyr::spark_config())\n",
    "\n",
    "config <- yaml::yaml.load_file(\"ons-spark/config.yaml\")\n",
    "\n",
    "rescue <- sparklyr::spark_read_parquet(sc, config$rescue_path) %>%\n",
    "    sparklyr::select(incident_number, animal_group, cal_year, total_cost, origin_of_call) %>%\n",
    "    sparklyr::filter(animal_group %in% c(\"Cat\", \"Dog\", \"Hamster\", \"Sheep\"))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The minimal example is grouping by just one column, pivoting on another, just counting the rows, rather than an aggregating values in another column.\n",
    "\n",
    "<details>\n",
    "<summary><b>Python Example</b></summary>\n",
    "\n",
    "In PySpark, use `.groupBy()` and `.count()` as you normally would when grouping and getting the row count, but add `.pivot()` between the two functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----+----+----+----+----+----+----+----+----+----+----+\n",
      "|animal_group|2009|2010|2011|2012|2013|2014|2015|2016|2017|2018|2019|\n",
      "+------------+----+----+----+----+----+----+----+----+----+----+----+\n",
      "|     Hamster|null|   3|   3|null|   3|   1|null|   4|null|null|null|\n",
      "|         Cat| 262| 294| 309| 302| 312| 295| 262| 296| 257| 304|  16|\n",
      "|         Dog| 132| 122| 103| 100|  93|  90|  88| 107|  81|  91|   1|\n",
      "|       Sheep|   1|null|null|   1|null|null|   1|   1|null|null|null|\n",
      "+------------+----+----+----+----+----+----+----+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rescue_pivot = (rescue\n",
    "                .groupBy(\"animal_group\")\n",
    "                .pivot(\"cal_year\")\n",
    "                .count())\n",
    "\n",
    "rescue_pivot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><b>R Example</b></summary>\n",
    "\n",
    "In sparklyr, use `sdf_pivot()`. As the pipe (`%>%`) is being used to apply the function to the DataFrame, this minimal example takes just one argument, `formula`, which is a tilde expression. The left hand side is the grouping column, `animal_group`, and the right hand side is the pivot column, `cal_year`. The default aggregation is to get the row count, so there is no need to specify the other argument, `fun.aggregate`.\n",
    "\n",
    "Note that the R output will spill over to multiple rows. The second example resolves this by filtering on what will become the pivot columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "rescue_pivot <- rescue %>%\n",
    "    sparklyr::sdf_pivot(animal_group ~ cal_year)\n",
    "\n",
    "rescue_pivot %>%\n",
    "    sparklyr::collect() %>%\n",
    "    print()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "Another way of viewing the same information would be to just use a regular grouping expression, but it is harder to compare between years when displaying the DataFrame in this way:\n",
    "\n",
    "<details>\n",
    "<summary><b>Python Example</b></summary>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+-----+\n",
      "|animal_group|cal_year|count|\n",
      "+------------+--------+-----+\n",
      "|         Cat|    2009|  262|\n",
      "|         Cat|    2010|  294|\n",
      "|         Cat|    2011|  309|\n",
      "|         Cat|    2012|  302|\n",
      "|         Cat|    2013|  312|\n",
      "|         Cat|    2014|  295|\n",
      "|         Cat|    2015|  262|\n",
      "|         Cat|    2016|  296|\n",
      "|         Cat|    2017|  257|\n",
      "|         Cat|    2018|  304|\n",
      "|         Cat|    2019|   16|\n",
      "|         Dog|    2009|  132|\n",
      "|         Dog|    2010|  122|\n",
      "|         Dog|    2011|  103|\n",
      "|         Dog|    2012|  100|\n",
      "|         Dog|    2013|   93|\n",
      "|         Dog|    2014|   90|\n",
      "|         Dog|    2015|   88|\n",
      "|         Dog|    2016|  107|\n",
      "|         Dog|    2017|   81|\n",
      "|         Dog|    2018|   91|\n",
      "|         Dog|    2019|    1|\n",
      "|     Hamster|    2010|    3|\n",
      "|     Hamster|    2011|    3|\n",
      "|     Hamster|    2013|    3|\n",
      "|     Hamster|    2014|    1|\n",
      "|     Hamster|    2016|    4|\n",
      "|       Sheep|    2009|    1|\n",
      "|       Sheep|    2012|    1|\n",
      "|       Sheep|    2015|    1|\n",
      "|       Sheep|    2016|    1|\n",
      "+------------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rescue_grouped = (rescue\n",
    "                  .groupBy(\"animal_group\", \"cal_year\")\n",
    "                  .count()\n",
    "                  .orderBy(\"animal_group\", \"cal_year\"))\n",
    "\n",
    "rescue_grouped.show(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><b>R Example</b></summary>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "rescue_grouped <- rescue %>%\n",
    "    dplyr::group_by(animal_group, cal_year) %>%\n",
    "    dplyr::summarise(n()) %>%\n",
    "    sparklyr::sdf_sort(c(\"animal_group\", \"cal_year\"))\n",
    "\n",
    "rescue_grouped %>%\n",
    "    sparklyr::collect() %>%\n",
    "    print()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "### Example 2: Aggregate by another column and specify values\n",
    "\n",
    "<details>\n",
    "<summary><b>Python Example</b></summary>\n",
    "\n",
    "You can use [`.agg()`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.GroupedData.agg.html) with `.pivot()` in the same way as you do with `.groupBy()`. This example will sum the `total_cost`.\n",
    "\n",
    "The [documentation](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.GroupedData.pivot.html) explains why it is more efficient to manually provide the `values` argument; as an example, we just look at three years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+-------+-------+\n",
      "|animal_group|   2009|   2010|   2011|\n",
      "+------------+-------+-------+-------+\n",
      "|     Hamster|   null|  780.0|  780.0|\n",
      "|         Cat|76685.0|88140.0|89440.0|\n",
      "|         Dog|39295.0|38480.0|31200.0|\n",
      "|       Sheep|  255.0|   null|   null|\n",
      "+------------+-------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rescue_pivot = (rescue\n",
    "                .groupBy(\"animal_group\")\n",
    "                .pivot(\"cal_year\", values=[\"2009\", \"2010\", \"2011\"])\n",
    "                .agg(F.sum(\"total_cost\")))\n",
    "\n",
    "rescue_pivot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</details>\n",
    "    \n",
    "<details>\n",
    "<summary><b>R Example</b></summary>\n",
    "To group by several columns express this on the left side of the `formula` argument, concatenating them with `+`, in this example `AnimalGroup + OriginOfCall ~ CalYear`.\n",
    "\n",
    "To only look at a certain subset of the pivot column you can just use `filter()` before pivoting. This is a good idea if your pivot column has a large number of distinct values. As an example, we just look at three years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "rescue_pivot <- rescue %>%\n",
    "    sparklyr::filter(cal_year %in% c(\"2009\", \"2010\", \"2011\")) %>%\n",
    "    sparklyr::sdf_pivot(\n",
    "        animal_group ~ cal_year,\n",
    "        fun.aggregate = list(total_cost = \"sum\"))\n",
    "\n",
    "rescue_pivot %>%\n",
    "    sparklyr::collect() %>%\n",
    "    print()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "### Example 3: Multiple groupings and aggregations, fill nulls and sort\n",
    "\n",
    "<details>\n",
    "<summary><b>Python Example</b></summary>\n",
    "\n",
    "You can only supply one column to `.pivot()`, but you can have multiple aggregations. Adding an `.alias()` makes the result easier to read.\n",
    "\n",
    "Any missing combinations of the grouping and pivot will be returned as `null`, e.g. there are no incidents with `Hamster`, `Person (land line)` and `2009`. To set this to zero, use `.fillna()`.\n",
    "\n",
    "If grouping by multiple columns you may also want to add `.orderBy()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+--------+--------+--------+--------+--------+--------+\n",
      "|animal_group|      origin_of_call|2009_sum|2009_max|2010_sum|2010_max|2011_sum|2011_max|\n",
      "+------------+--------------------+--------+--------+--------+--------+--------+--------+\n",
      "|         Cat|           Ambulance|     0.0|     0.0|     0.0|     0.0|     0.0|     0.0|\n",
      "|         Cat|           Other FRS|   260.0|   260.0|   520.0|   260.0|  1040.0|   520.0|\n",
      "|         Cat|  Person (land line)| 45365.0|   780.0| 53040.0|  1040.0| 53040.0|  1040.0|\n",
      "|         Cat|     Person (mobile)| 30545.0|  1820.0| 33800.0|  2080.0| 34580.0|  1040.0|\n",
      "|         Cat|Person (running c...|     0.0|     0.0|   260.0|   260.0|     0.0|     0.0|\n",
      "|         Cat|              Police|   515.0|   260.0|   520.0|   260.0|   780.0|   260.0|\n",
      "|         Dog|           Ambulance|   255.0|   255.0|     0.0|     0.0|     0.0|     0.0|\n",
      "|         Dog|           Other FRS|  1540.0|   765.0|  1040.0|   520.0|     0.0|     0.0|\n",
      "|         Dog|  Person (land line)| 13460.0|   780.0|  9880.0|  1040.0|  9100.0|   520.0|\n",
      "|         Dog|     Person (mobile)| 20675.0|   780.0| 24180.0|  1040.0| 21320.0|  1040.0|\n",
      "|         Dog|              Police|  3365.0|   765.0|  3380.0|  1300.0|   780.0|   260.0|\n",
      "|     Hamster|  Person (land line)|     0.0|     0.0|   260.0|   260.0|   520.0|   260.0|\n",
      "|     Hamster|     Person (mobile)|     0.0|     0.0|   520.0|   260.0|   260.0|   260.0|\n",
      "|       Sheep|           Other FRS|     0.0|     0.0|     0.0|     0.0|     0.0|     0.0|\n",
      "|       Sheep|  Person (land line)|   255.0|   255.0|     0.0|     0.0|     0.0|     0.0|\n",
      "|       Sheep|     Person (mobile)|     0.0|     0.0|     0.0|     0.0|     0.0|     0.0|\n",
      "+------------+--------------------+--------+--------+--------+--------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rescue_pivot = (rescue\n",
    "           .groupBy(\"animal_group\", \"origin_of_call\")\n",
    "           .pivot(\"cal_year\", values = [\"2009\", \"2010\", \"2011\"])\n",
    "           .agg(F.sum(\"total_cost\").alias(\"sum\"), F.max(\"total_cost\").alias(\"max\"))\n",
    "           .fillna(0)\n",
    "           .orderBy(\"animal_group\", \"origin_of_call\"))\n",
    "\n",
    "rescue_pivot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><b>R Example</b></summary>\n",
    "\n",
    "`sdf_pivot()` is quite awkward with multiple aggregations on the same column. `fun.aggregate` can take a named list, but only one aggregation can be applied to each column. As we want to get the `sum` and `max` of `total_cost`, we can create another column, `total_cost_copy`, and aggregate on this. To rename the result columns dynamically, use [`rename_with()`](https://dplyr.tidyverse.org/reference/rename.html).\n",
    "\n",
    "Any missing combinations of the grouping and pivot will be returned as `NA`, e.g. there are no incidents with `Hamster`, `Person (land line)` and `2009`. To set this to zero, use `na.replace()`.\n",
    "\n",
    "If grouping by multiple columns you may also want to add `sdf_sort()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "rescue_pivot <- rescue %>%\n",
    "    sparklyr::filter(cal_year %in% c(\"2009\", \"2010\", \"2011\")) %>%\n",
    "    sparklyr::mutate(total_cost_copy = total_cost) %>%\n",
    "    sparklyr::sdf_pivot(\n",
    "        animal_group + origin_of_call ~ cal_year,\n",
    "        fun.aggregate = list(\n",
    "            total_cost_copy = \"sum\",\n",
    "            total_cost = \"max\"\n",
    "        )) %>%\n",
    "    dplyr::rename_with(~substr(., 1, 8), contains(c(\"_max\", \"_sum\"))) %>%\n",
    "    sparklyr::sdf_sort(c(\"animal_group\", \"origin_of_call\")) %>%\n",
    "    sparklyr::na.replace(0)\n",
    "\n",
    "rescue_pivot %>%\n",
    "    sparklyr::collect() %>%\n",
    "    print()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "</details>\n",
    "\n",
    "### Comparison with `pivot_wider()`\n",
    "\n",
    "This section is just for those interested in R and dplyr.\n",
    "\n",
    "<details>\n",
    "<summary><b>R Explanation</b></summary>\n",
    "\n",
    "`sdf_pivot()` can only be used on sparklyr DataFrames. If you have a base R DataFrame or tibble you can use [`tidyr::pivot_wider()`](https://tidyr.tidyverse.org/reference/pivot_wider.html). The [documentation](https://spark.rstudio.com/packages/sparklyr/latest/reference/sdf_pivot.html) for `sdf_pivot()` explains that it was based on `reshape2::dcast()`, but it is now recommended to use the `tidyr` package rather than `reshape2`. The syntax is different to `sdf_pivot()` and so it is worth looking at an example for comparison.\n",
    "\n",
    "First, filter the sparklyr DataFrame and convert to a tibble. Be careful when collecting data from the Spark cluster to the driver; in this example the `rescue` DataFrame is small, but it will not work if your DataFrame is large:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "rescue_tibble <- rescue %>%\n",
    "    sparklyr::filter(cal_year %in% c(\"2009\", \"2010\", \"2011\")) %>%\n",
    "    sparklyr::collect()\n",
    "\n",
    "# Check that this is a tibble\n",
    "class(rescue_tibble)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use `pivot_wider()`; note that rather than a formula with `~` it used `names_from` and `names_to`, and it groups by all columns not given in these arguments:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "tibble_pivot <- rescue_tibble %>%\n",
    "    sparklyr::select(animal_group, origin_of_call, cal_year, total_cost) %>%\n",
    "    tidyr::pivot_wider(\n",
    "        names_from = cal_year,\n",
    "        values_from = total_cost,\n",
    "        values_fn = list(total_cost = sum)) %>%\n",
    "    dplyr::arrange(animal_group, origin_of_call)\n",
    "    \n",
    "tibble_pivot %>%\n",
    "    print()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "### Further Resources\n",
    "\n",
    "PySpark Documentation:\n",
    "- [`.pivot()`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.GroupedData.pivot.html)\n",
    "- [`.groupBy()`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.groupBy.html)\n",
    "- [`.agg()`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.GroupedData.agg.html) \n",
    "\n",
    "sparklyr and tidyverse Documentation:\n",
    "- [`sdf_pivot()`](https://spark.rstudio.com/packages/sparklyr/latest/reference/sdf_pivot.html)\n",
    "- [`pivot_wider()`](https://tidyr.tidyverse.org/reference/pivot_wider.html)\n",
    "- [`rename_with()`](https://dplyr.tidyverse.org/reference/rename.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
