{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "  <img src=\"https://officenationalstatistics.sharepoint.com/:i:/r/sites/itoDSTPMO/DAPCATS/04.%20Technical/02.%20Development_Test/images_for_gitlab/dap-cats-ds-logo.png\"/>\n",
    "</div>\n",
    "\n",
    "## Tip of the Week: Use checkpoint to break DataFrame lineage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is lineage?\n",
    "\n",
    "As we know, Spark uses lazy evaluation. As we build up many transformations, e.g. `.withColumn()`, Spark creates an execution plan for the DataFrame and the plan is executed when an action is called, e.g. `show()`. This execution plan represents the DataFrame's lineage.\n",
    "\n",
    "Sometimes the DataFrame's lineage can grow long and complex, which will slow down the processing and maybe even throw an (unhelpful) error, however we can get around this by breaking the lineage.\n",
    "\n",
    "### What is `.checkpoint()`?\n",
    "\n",
    "There is more than one way of breaking the lineage, we've covered some ideas already in tip of the week, e.g. [using staging tables](http://np2rvlapxx507/DAP_CATS/troubleshooting/tip-of-the-week/blob/master/tip_15_staging_tables.ipynb). Here's one called checkpoint, which is essentially a short cut to writing the DataFrame out to disk and reading it back in. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How we will test it\n",
    "\n",
    "To demonstrate the benefit of using `.checkpoint()` is we'll time how long it takes to create a DataFrame without `.checkpoint()` then again with using `.checkpoint()`. \n",
    "\n",
    "We'll create a new Spark session each time just in case there's an advantage when processing the DataFrame a second time in the same session. For this reason we will put the Spark session into a function. We will also use the Python module `time` to measure the time taken to create the DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from time import time\n",
    "\n",
    "def spark_session():\n",
    "    return (\n",
    "        SparkSession.builder.appName(\"checkpoint-tip\")\n",
    "        .config(\"spark.executor.memory\", \"1g\")\n",
    "        .config(\"spark.executor.cores\", 1)\n",
    "        .config(\"spark.dynamicAllocation.enabled\", \"true\")\n",
    "        .config(\"spark.dynamicAllocation.maxExecutors\", 3)\n",
    "        .config(\"spark.sql.shuffle.partitions\", 12)\n",
    "        .config(\"spark.shuffle.service.enabled\", \"true\")\n",
    "        .config(\"spark.ui.showConsoleProgress\", \"false\")\n",
    "        .enableHiveSupport()\n",
    "        .getOrCreate()\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = spark_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to create a new DataFrame with an `id` column and a column called `col_0` that will consist of random numbers. We'll then create a loop to add new columns where the values depend on a previous column. In general, we try to avoid using loops with Spark and this example shows why. But sometimes using loops to create new columns is the only way of achieving what we need.\n",
    "\n",
    "We will set a `seed_num` when creating the random numbers to make the results repeatable. The DataFrame will have `num_rows` amount of rows, which we will set to a thousand and the loop will iterate 11 times to create `col_1` to `col_11`.\n",
    "\n",
    "The contents of the columns isn't important here. What is important is that Spark is creating an execution plan that it getting longer with each iteration of the loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cols = 12\n",
    "seed_num = 42\n",
    "num_rows = 10**3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without `.checkpoint()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+------+------+\n",
      "| id|col_0|col_1|col_2|col_3|col_4|col_5|col_6|col_7|col_8|col_9|col_10|col_11|\n",
      "+---+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+------+------+\n",
      "|  0|    8|    8|    8|    8|    8|    8|    8|    8|    0|    0|     0|     0|\n",
      "|  1|   11|   11|   11|   11|   11|   11|   11|   11|   11|   11|    11|     0|\n",
      "|  2|   11|   11|   11|   11|   11|   11|   11|   11|   11|   11|    11|     0|\n",
      "|  3|   11|   11|   11|   11|   11|   11|   11|   11|   11|   11|    11|     0|\n",
      "|  4|    6|    6|    6|    6|    6|    6|    0|    0|    0|    0|     0|     0|\n",
      "|  5|    7|    7|    7|    7|    7|    7|    7|    0|    0|    0|     0|     0|\n",
      "|  6|    1|    0|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|\n",
      "|  7|    2|    2|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|\n",
      "|  8|    4|    4|    4|    4|    0|    0|    0|    0|    0|    0|     0|     0|\n",
      "|  9|    9|    9|    9|    9|    9|    9|    9|    9|    9|    0|     0|     0|\n",
      "| 10|    1|    0|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|\n",
      "| 11|    1|    0|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|\n",
      "| 12|    6|    6|    6|    6|    6|    6|    0|    0|    0|    0|     0|     0|\n",
      "| 13|   11|   11|   11|   11|   11|   11|   11|   11|   11|   11|    11|     0|\n",
      "| 14|    5|    5|    5|    5|    5|    0|    0|    0|    0|    0|     0|     0|\n",
      "| 15|    6|    6|    6|    6|    6|    6|    0|    0|    0|    0|     0|     0|\n",
      "| 16|   11|   11|   11|   11|   11|   11|   11|   11|   11|   11|    11|     0|\n",
      "| 17|    9|    9|    9|    9|    9|    9|    9|    9|    9|    0|     0|     0|\n",
      "| 18|    3|    3|    3|    0|    0|    0|    0|    0|    0|    0|     0|     0|\n",
      "| 19|    1|    0|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|\n",
      "+---+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Time taken to create the DataFrame:  21.981037378311157\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "\n",
    "df = spark.range(num_rows)\n",
    "df = df.withColumn(\"col_0\", F.ceil(F.rand(seed_num) * new_cols))\n",
    "\n",
    "for i in range(1, new_cols):\n",
    "    df = df.withColumn(\"col_\"+str(i), F.when(F.col(\"col_\"+str(i-1)) > i, F.col(\"col_\"+str(i-1))).otherwise(0))\n",
    "\n",
    "df.show()\n",
    "\n",
    "time_taken = time() - start_time\n",
    "print(f\"Time taken to create the DataFrame:  {time_taken}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result above shows how long Spark took to create the plan and execute it to show the top 20 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "\n",
    "spark = spark_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With `.checkpoint()`\n",
    "\n",
    "Now let's do the same thing again, but this time we will checkpoint the DataFrame every 3 interations of the loop so that the lineage doesn't grow as long.\n",
    "\n",
    "To perform a checkpoint we need to set up a checkpoint directory on HDFS, which is where the checkpointed DataFrames will be stored. It's important to practice good housekeeping with this directory because new files are created with every checkpoint, but they are **not** automatically deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "username = os.getenv(\"HADOOP_USER_NAME\")\n",
    "checkpoint_path = f\"/user/{username}/checkpoints\"\n",
    "spark.sparkContext.setCheckpointDir(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+------+------+\n",
      "| id|col_0|col_1|col_2|col_3|col_4|col_5|col_6|col_7|col_8|col_9|col_10|col_11|\n",
      "+---+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+------+------+\n",
      "|  0|    8|    8|    8|    8|    8|    8|    8|    8|    0|    0|     0|     0|\n",
      "|  1|   11|   11|   11|   11|   11|   11|   11|   11|   11|   11|    11|     0|\n",
      "|  2|   11|   11|   11|   11|   11|   11|   11|   11|   11|   11|    11|     0|\n",
      "|  3|   11|   11|   11|   11|   11|   11|   11|   11|   11|   11|    11|     0|\n",
      "|  4|    6|    6|    6|    6|    6|    6|    0|    0|    0|    0|     0|     0|\n",
      "|  5|    7|    7|    7|    7|    7|    7|    7|    0|    0|    0|     0|     0|\n",
      "|  6|    1|    0|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|\n",
      "|  7|    2|    2|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|\n",
      "|  8|    4|    4|    4|    4|    0|    0|    0|    0|    0|    0|     0|     0|\n",
      "|  9|    9|    9|    9|    9|    9|    9|    9|    9|    9|    0|     0|     0|\n",
      "| 10|    1|    0|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|\n",
      "| 11|    1|    0|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|\n",
      "| 12|    6|    6|    6|    6|    6|    6|    0|    0|    0|    0|     0|     0|\n",
      "| 13|   11|   11|   11|   11|   11|   11|   11|   11|   11|   11|    11|     0|\n",
      "| 14|    5|    5|    5|    5|    5|    0|    0|    0|    0|    0|     0|     0|\n",
      "| 15|    6|    6|    6|    6|    6|    6|    0|    0|    0|    0|     0|     0|\n",
      "| 16|   11|   11|   11|   11|   11|   11|   11|   11|   11|   11|    11|     0|\n",
      "| 17|    9|    9|    9|    9|    9|    9|    9|    9|    9|    0|     0|     0|\n",
      "| 18|    3|    3|    3|    0|    0|    0|    0|    0|    0|    0|     0|     0|\n",
      "| 19|    1|    0|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|\n",
      "+---+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Time taken to create the DataFrame:  13.811508893966675\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "\n",
    "df = spark.range(num_rows)\n",
    "df = df.withColumn(\"col_0\", F.ceil(F.rand(seed_num) * new_cols))\n",
    "\n",
    "for i in range(1, new_cols):\n",
    "    df = df.withColumn(\"col_\"+str(i), F.when(F.col(\"col_\"+str(i-1)) > i, F.col(\"col_\"+str(i-1))).otherwise(0))\n",
    "    if i % 3 == 0: # this means if i is divisable by three then...\n",
    "        df = df.checkpoint() # here is the checkpoint\n",
    "        \n",
    "df.show()\n",
    "\n",
    "time_taken = time() - start_time\n",
    "print(f\"Time taken to create the DataFrame:  {time_taken}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exact times will vary with each run of this notebook, but hopefully you will see that using the `.checkpoint()` was more efficient.\n",
    "\n",
    "As mentioned earlier, the checkpoint files are not deleted on HDFS automatically. The files are not intended to be used after you stop the Spark session, so make sure you delete these files after a session. There have been cases in ONS where checkpoint directories have grown to be terabytes in size, which can create problems for other DAP users.\n",
    "\n",
    "The easiest way to delete files is in HUE, but the cell below is handy to have at the end of your scripts to make sure you don't forget to empty the checkpoint folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "cmd = f'hdfs dfs -rm -r -skipTrash {checkpoint_path}' \n",
    "p = subprocess.run(cmd, shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where `checkpoint_path` is the path defined in `.setCheckpointDir()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is 3 the magic number?\n",
    "\n",
    "How did we come up with the number 3 for number of iterations to checkpoint? Trial and error. Unfortunately, you may not have the luxury of trying to find the optimum number, but have a go at checkpointing and see if you can get any improvements in performance.\n",
    "\n",
    "### Why not use `.cache()`?\n",
    "\n",
    "There are various pros and cons involved between the different ways of persisting and `.cache()` and `.checkpoint()` work in different ways. For more information on the different ways of persisting data in Spark, please see our [persist](http://np2rvlapxx507/DAP_CATS/Training/more-spark/blob/master/more_spark/notebooks/material/persist.ipynb) notebook in the [More Spark](http://np2rvlapxx507/DAP_CATS/Training/more-spark) courses.\n",
    "\n",
    "### Documentation\n",
    "\n",
    "[`.checkpoint()`](https://spark.apache.org/docs/2.4.4/api/python/pyspark.sql.html?highlight=checkpoint#pyspark.sql.DataFrame.checkpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
