{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoints and Staging Tables\n",
    "\n",
    "### Persisting to disk\n",
    "\n",
    "Spark uses lazy evaluation. As we build up many transformations Spark creates an execution plan for the DataFrame and the plan is executed when an action is called. This execution plan represents the DataFrame's lineage.\n",
    "\n",
    "Sometimes the DataFrame's lineage can grow long and complex, which will slow down the processing and maybe even return an error. However, we can get around this by breaking the lineage.\n",
    "\n",
    "There is more than one way of breaking the lineage, this is discussed in more detail in the [Persisting](../spark-concepts/persistence) article. In this article we cover two simple methods of persisting to disk called checkpointing and staging tables. The former is essentially an out of the box shortcut to a write/read operation, where the latter is a more useful alternative to a checkpoint in which the data are written out as a Hive table in a database.\n",
    "\n",
    "## Checkpoint\n",
    "\n",
    "### Experiment\n",
    "\n",
    "To demonstrate the potential benefit of checkpointing we'll time how long it takes to create a DataFrame using an iterative calculation. We will run the process without persisting, then again using a checkpoint. \n",
    "\n",
    "We'll create a new Spark session each time just in case there's an advantage when processing the DataFrame a second time in the same session. We will also use the Python module [`time`](https://docs.python.org/3/library/time.html) to measure the time taken to create the DataFrame. \n",
    "\n",
    "We're going to create a new DataFrame with an `id` column and a column called `col_0` that will consist of random numbers. We'll then create a loop to add new columns where the values depend on a previous column. The contents of the columns isn't important here. What is important is that Spark is creating an execution plan that it getting longer with each iteration of the loop.\n",
    "\n",
    "In general, we try to avoid using loops with Spark and this example shows why. A better solution to this problem using Spark would be to add new rows with each iteration as opposed to columns.\n",
    "\n",
    "We will set a `seed_num` when creating the random numbers to make the results repeatable. The DataFrame will have `num_rows` amount of rows, which we will set to a thousand and the loop will iterate 11 times to create `col_1` to `col_11`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "Py4JError",
     "evalue": "org.apache.spark.api.python.PythonUtils.isEncryptionEnabled does not exist in the JVM",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-de20c31f32bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m spark = (SparkSession.builder.master(\"local[2]\")\n\u001b[0;32m----> 7\u001b[0;31m          \u001b[0;34m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"checkpoint\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m          .getOrCreate())\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/cdsw/.local/lib/python3.6/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    226\u001b[0m                             \u001b[0msparkConf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m                         \u001b[0;31m# This SparkContext may be an existing one.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m                         \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparkConf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m                     \u001b[0;31m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m                     \u001b[0;31m# by all sessions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/cdsw/.local/lib/python3.6/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    390\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/cdsw/.local/lib/python3.6/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\u001b[0;32m--> 147\u001b[0;31m                           conf, jsc, profiler_cls)\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0;31m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/cdsw/.local/lib/python3.6/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_do_init\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;31m# data via a socket.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;31m# scala's mangled names w/ $ in them require special treatment.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encryption_enabled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misEncryptionEnabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"SPARK_AUTH_SOCKET_TIMEOUT\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetPythonAuthSocketTimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/cdsw/.local/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1547\u001b[0m             raise Py4JError(\n\u001b[0;32m-> 1548\u001b[0;31m                 \"{0}.{1} does not exist in the JVM\".format(self._fqn, name))\n\u001b[0m\u001b[1;32m   1549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1550\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JError\u001b[0m: org.apache.spark.api.python.PythonUtils.isEncryptionEnabled does not exist in the JVM"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from time import time\n",
    "import yaml\n",
    "\n",
    "spark = (SparkSession.builder.master(\"local[2]\")\n",
    "         .appName(\"checkpoint\")\n",
    "         .getOrCreate())\n",
    "\n",
    "new_cols = 12\n",
    "seed_num = 42\n",
    "num_rows = 10**3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r \n",
    "library(sparklyr)\n",
    "library(dplyr)\n",
    "library(DBI)\n",
    "\n",
    "sc <- sparklyr::spark_connect(\n",
    "    master = \"local[2]\",\n",
    "    app_name = \"checkpoint\",\n",
    "    config = sparklyr::spark_config())\n",
    "\n",
    "\n",
    "set.seed(42)\n",
    "new_cols <- 12\n",
    "num_rows <- 10^3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without persisting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+------+------+\n",
      "| id|col_0|col_1|col_2|col_3|col_4|col_5|col_6|col_7|col_8|col_9|col_10|col_11|\n",
      "+---+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+------+------+\n",
      "|  0|    8|    8|    8|    8|    8|    8|    8|    8|    0|    0|     0|     0|\n",
      "|  1|   11|   11|   11|   11|   11|   11|   11|   11|   11|   11|    11|     0|\n",
      "|  2|   11|   11|   11|   11|   11|   11|   11|   11|   11|   11|    11|     0|\n",
      "|  3|   11|   11|   11|   11|   11|   11|   11|   11|   11|   11|    11|     0|\n",
      "|  4|    6|    6|    6|    6|    6|    6|    0|    0|    0|    0|     0|     0|\n",
      "|  5|    7|    7|    7|    7|    7|    7|    7|    0|    0|    0|     0|     0|\n",
      "|  6|    1|    0|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|\n",
      "|  7|    2|    2|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|\n",
      "|  8|    4|    4|    4|    4|    0|    0|    0|    0|    0|    0|     0|     0|\n",
      "|  9|    9|    9|    9|    9|    9|    9|    9|    9|    9|    0|     0|     0|\n",
      "+---+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+------+------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Time taken to create the DataFrame:  8.401437520980835\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "\n",
    "df = spark.range(num_rows)\n",
    "df = df.withColumn(\"col_0\", F.ceil(F.rand(seed_num) * new_cols))\n",
    "\n",
    "for i in range(1, new_cols):\n",
    "    df = (df.withColumn(\"col_\"+str(i), \n",
    "                        F.when(F.col(\"col_\"+str(i-1)) > i, \n",
    "                               F.col(\"col_\"+str(i-1)))\n",
    "                        .otherwise(0)))\n",
    "\n",
    "df.show(10)\n",
    "\n",
    "time_taken = time() - start_time\n",
    "print(f\"Time taken to create the DataFrame:  {time_taken}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "start_time <- Sys.time()\n",
    "\n",
    "df = sparklyr::sdf_seq(sc, 1, num_rows) %>%\n",
    "    sparklyr::mutate(col_0 = ceiling(rand()*new_cols))\n",
    "\n",
    "for (i in 1: new_cols)\n",
    "{\n",
    "  column_name = paste0('col_', i)\n",
    "  prev_column = paste0('col_', i-1)\n",
    "  df <- df %>%\n",
    "    mutate(\n",
    "      !!column_name := case_when(\n",
    "        !!as.symbol(prev_column) > i ~ !!as.symbol(prev_column)))\n",
    "  \n",
    "}\n",
    "\n",
    "df %>%\n",
    "    head(10)%>%\n",
    "    collect()%>%\n",
    "    print()\n",
    "\n",
    "end_time <- Sys.time()\n",
    "time_taken = end_time - start_time\n",
    "\n",
    "cat(\"Time taken to create DataFrame\", time_taken)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result above shows how long Spark took to create the plan and execute it to show the top 10 rows. \n",
    "\n",
    "#### With checkpoints\n",
    "\n",
    "Next we will stop the Spark session and start a new one to repeat the operation using checkpoints. \n",
    "\n",
    "To perform a checkpoint we need to set up a checkpoint directory on the file system, which is where the checkpointed DataFrames will be stored. It's important to practice good housekeeping with this directory because new files are created with every checkpoint, but they are **not automatically deleted**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "\n",
    "spark = (SparkSession.builder.master(\"local[2]\")\n",
    "         .appName(\"checkpoint\")\n",
    "         .getOrCreate())\n",
    "\n",
    "with open(\"../../../config.yaml\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "    \n",
    "checkpoint_path = config[\"checkpoint_path\"]\n",
    "spark.sparkContext.setCheckpointDir(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r \n",
    "\n",
    "spark_disconnect(sc)\n",
    "\n",
    "sc <- sparklyr::spark_connect(\n",
    "    master = \"local[2]\",\n",
    "    app_name = \"checkpoint\",\n",
    "    config = sparklyr::spark_config())\n",
    "\n",
    "\n",
    "config <- yaml::yaml.load_file(\"/home/cdsw/ons-spark/config.yaml\")\n",
    "\n",
    "spark_set_checkpoint_dir(sc, config$checkpoint_path)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will checkpoint the DataFrame every 3 iterations of the loop so that the lineage doesn't grow as long. Again, we will time how long it takes for Spark to complete the operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+------+------+\n",
      "| id|col_0|col_1|col_2|col_3|col_4|col_5|col_6|col_7|col_8|col_9|col_10|col_11|\n",
      "+---+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+------+------+\n",
      "|  0|    8|    8|    8|    8|    8|    8|    8|    8|    0|    0|     0|     0|\n",
      "|  1|   11|   11|   11|   11|   11|   11|   11|   11|   11|   11|    11|     0|\n",
      "|  2|   11|   11|   11|   11|   11|   11|   11|   11|   11|   11|    11|     0|\n",
      "|  3|   11|   11|   11|   11|   11|   11|   11|   11|   11|   11|    11|     0|\n",
      "|  4|    6|    6|    6|    6|    6|    6|    0|    0|    0|    0|     0|     0|\n",
      "|  5|    7|    7|    7|    7|    7|    7|    7|    0|    0|    0|     0|     0|\n",
      "|  6|    1|    0|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|\n",
      "|  7|    2|    2|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|\n",
      "|  8|    4|    4|    4|    4|    0|    0|    0|    0|    0|    0|     0|     0|\n",
      "|  9|    9|    9|    9|    9|    9|    9|    9|    9|    9|    0|     0|     0|\n",
      "+---+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+------+------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Time taken to create the DataFrame:  1.0542099475860596\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "\n",
    "df = spark.range(num_rows)\n",
    "df = df.withColumn(\"col_0\", F.ceil(F.rand(seed_num) * new_cols))\n",
    "\n",
    "for i in range(1, new_cols):\n",
    "    df = (df.withColumn(\"col_\"+str(i), \n",
    "                       F.when(F.col(\"col_\"+str(i-1)) > i, \n",
    "                              F.col(\"col_\"+str(i-1)))\n",
    "                       .otherwise(0)))\n",
    "    if i % 3 == 0: # this means if i is divisable by three then...\n",
    "        df = df.checkpoint() # here is the checkpoint\n",
    "        \n",
    "df.show(10)\n",
    "\n",
    "time_taken = time() - start_time\n",
    "print(f\"Time taken to create the DataFrame:  {time_taken}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r \n",
    "start_time <- Sys.time()\n",
    "\n",
    "df1 = sparklyr::sdf_seq(sc, 1, num_rows) %>%\n",
    "    sparklyr::mutate(col_0 = ceiling(rand()*new_cols))\n",
    "\n",
    "for (i in 1: new_cols)\n",
    "{\n",
    "  column_name = paste0('col_', i)\n",
    "  prev_column = paste0('col_', i-1)\n",
    "  df1 <- df1 %>%\n",
    "    mutate(\n",
    "    !!column_name := case_when(\n",
    "        !!as.symbol(prev_column) > i ~ !!as.symbol(prev_column) ))\n",
    "  \n",
    "  \n",
    "  if (i %% 3 == 0) \n",
    "  {\n",
    "    sdf_checkpoint(df1, eager= TRUE)\n",
    "  }\n",
    "}\n",
    "\n",
    "df1 %>%\n",
    "    head(10)%>%\n",
    "    collect()%>%\n",
    "    print()\n",
    "\n",
    "end_time <- Sys.time()\n",
    "time_taken = end_time - start_time\n",
    "\n",
    "\n",
    "cat(\"Time taken to create DataFrame: \", time_taken)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exact times will vary with each run of this notebook, but hopefully you will see that using the [`.checkpoint()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.checkpoint.html) was more efficient.\n",
    "\n",
    "As mentioned earlier, the checkpoint files are not deleted on HDFS automatically. The files are not intended to be used after you stop the Spark session, so make sure you delete these files after a session.\n",
    "\n",
    "Often the easiest way to delete files is through some GUI, but the cell below is handy to have at the end of your scripts when using checkpoints to make sure you don't forget to empty the checkpoint folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "cmd = f'hdfs dfs -rm -r -skipTrash {checkpoint_path}' \n",
    "p = subprocess.run(cmd, shell=True)\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r \n",
    "cmd <- paste0(\"hdfs dfs -rm -r -skipTrash \", config$checkpoint_path)\n",
    "p <- system(command = cmd)\n",
    "\n",
    "spark_disconnect(sc)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How often should I checkpoint?\n",
    "\n",
    "How did we come up with the number 3 for number of iterations to checkpoint? Trial and error. Unfortunately, you may not have the luxury of trying to find the optimum number, but if your execution is very long have a go at checkpointing and see if you can get any improvements in performance.\n",
    "\n",
    "More frequent checkpointing means more writing and reading data, which does take some time, but the aim is to save some time by simplifying the execution plan.\n",
    "\n",
    "As mentioned above, the use of loops shown here is not considered good practice with Spark, but it was a convenient example of using checkpoints. Of course, checkpointing can also be used outside loops, see the [Persisting](../spark-concepts/persistence) article for more information on the different forms of persisting data in Spark and their applications."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Staging Tables\n",
    "\n",
    "### The concept\n",
    "\n",
    "You can write a staging table to the file system (e.g. HDFS) with `df.write.mode(\"overwrite\").saveAsTable(table_name, format=\"parquet\")` or `df.write.insertInto(table_name, overwrite=True)`(of course, if using `.insertInto()` you will need to create the table first). You can then read the table back in with `spark.read.table()`. Like with checkpointing, this will break the lineage of the DataFrame, and therefore they can be useful in large, complex pipelines, or those that involve processes in a loop.\n",
    "\n",
    "Staging has some advantages over checkpointing:\n",
    "- The same table can be overwritten, meaning there is no need to clean up old checkpointed files\n",
    "- It is stored in a location that is easier to access, rather than the checkpointing folder, which can help with debugging and testing changes to the code\n",
    "- They can be re-used elsewhere, e.g. queried from a Hive engine\n",
    "- If `.insertInto()` is used, you can take advantage of the table schema, as an exception will be raised if the DataFrame and table schemas do not match\n",
    "- It is more efficient for Spark to read Hive tables than CSV files as the underlying format is most likely something like Parquet, so if your data are delivered as CSV files you may want to stage them as Hive tables first. \n",
    "\n",
    "There are also some disadvantages:\n",
    "- Takes longer to write the code\n",
    "- More difficult to maintain, especially if `.insertInto()` is used, as you will have to alter the table if the DataFrame structure changes\n",
    "- Ensure that you are not using them unnecessarily (the same is true with any method of persisting data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "Our example will be very simple, and show how to read a CSV file, perform some basic data cleansing, then stage as a Hive table, and then read it back in as a DataFrame. \n",
    "\n",
    "Often staging tables are most useful in large, complex pipelines; for obvious reasons our example will instead be simple!\n",
    "\n",
    "First, import the relevant modules and create a Spark session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F \n",
    "\n",
    "spark = (SparkSession.builder.master(\"local[2]\")\n",
    "         .appName(\"staging-tables\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r \n",
    "library(sparklyr)\n",
    "library(dplyr)\n",
    "\n",
    "sc <- sparklyr::spark_connect(\n",
    "    master = \"local[2]\",\n",
    "    app_name = \"staging_tables\",\n",
    "    config = sparklyr::spark_config())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now read in the CSV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rescue_path = config['rescue_path_csv']\n",
    "df = spark.read.csv(\"/training/animal_rescue.csv\", header = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r \n",
    "animal_rescue_csv = config$rescue_path_csv\n",
    "\n",
    "df = sparklyr::spark_read_csv(sc,\n",
    "                              path=animal_rescue_csv,\n",
    "                              header=TRUE,\n",
    "                              infer_schema=TRUE)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then do some preparation: drop and rename some columns, change the format, then sort.\n",
    "\n",
    "Note that if saving as a Hive table there are some stricter rules, including:\n",
    "- Some characters aren't allowed in column names, including `£`\n",
    "- The table won't load in some browser interfaces e.g. HUE, if you use date, but will accept a timestamp\n",
    "\n",
    "We then preview the DataFrame with `.toPandas()` (remember to use `.limit()` when looking at data in this way):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true,
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IncidentNumber</th>\n",
       "      <th>DateTimeOfCall</th>\n",
       "      <th>CalYear</th>\n",
       "      <th>FinYear</th>\n",
       "      <th>TypeOfIncident</th>\n",
       "      <th>EngineCount</th>\n",
       "      <th>JobHours</th>\n",
       "      <th>HourlyCost</th>\n",
       "      <th>TotalCost</th>\n",
       "      <th>Description</th>\n",
       "      <th>AnimalGroup</th>\n",
       "      <th>OriginOfCall</th>\n",
       "      <th>PropertyType</th>\n",
       "      <th>PropertyCategory</th>\n",
       "      <th>SpecialServiceTypeCategory</th>\n",
       "      <th>SpecialServiceType</th>\n",
       "      <th>Ward</th>\n",
       "      <th>Borough</th>\n",
       "      <th>StnGroundName</th>\n",
       "      <th>PostcodeDistrict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000014-03092018M</td>\n",
       "      <td>2018-09-03</td>\n",
       "      <td>2018</td>\n",
       "      <td>2018/19</td>\n",
       "      <td>Special Service</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>333</td>\n",
       "      <td>999.0</td>\n",
       "      <td>None</td>\n",
       "      <td>Unknown - Heavy Livestock Animal</td>\n",
       "      <td>Other FRS</td>\n",
       "      <td>Animal harm outdoors</td>\n",
       "      <td>Outdoor</td>\n",
       "      <td>Other animal assistance</td>\n",
       "      <td>Animal harm involving livestock</td>\n",
       "      <td>CARSHALTON SOUTH AND CLOCKHOUSE</td>\n",
       "      <td>SUTTON</td>\n",
       "      <td>Wallington</td>\n",
       "      <td>CR8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000099-01012017</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>2017</td>\n",
       "      <td>2016/17</td>\n",
       "      <td>Special Service</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>326</td>\n",
       "      <td>652.0</td>\n",
       "      <td>DOG WITH HEAD STUCK IN RAILINGS CALLED BY OWNER</td>\n",
       "      <td>Dog</td>\n",
       "      <td>Person (mobile)</td>\n",
       "      <td>Railings</td>\n",
       "      <td>Outdoor Structure</td>\n",
       "      <td>Other animal assistance</td>\n",
       "      <td>Assist trapped domestic animal</td>\n",
       "      <td>BROMLEY TOWN</td>\n",
       "      <td>BROMLEY</td>\n",
       "      <td>Bromley</td>\n",
       "      <td>BR2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000260-01012017</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>2017</td>\n",
       "      <td>2016/17</td>\n",
       "      <td>Special Service</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>326</td>\n",
       "      <td>326.0</td>\n",
       "      <td>BIRD TRAPPED IN NETTING BY THE 02 SHOP AND NEA...</td>\n",
       "      <td>Bird</td>\n",
       "      <td>Person (land line)</td>\n",
       "      <td>Single shop</td>\n",
       "      <td>Non Residential</td>\n",
       "      <td>Animal rescue from height</td>\n",
       "      <td>Animal rescue from height - Bird</td>\n",
       "      <td>Fairfield</td>\n",
       "      <td>CROYDON</td>\n",
       "      <td>Croydon</td>\n",
       "      <td>CR0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     IncidentNumber DateTimeOfCall CalYear  FinYear   TypeOfIncident  \\\n",
       "0  000014-03092018M     2018-09-03    2018  2018/19  Special Service   \n",
       "1   000099-01012017     2017-01-01    2017  2016/17  Special Service   \n",
       "2   000260-01012017     2017-01-01    2017  2016/17  Special Service   \n",
       "\n",
       "  EngineCount JobHours HourlyCost TotalCost  \\\n",
       "0         2.0      3.0        333     999.0   \n",
       "1         1.0      2.0        326     652.0   \n",
       "2         1.0      1.0        326     326.0   \n",
       "\n",
       "                                         Description  \\\n",
       "0                                               None   \n",
       "1    DOG WITH HEAD STUCK IN RAILINGS CALLED BY OWNER   \n",
       "2  BIRD TRAPPED IN NETTING BY THE 02 SHOP AND NEA...   \n",
       "\n",
       "                        AnimalGroup        OriginOfCall          PropertyType  \\\n",
       "0  Unknown - Heavy Livestock Animal           Other FRS  Animal harm outdoors   \n",
       "1                               Dog     Person (mobile)              Railings   \n",
       "2                              Bird  Person (land line)          Single shop    \n",
       "\n",
       "    PropertyCategory SpecialServiceTypeCategory  \\\n",
       "0            Outdoor    Other animal assistance   \n",
       "1  Outdoor Structure    Other animal assistance   \n",
       "2    Non Residential  Animal rescue from height   \n",
       "\n",
       "                 SpecialServiceType                             Ward  Borough  \\\n",
       "0   Animal harm involving livestock  CARSHALTON SOUTH AND CLOCKHOUSE   SUTTON   \n",
       "1    Assist trapped domestic animal                     BROMLEY TOWN  BROMLEY   \n",
       "2  Animal rescue from height - Bird                        Fairfield  CROYDON   \n",
       "\n",
       "  StnGroundName PostcodeDistrict  \n",
       "0    Wallington              CR8  \n",
       "1       Bromley              BR2  \n",
       "2       Croydon              CR0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = (df.\n",
    "    drop(\n",
    "        \"WardCode\", \n",
    "        \"BoroughCode\", \n",
    "        \"Easting_m\", \n",
    "        \"Northing_m\", \n",
    "        \"Easting_rounded\", \n",
    "        \"Northing_rounded\")\n",
    "    .withColumnRenamed(\"PumpCount\", \"EngineCount\")\n",
    "    .withColumnRenamed(\"FinalDescription\", \"Description\")\n",
    "    .withColumnRenamed(\"HourlyNotionalCost(£)\", \"HourlyCost\")\n",
    "    .withColumnRenamed(\"IncidentNotionalCost(£)\", \"TotalCost\")\n",
    "    .withColumnRenamed(\"OriginofCall\", \"OriginOfCall\")\n",
    "    .withColumnRenamed(\"PumpHoursTotal\", \"JobHours\")\n",
    "    .withColumnRenamed(\"AnimalGroupParent\", \"AnimalGroup\")\n",
    "    .withColumn(\n",
    "        \"DateTimeOfCall\", F.to_timestamp(F.col(\"DateTimeOfCall\"), \"dd/MM/yyyy\"))\n",
    "    .orderBy(\"IncidentNumber\")\n",
    "    )\n",
    "\n",
    "df.limit(3).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "df %>%\n",
    "    sparklyr::select( \n",
    "             -(\"WardCode\"), \n",
    "             -(\"BoroughCode\"), \n",
    "             -(\"Easting_m\"), \n",
    "             -(\"Northing_m\"), \n",
    "             -(\"Easting_rounded\"), \n",
    "             -(\"Northing_rounded\"), \n",
    "            \"EngineCount\" = \"PumpCount\",\n",
    "            \"Description\" = \"FinalDescription\",\n",
    "            \"HourlyCost\" = \"HourlyNotionalCostGBP\",\n",
    "            \"TotalCost\" = \"IncidentNotionalCostGBP\",\n",
    "            \"OriginOfCall\" = \"OriginofCall\",\n",
    "            \"JobHours\" = \"PumpHoursTotal\",\n",
    "            \"AnimalGroup\" = \"AnimalGroupParent\") %>%\n",
    "\n",
    "    sparklyr::mutate(DateTimeOfCall = to_date(DateTimeOfCall, \"dd/MM/yyyy\")) %>%\n",
    "    dplyr::arrange(desc(IncidentNumber)) %>%\n",
    "    head(3) %>%\n",
    "    sparklyr::collect() %>%\n",
    "    print() \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the plan with `df.explain()`. This displays what precisely Spark will do once an action is called (*lazy evaluation*). This is a simple example but in long pipelines this plan can get complicated. Using a staging table can split this process, referred to as *cutting the lineage*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) Sort [IncidentNumber#439 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(IncidentNumber#439 ASC NULLS FIRST, 200)\n",
      "   +- *(1) Project [IncidentNumber#439, cast(unix_timestamp(DateTimeOfCall#440, dd/MM/yyyy, Some(Europe/London)) as timestamp) AS DateTimeOfCall#658, CalYear#441, FinYear#442, TypeOfIncident#443, PumpCount#444 AS EngineCount#511, PumpHoursTotal#445 AS JobHours#616, HourlyNotionalCost(£)#446 AS HourlyCost#553, IncidentNotionalCost(£)#447 AS TotalCost#574, FinalDescription#448 AS Description#532, AnimalGroupParent#449 AS AnimalGroup#637, OriginofCall#450 AS OriginOfCall#595, PropertyType#451, PropertyCategory#452, SpecialServiceTypeCategory#453, SpecialServiceType#454, Ward#456, Borough#458, StnGroundName#459, PostcodeDistrict#460]\n",
      "      +- *(1) FileScan csv [IncidentNumber#439,DateTimeOfCall#440,CalYear#441,FinYear#442,TypeOfIncident#443,PumpCount#444,PumpHoursTotal#445,HourlyNotionalCost(£)#446,IncidentNotionalCost(£)#447,FinalDescription#448,AnimalGroupParent#449,OriginofCall#450,PropertyType#451,PropertyCategory#452,SpecialServiceTypeCategory#453,SpecialServiceType#454,Ward#456,Borough#458,StnGroundName#459,PostcodeDistrict#460] Batched: false, Format: CSV, Location: InMemoryFileIndex[hdfs://dnt01/training/animal_rescue.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<IncidentNumber:string,DateTimeOfCall:string,CalYear:string,FinYear:string,TypeOfIncident:s...\n"
     ]
    }
   ],
   "source": [
    "df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "explain(df)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now save the DataFrame as table, using `mode(\"overwrite\")`, which overwrites the existing table if there is one. The first time you create a staging table this option will be redundant, but on subsequent runs on the code you will get an error without this as the table will already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = os.getenv('HADOOP_USER_NAME') \n",
    "table_name = f\"train_tmp.staging_example_{username}\"\n",
    "\n",
    "df.write.mode(\"overwrite\").saveAsTable(table_name, format=\"parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "username <- Sys.getenv('HADOOP_USER_NAME')\n",
    "invisible(sdf_register(df, 'df'))\n",
    "sql <- paste0('DROP TABLE IF EXISTS train_tmp.staging_example_', username)\n",
    "dbExecute(sc, sql)\n",
    "\n",
    "tbl_change_db(sc, \"train_tmp\")\n",
    "table_name <- paste0('staging_example_', username) \n",
    "spark_write_table(df, name = table_name)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now read the data in again and preview:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true,
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IncidentNumber</th>\n",
       "      <th>DateTimeOfCall</th>\n",
       "      <th>CalYear</th>\n",
       "      <th>FinYear</th>\n",
       "      <th>TypeOfIncident</th>\n",
       "      <th>EngineCount</th>\n",
       "      <th>JobHours</th>\n",
       "      <th>HourlyCost</th>\n",
       "      <th>TotalCost</th>\n",
       "      <th>Description</th>\n",
       "      <th>AnimalGroup</th>\n",
       "      <th>OriginOfCall</th>\n",
       "      <th>PropertyType</th>\n",
       "      <th>PropertyCategory</th>\n",
       "      <th>SpecialServiceTypeCategory</th>\n",
       "      <th>SpecialServiceType</th>\n",
       "      <th>Ward</th>\n",
       "      <th>Borough</th>\n",
       "      <th>StnGroundName</th>\n",
       "      <th>PostcodeDistrict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>004812-12012017</td>\n",
       "      <td>2017-01-12</td>\n",
       "      <td>2017</td>\n",
       "      <td>2016/17</td>\n",
       "      <td>Special Service</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>326</td>\n",
       "      <td>326.0</td>\n",
       "      <td>CAT TRAPPED BETWEEN 2 WALLS  WEDGED BEHIND MET...</td>\n",
       "      <td>Cat</td>\n",
       "      <td>Person (mobile)</td>\n",
       "      <td>Fence</td>\n",
       "      <td>Outdoor Structure</td>\n",
       "      <td>Other animal assistance</td>\n",
       "      <td>Assist trapped domestic animal</td>\n",
       "      <td>HAMPTON</td>\n",
       "      <td>RICHMOND UPON THAMES</td>\n",
       "      <td>Twickenham</td>\n",
       "      <td>TW12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>004997-14012016</td>\n",
       "      <td>2016-01-14</td>\n",
       "      <td>2016</td>\n",
       "      <td>2015/16</td>\n",
       "      <td>Special Service</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>298</td>\n",
       "      <td>298.0</td>\n",
       "      <td>PIDGEON CAUGHT IN NETTING  CALL FOR ASSISTANCE...</td>\n",
       "      <td>Bird</td>\n",
       "      <td>Person (land line)</td>\n",
       "      <td>Electricity power station</td>\n",
       "      <td>Non Residential</td>\n",
       "      <td>Other animal assistance</td>\n",
       "      <td>Assist trapped wild animal</td>\n",
       "      <td>FIGGE'S MARSH</td>\n",
       "      <td>MERTON</td>\n",
       "      <td>Mitcham</td>\n",
       "      <td>CR4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>005140-12012017</td>\n",
       "      <td>2017-01-12</td>\n",
       "      <td>2017</td>\n",
       "      <td>2016/17</td>\n",
       "      <td>Special Service</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>326</td>\n",
       "      <td>326.0</td>\n",
       "      <td>CAT TRAPPED BEHIND CUPBOARD</td>\n",
       "      <td>Cat</td>\n",
       "      <td>Person (mobile)</td>\n",
       "      <td>Converted Flat/Maisonette - Up to 2 storeys</td>\n",
       "      <td>Dwelling</td>\n",
       "      <td>Other animal assistance</td>\n",
       "      <td>Assist trapped domestic animal</td>\n",
       "      <td>TOTTENHAM GREEN</td>\n",
       "      <td>HARINGEY</td>\n",
       "      <td>Tottenham</td>\n",
       "      <td>N15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    IncidentNumber DateTimeOfCall CalYear  FinYear   TypeOfIncident  \\\n",
       "0  004812-12012017     2017-01-12    2017  2016/17  Special Service   \n",
       "1  004997-14012016     2016-01-14    2016  2015/16  Special Service   \n",
       "2  005140-12012017     2017-01-12    2017  2016/17  Special Service   \n",
       "\n",
       "  EngineCount JobHours HourlyCost TotalCost  \\\n",
       "0         1.0      1.0        326     326.0   \n",
       "1         1.0      1.0        298     298.0   \n",
       "2         1.0      1.0        326     326.0   \n",
       "\n",
       "                                         Description AnimalGroup  \\\n",
       "0  CAT TRAPPED BETWEEN 2 WALLS  WEDGED BEHIND MET...         Cat   \n",
       "1  PIDGEON CAUGHT IN NETTING  CALL FOR ASSISTANCE...        Bird   \n",
       "2                        CAT TRAPPED BEHIND CUPBOARD         Cat   \n",
       "\n",
       "         OriginOfCall                                  PropertyType  \\\n",
       "0     Person (mobile)                                         Fence   \n",
       "1  Person (land line)                    Electricity power station    \n",
       "2     Person (mobile)  Converted Flat/Maisonette - Up to 2 storeys    \n",
       "\n",
       "    PropertyCategory SpecialServiceTypeCategory  \\\n",
       "0  Outdoor Structure    Other animal assistance   \n",
       "1    Non Residential    Other animal assistance   \n",
       "2           Dwelling    Other animal assistance   \n",
       "\n",
       "               SpecialServiceType             Ward               Borough  \\\n",
       "0  Assist trapped domestic animal          HAMPTON  RICHMOND UPON THAMES   \n",
       "1      Assist trapped wild animal    FIGGE'S MARSH                MERTON   \n",
       "2  Assist trapped domestic animal  TOTTENHAM GREEN              HARINGEY   \n",
       "\n",
       "  StnGroundName PostcodeDistrict  \n",
       "0    Twickenham             TW12  \n",
       "1       Mitcham              CR4  \n",
       "2     Tottenham              N15  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.table(table_name)\n",
    "df.limit(3).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "df <- spark_read_table(sc, table_name, repartition = 0)\n",
    "\n",
    "df %>%\n",
    "    head(3) %>%\n",
    "    sparklyr::collect() %>%\n",
    "    print()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DataFrame has the same structure as previously, but when we look at the plan with `df.explain()` we can see that less is being done. This is an example of cutting the lineage and can be useful when you have complex plans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) FileScan parquet train_tmp.staging_example_mitchs[IncidentNumber#739,DateTimeOfCall#740,CalYear#741,FinYear#742,TypeOfIncident#743,EngineCount#744,JobHours#745,HourlyCost#746,TotalCost#747,Description#748,AnimalGroup#749,OriginOfCall#750,PropertyType#751,PropertyCategory#752,SpecialServiceTypeCategory#753,SpecialServiceType#754,Ward#755,Borough#756,StnGroundName#757,PostcodeDistrict#758] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://dnt01/training/train_tmp/hive/staging_example_mitchs], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<IncidentNumber:string,DateTimeOfCall:timestamp,CalYear:string,FinYear:string,TypeOfInciden...\n"
     ]
    }
   ],
   "source": [
    "df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "explain(df)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `.insertInto()`\n",
    "\n",
    "Another method is to create an empty table and then use `.insertInto()`; here we will just use a small number of columns as an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_table = f\"train_tmp.staging_small_{username}\"\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE {small_table} (\n",
    "        IncidentNumber STRING,\n",
    "        CalYear INT,\n",
    "        EngineCount INT,\n",
    "        AnimalGroup STRING\n",
    "    )\n",
    "    STORED AS PARQUET\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the columns will be inserted by position, not name, so it's a good idea to re-select the column order to match that of the table before inserting in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_order = spark.read.table(small_table).columns\n",
    "df.select(col_order).write.insertInto(small_table, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can then be read in as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------+-----------+-----------+\n",
      "| IncidentNumber|CalYear|EngineCount|AnimalGroup|\n",
      "+---------------+-------+-----------+-----------+\n",
      "|004812-12012017|   2017|          1|        Cat|\n",
      "|004997-14012016|   2016|          1|       Bird|\n",
      "|005140-12012017|   2017|          1|        Cat|\n",
      "|005168-13012019|   2019|          1|        Cat|\n",
      "|005178-13012018|   2018|          1|       Bird|\n",
      "+---------------+-------+-----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.table(small_table)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we will drop the tables used in this example, which we can do with the `DROP` SQL statement. This is much easier than deleting a checkpointed file.\n",
    "\n",
    "Of course, with staging tables you generally want to keep the table, but just overwrite the data each time, so this step often won't be needed.\n",
    "\n",
    "Always be very careful when using `DROP` as this will delete the table without warning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"DROP TABLE {table_name}\")\n",
    "spark.sql(f\"DROP TABLE {small_table}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Resources\n",
    "\n",
    "Spark at the ONS Articles:\n",
    "- [Persisting](../../spark-concepts/persistence)\n",
    "- [Caching](../../spark-concepts/cache)\n",
    "\n",
    "PySpark Documentation:\n",
    "- [`.checkpoint()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.checkpoint.html)\n",
    "- [df.write.insertInto()](https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#pyspark.sql.DataFrameWriter.insertInto)\n",
    "- [df.write.saveAsTable()](https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#pyspark.sql.DataFrameWriter.saveAsTable)\n",
    "- [spark.read.csv()](https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.csv)\n",
    "- [spark.read.table()](https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.table)\n",
    "\n",
    "Python Documentation:\n",
    "- [`time`](https://docs.python.org/3/library/time.html)\n",
    "\n",
    "Other material:\n",
    "- <a href=\"https://en.wikipedia.org/wiki/Staging_(data)\">Staging (data) article on Wikipedia</a>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "9c57779df1dea1b9eb188cf873833e898918bc3cfb3a87a3118d4020e0c6a26f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
