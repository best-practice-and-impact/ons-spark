{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint and Staging tables\n",
    "\n",
    "## Persisting to disk\n",
    "\n",
    "Spark uses lazy evaluation. As we build up many transformations, e.g. `.withColumn()`, Spark creates an execution plan for the DataFrame and the plan is executed when an action is called, e.g. `show()`. This execution plan represents the DataFrame's lineage.\n",
    "\n",
    "Sometimes the DataFrame's lineage can grow long and complex, which will slow down the processing and maybe even return an error. However, we can get around this by breaking the lineage.\n",
    "\n",
    "There is more than one way of breaking the lineage, this is discussed in more detail in the **Persistance** article. In this article we cover two methods of persisting to disk. We will first look at `checkpoint`, which is using a shortcut to a write/read operation called `checkpoint`, the other is a more manual process to do essentially the same thing by writing a Hive table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment\n",
    "\n",
    "To demonstrate the benefit of persisting to disk we'll time how long it takes to create a DataFrame using an iterative calculation. We will run the process without persisting, then again using `.checkpoint()` , then a third time using staging tables. \n",
    "\n",
    "We'll create a new Spark session each time just in case there's an advantage when processing the DataFrame a second time in the same session. For this reason we will put the Spark session into a function. We will also use the Python module `time` to measure the time taken to create the DataFrame. \n",
    "\n",
    "### No persistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from time import time\n",
    "import yaml\n",
    "\n",
    "spark = (SparkSession.builder.master(\"local[2]\")\n",
    "         .appName(\"checkpoint\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to create a new DataFrame with an `id` column and a column called `col_0` that will consist of random numbers. We'll then create a loop to add new columns where the values depend on a previous column. In general, we try to avoid using loops with Spark and this example shows why. But sometimes using loops to create new columns is the only way of achieving what we need.\n",
    "\n",
    "We will set a `seed_num` when creating the random numbers to make the results repeatable. The DataFrame will have `num_rows` amount of rows, which we will set to a thousand and the loop will iterate 11 times to create `col_1` to `col_11`.\n",
    "\n",
    "The contents of the columns isn't important here. What is important is that Spark is creating an execution plan that it getting longer with each iteration of the loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cols = 12\n",
    "seed_num = 42\n",
    "num_rows = 10**3\n",
    "\n",
    "with open(\"../../../config.yaml\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "checkpoint_path = config[\"checkpoint_path\"] + \"/temp.parquet\"\n",
    "\n",
    "def write_delete(sdf, checkpoint_path):\n",
    "    sdf.write.mode(\"overwrite\").parquet(checkpoint_path)\n",
    "    cmd = f'rm -r -skipTrash {path}'\n",
    "    p = subprocess.run(cmd, shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without `.checkpoint()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+------+------+\n",
      "| id|col_0|col_1|col_2|col_3|col_4|col_5|col_6|col_7|col_8|col_9|col_10|col_11|\n",
      "+---+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+------+------+\n",
      "|  0|    8|    8|    8|    8|    8|    8|    8|    8|    0|    0|     0|     0|\n",
      "|  1|   11|   11|   11|   11|   11|   11|   11|   11|   11|   11|    11|     0|\n",
      "|  2|   11|   11|   11|   11|   11|   11|   11|   11|   11|   11|    11|     0|\n",
      "|  3|   11|   11|   11|   11|   11|   11|   11|   11|   11|   11|    11|     0|\n",
      "|  4|    6|    6|    6|    6|    6|    6|    0|    0|    0|    0|     0|     0|\n",
      "|  5|    7|    7|    7|    7|    7|    7|    7|    0|    0|    0|     0|     0|\n",
      "|  6|    1|    0|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|\n",
      "|  7|    2|    2|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|\n",
      "|  8|    4|    4|    4|    4|    0|    0|    0|    0|    0|    0|     0|     0|\n",
      "|  9|    9|    9|    9|    9|    9|    9|    9|    9|    9|    0|     0|     0|\n",
      "| 10|    1|    0|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|\n",
      "| 11|    1|    0|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|\n",
      "| 12|    6|    6|    6|    6|    6|    6|    0|    0|    0|    0|     0|     0|\n",
      "| 13|   11|   11|   11|   11|   11|   11|   11|   11|   11|   11|    11|     0|\n",
      "| 14|    5|    5|    5|    5|    5|    0|    0|    0|    0|    0|     0|     0|\n",
      "| 15|    6|    6|    6|    6|    6|    6|    0|    0|    0|    0|     0|     0|\n",
      "| 16|   11|   11|   11|   11|   11|   11|   11|   11|   11|   11|    11|     0|\n",
      "| 17|    9|    9|    9|    9|    9|    9|    9|    9|    9|    0|     0|     0|\n",
      "| 18|    3|    3|    3|    0|    0|    0|    0|    0|    0|    0|     0|     0|\n",
      "| 19|    1|    0|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|\n",
      "+---+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Time taken to create the DataFrame:  4.9744603633880615\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "\n",
    "df = spark.range(num_rows)\n",
    "df = df.withColumn(\"col_0\", F.ceil(F.rand(seed_num) * new_cols))\n",
    "\n",
    "for i in range(1, new_cols):\n",
    "    df = df.withColumn(\"col_\"+str(i), F.when(F.col(\"col_\"+str(i-1)) > i, F.col(\"col_\"+str(i-1))).otherwise(0))\n",
    "\n",
    "df.show()\n",
    "\n",
    "time_taken = time() - start_time\n",
    "print(f\"Time taken to create the DataFrame:  {time_taken}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result above shows how long Spark took to create the plan and execute it to show the top 20 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "\n",
    "spark = (SparkSession.builder.master(\"local[2]\")\n",
    "         .appName(\"checkpoint\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With `.checkpoint()`\n",
    "\n",
    "Now let's do the same thing again, but this time we will checkpoint the DataFrame every 3 interations of the loop so that the lineage doesn't grow as long.\n",
    "\n",
    "To perform a checkpoint we need to set up a checkpoint directory on HDFS, which is where the checkpointed DataFrames will be stored. It's important to practice good housekeeping with this directory because new files are created with every checkpoint, but they are **not** automatically deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "username = os.getenv(\"HADOOP_USER_NAME\")\n",
    "checkpoint_path = f\"/user/{username}/checkpoints\"\n",
    "spark.sparkContext.setCheckpointDir(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+------+------+\n",
      "| id|col_0|col_1|col_2|col_3|col_4|col_5|col_6|col_7|col_8|col_9|col_10|col_11|\n",
      "+---+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+------+------+\n",
      "|  0|    8|    8|    8|    8|    8|    8|    8|    8|    0|    0|     0|     0|\n",
      "|  1|   11|   11|   11|   11|   11|   11|   11|   11|   11|   11|    11|     0|\n",
      "|  2|   11|   11|   11|   11|   11|   11|   11|   11|   11|   11|    11|     0|\n",
      "|  3|   11|   11|   11|   11|   11|   11|   11|   11|   11|   11|    11|     0|\n",
      "|  4|    6|    6|    6|    6|    6|    6|    0|    0|    0|    0|     0|     0|\n",
      "|  5|    7|    7|    7|    7|    7|    7|    7|    0|    0|    0|     0|     0|\n",
      "|  6|    1|    0|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|\n",
      "|  7|    2|    2|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|\n",
      "|  8|    4|    4|    4|    4|    0|    0|    0|    0|    0|    0|     0|     0|\n",
      "|  9|    9|    9|    9|    9|    9|    9|    9|    9|    9|    0|     0|     0|\n",
      "| 10|    1|    0|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|\n",
      "| 11|    1|    0|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|\n",
      "| 12|    6|    6|    6|    6|    6|    6|    0|    0|    0|    0|     0|     0|\n",
      "| 13|   11|   11|   11|   11|   11|   11|   11|   11|   11|   11|    11|     0|\n",
      "| 14|    5|    5|    5|    5|    5|    0|    0|    0|    0|    0|     0|     0|\n",
      "| 15|    6|    6|    6|    6|    6|    6|    0|    0|    0|    0|     0|     0|\n",
      "| 16|   11|   11|   11|   11|   11|   11|   11|   11|   11|   11|    11|     0|\n",
      "| 17|    9|    9|    9|    9|    9|    9|    9|    9|    9|    0|     0|     0|\n",
      "| 18|    3|    3|    3|    0|    0|    0|    0|    0|    0|    0|     0|     0|\n",
      "| 19|    1|    0|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|\n",
      "+---+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Time taken to create the DataFrame:  1.1168088912963867\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "\n",
    "df = spark.range(num_rows)\n",
    "df = df.withColumn(\"col_0\", F.ceil(F.rand(seed_num) * new_cols))\n",
    "\n",
    "for i in range(1, new_cols):\n",
    "    df = df.withColumn(\"col_\"+str(i), F.when(F.col(\"col_\"+str(i-1)) > i, F.col(\"col_\"+str(i-1))).otherwise(0))\n",
    "    if i % 3 == 0: # this means if i is divisable by three then...\n",
    "        df = df.checkpoint() # here is the checkpoint\n",
    "        \n",
    "df.show()\n",
    "\n",
    "time_taken = time() - start_time\n",
    "print(f\"Time taken to create the DataFrame:  {time_taken}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exact times will vary with each run of this notebook, but hopefully you will see that using the `.checkpoint()` was more efficient.\n",
    "\n",
    "As mentioned earlier, the checkpoint files are not deleted on HDFS automatically. The files are not intended to be used after you stop the Spark session, so make sure you delete these files after a session. There have been cases in ONS where checkpoint directories have grown to be terabytes in size, which can create problems for other DAP users.\n",
    "\n",
    "The easiest way to delete files is in HUE, but the cell below is handy to have at the end of your scripts to make sure you don't forget to empty the checkpoint folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "cmd = f'hdfs dfs -rm -r -skipTrash {checkpoint_path}' \n",
    "p = subprocess.run(cmd, shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where `checkpoint_path` is the path defined in `.setCheckpointDir()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is 3 the magic number?\n",
    "\n",
    "How did we come up with the number 3 for number of iterations to checkpoint? Trial and error. Unfortunately, you may not have the luxury of trying to find the optimum number, but have a go at checkpointing and see if you can get any improvements in performance.\n",
    "\n",
    "### Why not use `.cache()`?\n",
    "\n",
    "There are various pros and cons involved between the different ways of persisting and `.cache()` and `.checkpoint()` work in different ways. For more information on the different ways of persisting data in Spark, please see our [persist](http://np2rvlapxx507/DAP_CATS/Training/more-spark/blob/master/more_spark/notebooks/material/persist.ipynb) notebook in the [More Spark](http://np2rvlapxx507/DAP_CATS/Training/more-spark) courses.\n",
    "\n",
    "### Documentation\n",
    "\n",
    "[`.checkpoint()`](https://spark.apache.org/docs/2.4.4/api/python/pyspark.sql.html?highlight=checkpoint#pyspark.sql.DataFrame.checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Staging Tables\n",
    "\n",
    "Staging tables are an alternative way of checkpointing data in Spark, in which the data is written out as a named Hive table in a database, rather than to the checkpointing location.\n",
    "\n",
    "### A quick recap of persisting\n",
    "\n",
    "Persisting in Spark is where we store the data at an intermediate point of the code in memory or on disk. This is generally done with `.cache()` (to store the data in memory) or `.checkpoint()` (to write it to disk). If you are not familiar with the concept of persisting, please read the [Persisting in Spark notebook](http://np2rvlapxx507/DAP_CATS/Training/more-spark/blob/master/more_spark/notebooks/material/persist.ipynb) which explains these concepts in more detail.\n",
    "\n",
    "Importantly, if your code is short and non-complex then it is unlikely you will need any form of persisting in your code, but if you have long and complex code sensible persisting can help make it more efficient.\n",
    "\n",
    "### Staging tables: the concept\n",
    "\n",
    "You can write a staging table to HDFS with `df.write.mode(\"overwrite\").saveAsTable(table_name, format=\"parquet\")` or `df.write.insertInto(table_name, overwrite=True)`(of course, if using `.insertInto()` you will need to create the table first). You can then read the table back in with `spark.read.table()`. Like with checkpointing, this will break the lineage of the DataFrame, and therefore they can be useful in large, complex pipelines, or those that involve processes in a loop. As Spark is more efficient at reading in tables than CSV files, another use case is staging CSV files as tables at the start of your code before doing any complex calculations.\n",
    "\n",
    "Staging has some advantages over checkpointing:\n",
    "- The same table can be overwritten, meaning there is no need to clean up old checkpointed files\n",
    "- It is stored in a location that is easier to access, rather than the checkpointing folder, which can help with debugging and testing changes to the code\n",
    "- They can be re-used elsewhere\n",
    "- If `.insertInto()` is used, you can take advantage of the table schema, as an exception will be raised if the DataFrame and table schemas do not match\n",
    "- It is more efficient for Spark to read Hive tables than CSV files as the underlying format is Parquet, so if your data are delivered as CSV files you may want to stage them as Hive tables first. For more information see the [Storing as a Parquet tip](http://np2rvlapxx507/DAP_CATS/troubleshooting/tip-of-the-week/blob/master/tip_18_parquet.ipynb).\n",
    "\n",
    "There are also some disadvantages:\n",
    "- Takes longer to write the code\n",
    "- More difficult to maintain, especially if `.insertInto()` is used, as you will have to alter the table if the DataFrame structure changes\n",
    "- Ensure that you are not using them unnecessarily (the same is true with any method of persisting data)\n",
    "\n",
    "The examples here use PySpark, but the same principles apply to R users who are using sparklyr in DAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = \"train_tmp.checkpoint\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'Cannot overwrite table train_tmp.checkpoint that is also being read from;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/opt/cloudera/parcels/CDH/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o893.saveAsTable.\n: org.apache.spark.sql.AnalysisException: Cannot overwrite table train_tmp.checkpoint that is also being read from;\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:437)\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:409)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-a0a8d0257d28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"col_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"col_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"col_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0motherwise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# this means if i is divisable by three then...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"iteration\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"overwrite\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"parquet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/CDH/lib/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msaveAsTable\u001b[0;34m(self, name, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    773\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 775\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/CDH/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'Cannot overwrite table train_tmp.checkpoint that is also being read from;'"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "\n",
    "df = spark.range(num_rows)\n",
    "df = df.withColumn(\"col_0\", F.ceil(F.rand(seed_num) * new_cols))\n",
    "\n",
    "for i in range(1, new_cols):\n",
    "    df = df.withColumn(\"col_\"+str(i), F.when(F.col(\"col_\"+str(i-1)) > i, F.col(\"col_\"+str(i-1))).otherwise(0))\n",
    "    if i % 3 == 0: # this means if i is divisable by three then...\n",
    "        df.withColumn(\"iteration\", F.lit(i)).write.mode(\"overwrite\").saveAsTable(table_name, format=\"parquet\")\n",
    "        df = spark.read.table(table_name)\n",
    "        \n",
    "df.show()\n",
    "\n",
    "time_taken = time() - start_time\n",
    "print(f\"Time taken to create the DataFrame:  {time_taken}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
