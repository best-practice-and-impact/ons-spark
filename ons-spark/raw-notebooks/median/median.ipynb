{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Median in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "Set-up Spark session and read config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "import yaml\n",
    "\n",
    "spark = (SparkSession.builder.master(\"local[2]\")\n",
    "         .appName(\"ons-spark\")\n",
    "         .getOrCreate())\n",
    "\n",
    "with open(\"../../../config.yaml\") as f:\n",
    "    config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "library(sparklyr)\n",
    "library(dplyr)\n",
    "\n",
    "sc <- sparklyr::spark_connect(\n",
    "  master = \"local[2]\",\n",
    "  app_name = \"ons-spark\",\n",
    "  config = sparklyr::spark_config(),\n",
    "  )\n",
    "\n",
    "config <- yaml::yaml.load_file(\"ons-spark/config.yaml\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in population dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- postcode_district: string (nullable = true)\n",
      " |-- population: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pop_df = spark.read.parquet(config[\"population_path\"])\n",
    "\n",
    "pop_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "pop_df <- sparklyr::spark_read_csv(sc, path = config$population_path)\n",
    "                                     \n",
    "sparklyr::sdf_schema(pop_df)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the median for a Spark DataFrame\n",
    "\n",
    "We can compute the medians for big data in Spark using the [Greenwald-Khanna](http://infolab.stanford.edu/~datar/courses/cs361a/papers/quantiles.pdf) algorithm that approximates a given quantile or quantiles of a distribution where the number of observations is very large. The relative error of the function can be adjusted to give a more accurate estimate for a given quantile at the cost of increased computation. \n",
    "\n",
    "In PySpark, the Greenwald-Khanna algorithm is implemented with [`approxQuantile`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.approxQuantile.html), which extends [`pyspark.sql.DataFrame`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/dataframe.html#). To find the exact median of the `population` column with PySpark, we apply the `approxQuantile` to our population DataFrame and specify the column name, the quantile of interest (in this case, the median or second quartile, 0.5), and the relative error, which is set to 0 to give the exact median.\n",
    "\n",
    "In SparklyR, the Greenwald-Khanna algorithm is implemented with [`sdf_quantile`](https://spark.rstudio.com/packages/sparklyr/latest/reference/sdf_quantile.html#sdf_quantile). To find the exact median of the `population` column with SparklyR, we apply `sdf_quantile` to our popuation DataFrame, specifying the same parameters as in the PySpark example - the quantile to compute (the median, 0.5) and the relative error (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[22331.0]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pop_df.approxQuantile(\"population\", [0.5], 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "sdf_quantile(pop_df, \"population\", probabilities = c(0.5), relative.error = 0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next example, we will compute the 1st, 2nd, and 3rd quartiles of the `population` column. We will assume that computing three quantiles would be too computationally expensive given our available resources, so we will increase the relative error parameter to reduce the accuracy of our estimates in return for decreased computation cost. We will increase the relative error parameter to 0.2, or 20%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9643.0, 19490.0, 28199.0]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pop_df.approxQuantile(\"population\", [0.25, 0.5, 0.75], 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "sdf_quantile(pop_df, \"population\", probabilities = c(0.25, 0.5, 0.75), relative.error = 0.2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the median for aggregated data\n",
    "\n",
    "Usually when performing data analysis you will want to find the median of aggregations created from your dataset, rather than just computing the median of entire columns. We will first read in the borough and postcode information from the animal rescue dataset and join these with the population dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+----------+\n",
      "|postcode_district|             borough|population|\n",
      "+-----------------+--------------------+----------+\n",
      "|             SE19|             Croydon|     27639|\n",
      "|             SE25|             Croydon|     34521|\n",
      "|              SM5|              Sutton|     38291|\n",
      "|              UB9|          Hillingdon|     14336|\n",
      "|              RM3|            Havering|     40272|\n",
      "|             RM10|Barking And Dagenham|     38157|\n",
      "|              E11|      Waltham Forest|     55128|\n",
      "|              E12|           Redbridge|     41869|\n",
      "|              CR0|             Croydon|    153812|\n",
      "|               E5|             Hackney|     47669|\n",
      "+-----------------+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "borough_df = spark.read.parquet(config[\"rescue_clean_path\"])\n",
    "\n",
    "borough_df = borough_df.select(\"borough\", F.upper(borough_df[\"postcodedistrict\"]).alias(\"postcode_district\"))\n",
    "\n",
    "pop_borough_df = borough_df.join(\n",
    "    pop_df,\n",
    "    on = \"postcode_district\",\n",
    "    how = \"left\"\n",
    ")\n",
    "\n",
    "pop_borough_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "borough_df <- sparklyr::spark_read_csv(sc, path = config$rescue_clean_path)\n",
    "\n",
    "borough_df <- borough_df %>%\n",
    "    select(borough, postcode_district = postcodedistrict) %>%\n",
    "    mutate(postcode_district = upper(postcode_district)\n",
    "    \n",
    "pop_borough_df <- borough_df %>%\n",
    "    left_join(pop_df, by = \"postcode_district\")\n",
    "    \n",
    "glimpse(pop_borough_df)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will aggregate the population data across boroughs in the combined `pop_borough_df` and find the median population in each borough.\n",
    "\n",
    "To acheive this in PySpark, we will register the [`percentile_approx`](https://docs.databricks.com/en/sql/language-manual/functions/percentile_approx.html) Hive UDF as a PySpark expression and apply it within an aggregation. The parameters required in `percentile_approx` are similar to that of `approxQuantile`, with the difference being that you must provide the accuracy instead of relative error, where accuracy is defined as $ \\frac{1}{relative\\ error} $"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "percentile_udf = F.expr('percentile_approx(population, 0.5, 10000)')\n",
    "\n",
    "median_pop = pop_borough_df.groupBy(\"borough\").agg(percentile_udf.alias(\"median_population\"))\n",
    "\n",
    "median_pop.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DAP environment is currently limited to using PySpark version 2.4.0, but there are two functions present in later versions that are useful for calculating quantiles/medians.\n",
    "\n",
    "- In PySpark 3.1.0, [`percentile_approx`](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.functions.percentile_approx.html) was added to [`pyspark.sql.functions`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html#), which allows you to use the `percentile_approx` Hive UDF in PySpark directly.\n",
    "\n",
    "``` python\n",
    "pop_borough_df.groupBy(\"borough\").agg(\n",
    "        percentile_approx(\"population\", 0.5, 10000).alias(\"median_population\")\n",
    ").show()\n",
    "```\n",
    "- In PySpark 3.4.0, [`median`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.median.html#pyspark.sql.functions.median) was added to `pyspark.sql.functions`, which further simplifies the process of of computing the median within aggregations, as it does not require a parameter specifying the quantile, or an accuracy parameter. \n",
    "\n",
    "``` python\n",
    "pop_borough_df.groupBy(\"borough\").agg(\n",
    "        median(\"population\").alias(\"median_population\")\n",
    ").show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Close spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "sparklyr::spark_disconnect(sc)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
