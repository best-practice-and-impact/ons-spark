{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# This hidden cell is used for local execution of the notebook file, it does not appear in the published book\n",
    "\n",
    "# Setup working directory\n",
    "import os, re\n",
    "\n",
    "dir_match = re.search(\n",
    "    r\".+ons-spark$\",\n",
    "    str(os.getcwd())\n",
    ")\n",
    "\n",
    "if not dir_match: os.chdir('../../..')\n",
    "\n",
    "# Setup Spark environment variables for local use\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Median in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "When working with big data, simple operations like computing the median can have significant computational costs associated with them. In Spark, estimation strategies are employed to compute medians and other quantiles, with the accuracy of the estimation being balanced against the computational cost.\n",
    "\n",
    "\n",
    "First we will set-up a Spark session and read in the config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "import yaml\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.master(\"local\")\n",
    "    .appName(\"ons-spark\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "with open(\"config.yaml\") as f:\n",
    "    config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "library(sparklyr)\n",
    "library(dplyr)\n",
    "library(knitr) # For pretty printing of tables\n",
    "\n",
    "sc <- sparklyr::spark_connect(\n",
    "  master = \"local\",\n",
    "  app_name = \"ons-spark\",\n",
    "  config = sparklyr::spark_config(),\n",
    "  )\n",
    "\n",
    "config <- yaml::yaml.load_file(\"config.yaml\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in population dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- postcode_district: string (nullable = true)\n",
      " |-- population: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pop_df = spark.read.parquet(config[\"population_path\"])\n",
    "\n",
    "pop_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "pop_df <- sparklyr::spark_read_parquet(sc, path = config$population_path)\n",
    "   \n",
    "sparklyr::sdf_schema(pop_df)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the median for a Spark DataFrame\n",
    "\n",
    "We can compute the medians for big data in Spark using the [Greenwald-Khanna](http://infolab.stanford.edu/~datar/courses/cs361a/papers/quantiles.pdf) algorithm that approximates a given quantile or quantiles of a distribution where the number of observations is very large. The relative error of the function can be adjusted to give a more accurate estimate for a given quantile at the cost of increased computation. \n",
    "\n",
    "In PySpark, the Greenwald-Khanna algorithm is implemented with [`approxQuantile`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.approxQuantile.html), which extends [`pyspark.sql.DataFrame`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/dataframe.html#). To find the exact median of the `population` column with PySpark, we apply the `approxQuantile` to our population DataFrame and specify the column name, an array containing the quantile of interest (in this case, the median or second quartile, 0.5), and the relative error, which is set to 0 to give the exact median.\n",
    "\n",
    "In SparklyR, the Greenwald-Khanna algorithm is implemented with [`sdf_quantile`](https://spark.rstudio.com/packages/sparklyr/latest/reference/sdf_quantile.html#sdf_quantile). To find the exact median of the `population` column with SparklyR, we apply `sdf_quantile` to our popuation DataFrame, specifying the same parameters as in the PySpark example - an array containing the quantile to compute (the median, 0.5) and the relative error (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[22331.0]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pop_df.approxQuantile(\"population\", [0.5], 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "sdf_quantile(pop_df, \"population\", probabilities = 0.5, relative.error = 0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next example, we will compute the 1st, 2nd, and 3rd quartiles of the `population` column, which we will do by passing an array containing the numeric values of the three quartiles as our second argument (0.25, 0.5 and 0.75, respectively). We will assume that computing three quantiles exactly would be too computationally expensive given our available resources, so we will increase the relative error parameter to reduce the accuracy of our estimates in return for decreased computation cost. We will increase the relative error parameter to 0.01, or 1%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12051.0, 22106.0, 33443.0]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pop_df.approxQuantile(\"population\", [0.25, 0.5, 0.75], 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "sdf_quantile(pop_df, \"population\", probabilities = c(0.25, 0.5, 0.75), relative.error = 0.01)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the relationship between the relative error parameter and the accuracy of the estimated result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exact = pop_df.approxQuantile(\"population\", [0.5], 0)[0]\n",
    "estimate = pop_df.approxQuantile(\"population\", [0.5], 0.01)[0]\n",
    "\n",
    "round((exact - estimate)/exact, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "exact = sdf_quantile(pop_df, \"population\", probabilities = c(0.5), relative.error = 0)[[1]]\n",
    "estimate = sdf_quantile(pop_df, \"population\", probabilities = c(0.5), relative.error = 0.01)[[1]]\n",
    "\n",
    "print(round((exact-estimate)/exact, 3))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By specificying a relative error of 1%, we can see that the difference between the estimated median for the `population` column and the exact median is approximately 1%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the median for aggregated data\n",
    "\n",
    "Usually when performing data analysis you will want to find the median of aggregations created from your dataset, rather than just computing the median of entire columns. We will first read in the borough and postcode information from the animal rescue dataset and join these with the population dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+----------+\n",
      "|postcode_district|             borough|population|\n",
      "+-----------------+--------------------+----------+\n",
      "|             SE19|             Croydon|     27639|\n",
      "|             SE25|             Croydon|     34521|\n",
      "|              SM5|              Sutton|     38291|\n",
      "|              UB9|          Hillingdon|     14336|\n",
      "|              RM3|            Havering|     40272|\n",
      "|             RM10|Barking And Dagenham|     38157|\n",
      "|              E11|      Waltham Forest|     55128|\n",
      "|              E12|           Redbridge|     41869|\n",
      "|              CR0|             Croydon|    153812|\n",
      "|               E5|             Hackney|     47669|\n",
      "+-----------------+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "borough_df = spark.read.parquet(config[\"rescue_clean_path\"])\n",
    "\n",
    "borough_df = borough_df.select(\n",
    "    \"borough\", \n",
    "    F.upper(F.col(\"postcodedistrict\")).alias(\"postcode_district\")\n",
    ")\n",
    "\n",
    "pop_borough_df = borough_df.join(\n",
    "    pop_df,\n",
    "    on = \"postcode_district\",\n",
    "    how = \"left\"\n",
    ").filter(F.col(\"borough\").isNotNull())\n",
    "\n",
    "pop_borough_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "borough_df <- sparklyr::spark_read_parquet(sc, path = config$rescue_clean_path)\n",
    "\n",
    "borough_df <- borough_df %>%\n",
    "    select(borough, postcode_district = postcodedistrict) %>%\n",
    "    mutate(postcode_district = upper(postcode_district))\n",
    "    \n",
    "pop_borough_df <- borough_df %>%\n",
    "    left_join(pop_df, by = \"postcode_district\") %>%\n",
    "    filter(!is.na(borough))\n",
    "    \n",
    "knitr::kable(collect(pop_borough_df, n=10))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will aggregate the population data across boroughs in the combined `pop_borough_df` and find the median postcode population for each borough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Spark 3.1.0, [`percentile_approx`](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.functions.percentile_approx.html) was added to [`pyspark.sql.functions`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html#), which allows you to calculate quantiles on aggregatations. The parameters required in `percentile_approx` are similar to that of `approxQuantile`, with the difference being that you must provide the accuracy instead of relative error, where accuracy is defined as $ \\frac{1}{relative\\ error} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------------+\n",
      "|             borough|median_postcode_population|\n",
      "+--------------------+--------------------------+\n",
      "|       Epping Forest|                   [23599]|\n",
      "|             Croydon|                   [48428]|\n",
      "|          Wandsworth|                   [64566]|\n",
      "|              Bexley|                   [34767]|\n",
      "|Kingston Upon Thames|                   [31384]|\n",
      "|             Lambeth|                   [43845]|\n",
      "|Kensington And Ch...|                   [22381]|\n",
      "|              Camden|                   [60910]|\n",
      "|           Brentwood|                   [24715]|\n",
      "|           Greenwich|                   [63082]|\n",
      "+--------------------+--------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pop_borough_df.groupBy(\"borough\").agg(\n",
    "        F.percentile_approx(\"population\", [0.5], 100000)\n",
    "        .alias(\"median_postcode_population\")\n",
    ").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Spark 3.4.0, [`median`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.median.html#pyspark.sql.functions.median) was added to `pyspark.sql.functions`, which further simplifies the process of computing the median within aggregations, as it does not require a parameter specifying the quantile, or an accuracy parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------------+\n",
      "|             borough|median_postcode_population|\n",
      "+--------------------+--------------------------+\n",
      "|       Epping Forest|                   23599.0|\n",
      "|             Croydon|                   48428.0|\n",
      "|          Wandsworth|                   64566.0|\n",
      "|              Bexley|                   34767.0|\n",
      "|Kingston Upon Thames|                   31384.0|\n",
      "|             Lambeth|                   43845.0|\n",
      "|Kensington And Ch...|                   22381.0|\n",
      "|              Camden|                   60910.0|\n",
      "|           Brentwood|                   24715.0|\n",
      "|           Greenwich|                   63082.0|\n",
      "+--------------------+--------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pop_borough_df.groupBy(\"borough\").agg(\n",
    "        F.median(\"population\")\n",
    "        .alias(\"median_postcode_population\")\n",
    ").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Spark 2.4.0, to calculate quantiles inside aggregations we can use the built-in Spark SQL [`approx_percentile`](https://spark.apache.org/docs/latest/api/sql/index.html#approx_percentile) function by passing SQL code to the PySpark API as a string inside of an `expr`. The parameters of the SQL function are identical to the PySpark `percentile_approx` function, however we must give the list of quantiles we wish to calculate inside of a SQL `array()`.\n",
    "\n",
    "In SparklyR, we are able to apply the `percentile_approx` function to an aggregation, inside of a `summarise` function. The arguments passed to `percentile_approx` in SparklyR are identical to the PySpark implementation of `percentile_approx`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------------+\n",
      "|             borough|median_postcode_population|\n",
      "+--------------------+--------------------------+\n",
      "|       Epping Forest|                   [23599]|\n",
      "|             Croydon|                   [48428]|\n",
      "|          Wandsworth|                   [64566]|\n",
      "|              Bexley|                   [34767]|\n",
      "|Kingston Upon Thames|                   [31384]|\n",
      "|             Lambeth|                   [43845]|\n",
      "|Kensington And Ch...|                   [22381]|\n",
      "|              Camden|                   [60910]|\n",
      "|           Brentwood|                   [24715]|\n",
      "|           Greenwich|                   [63082]|\n",
      "+--------------------+--------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pop_borough_df.groupBy(\"borough\").agg(\n",
    "        F.expr('approx_percentile(population, array(0.5), 100000)')\n",
    "        .alias(\"median_postcode_population\")\n",
    ").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "pop_borough_grouped_df <- pop_borough_df %>%\n",
    "    group_by(borough) %>%\n",
    "    summarise(median_postcode_population = percentile_approx(population, c(0.5), as.integer(100000)))\n",
    "\n",
    "knitr::kable(collect(pop_borough_grouped_df, n=10))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Close the Spark session to prevent any hanging processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "sparklyr::spark_disconnect(sc)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The relationship between relative error and computational cost\n",
    "\n",
    "The Greenwald-Khanna algorithm has a worst-case space requirement of $ O(\\frac{1}{\\epsilon}log(\\epsilon N)) $, where $ \\epsilon $ is the relative error and $ N $ is the size of our input data. Given a constant input size, empirical measurements show that increasing the precision of $\\epsilon$ by a factor of 10 from 0.1 to 0.01 leads to a space requirement increase of between 8 and 10 times, depending on the size of the input data. Increasing the precision of $\\epsilon$ by a factor of 10 from 0.01 to 0.001 leads to a smaller space requirement increase of between 7 and 9 times. For most Spark applications, you will be able to able to specify a high degree of precision in calculating quantiles using implementations of the Greenwald-Khanna algorithm due to the logarithmic relationship between relative error and space requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further reading\n",
    "\n",
    "- [Greenwald, Michael, and Sanjeev Khanna. “Space-efficient online computation of quantile summaries.”](http://infolab.stanford.edu/%7Edatar/courses/cs361a/papers/quantiles.pdf)\n",
    "\n",
    "- [Implementation of Greenwald-Khanna algorithm](https://aakinshin.net/posts/greenwald-khanna-quantile-estimator/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
