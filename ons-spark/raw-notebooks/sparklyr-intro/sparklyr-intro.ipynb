{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a954f9e0",
   "metadata": {},
   "source": [
    "## Introduction to sparklyr\n",
    "\n",
    "This article aims to give hands on experience in working with the sparklyr package in R. You can download the raw R code used in this article from the [ONS Spark repository](https://github.com/best-practice-and-impact/ons-spark/blob/main/ons-spark/raw-notebooks/sparklyr-intro/r_input.R).\n",
    "\n",
    "We will not aim to cover all the sparklyr DataFrame functionality or go into detail of how Spark works, but instead focus on practicality by performing some common operations on an example dataset. There are a handful of exercises that you can complete while reading this article.\n",
    "\n",
    "Prerequisites for this article are some basic knowledge of R and dplyr. If you are completely new to R then it is recommended to complete an introductory course first; your organisation may have specific R training. Other resources include the [Introduction to dplyr](https://dplyr.tidyverse.org/articles/dplyr.html) and [R for Data Science](https://r4ds.had.co.nz/).  If you are an Python user, the [Introduction to PySpark](../pyspark-intro/pyspark-intro) article follows similar format.\n",
    "\n",
    "### sparklyr: a quick introduction\n",
    "\n",
    "Although this article focusses on practical usage to enable you to quickly use sparklyr, you do need to understand some basic theory of Spark and distributed computing.\n",
    "\n",
    "Spark is a powerful tool used to process huge data in an efficient way. We can access Spark in R with the sparklyr package. Spark has DataFrames, consisting of rows and columns, similar to base R DataFrames or tibbles. Many of the operations are also translated directly from dplyr, some with a `spark_` or `sdf_` prefix: e.g. you can use [`select()`](https://dplyr.tidyverse.org/reference/select.html), [`mutate()`](https://dplyr.tidyverse.org/reference/mutate.html), [`filter()`](https://dplyr.tidyverse.org/reference/filter.html) and [`summarise()`](https://dplyr.tidyverse.org/reference/summarise.html) as you would in dplyr.\n",
    "\n",
    "The key difference between sparklyr and base R is where the DataFrame is processed:\n",
    "- base R DataFrames and tibbles are processed on the driver; this could be on a local machine using a desktop IDE such as RStudio, or on a server, e.g. in a dedicated Docker container (such as a CDSW session). The amount of data you can process is limited to the driver memory, so base R is suitable for smaller data.\n",
    "- sparklyr DataFrames are processed on the Spark cluster. This is a big pool of linked machines, called nodes. sparklyr DataFrames are distributed into partitions, and are processed in parallel on the nodes in the Spark cluster. You can have much greater memory capacity with Spark and so is suitable for big data.\n",
    "\n",
    "The DataFrame is also processed differently:\n",
    "- In base R and dplyr, the DataFrame changes in memory at each point, e.g. you could create a DataFrame by reading from a CSV file, select some columns, filter the rows, add a column with `mutate()` and then write the data out. With each operation, the DataFrame is physically changing in memory. This can be useful for debugging as it is easy to see intermediate outputs.\n",
    "- In sparklyr, DataFrames are lazily evaluated. We give Spark a set of instructions, called transformations, which are only evaluated when necessary, for instance to get a row count or write out data to a file, referred to as an action. In the example above, the plan is triggered once the data are set to write out to a file.\n",
    "\n",
    "For more detail on how Spark works, you can refer to the articles in the Understanding and Optimising Spark chapter of this book. [Databricks: Learning Spark](https://pages.databricks.com/rs/094-YMS-629/images/LearningSpark2.0.pdf) is another useful resource.\n",
    "\n",
    "### The `sparklyr` package\n",
    "\n",
    "To use Spark with R, we make use of the [`sparklyr`](https://spark.rstudio.com/) package. There is also the [SparkR](https://spark.apache.org/docs/latest/sparkr.html) package which is an alternative, although this is not used at the ONS.\n",
    "\n",
    "It is recommended to install the full [`tidyverse`](https://www.tidyverse.org/) first before installing `sparklyr`, to ensure that you take take advantage of the full functionality of dplyr and related packages.\n",
    "\n",
    "When referencing functions from `sparklyr`, you can either import the package with `library()` or `require()`, or reference the function directly with `::`, e.g. `sparklyr::select()`. If you reference directly you will need to import `magrittr` to take advantage of the `%>%` pipes. See the article on [Avoiding Module Import Conflicts](../ancillary-topics/module-imports) for more information on referencing packages.\n",
    "\n",
    "In this article we both reference the packages directly and also import the packages, so if you run the raw code you can use either method. Knowing where a package comes from can be tricky when using sparklyr, so although the generally accepted good practice is to directly reference with `::` you may find it easier to just import the packages and use the functions without referencing.\n",
    "\n",
    "We also import `dplyr`, as we will frequently be converting sparklyr DataFrames to tibbles. As part of the setup we also read in a config file using `yaml` although as this is only being used once the package is directly referenced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49260150",
   "metadata": {},
   "source": [
    "```r\n",
    "library(sparklyr)\n",
    "library(dplyr)\n",
    "library(magrittr)\n",
    "\n",
    "config <- yaml::yaml.load_file(\"ons-spark/config.yaml\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe57f94",
   "metadata": {},
   "source": [
    "The help functionality in R works as usual with sparklyr functions, and can be accessed by prefixing a function with `?`, e.g. `?spark_read_csv`, although the easiest way is to look at the [documentation](https://spark.rstudio.com/packages/sparklyr/latest/reference/). There are a lot of functions in the sparklyr module and you will be very unlikely to use them all.\n",
    "\n",
    "### Create a Spark session: `spark_connect()`\n",
    "\n",
    "With the `sparklyr` package imported we now want to create a connection to the Spark cluster. We use [`spark_connect()`](https://spark.rstudio.com/packages/sparklyr/latest/reference/spark-connections.html) and assign this to `sc`.\n",
    "\n",
    "`spark_connect()` takes a [`spark_config()`](https://spark.rstudio.com/packages/sparklyr/latest/reference/spark_config.html) as an input and this has many options for controlling the size of the Spark session; see the [Guidance on Spark Sessions](../spark-overview/spark-session-guidance) and also [Example Spark Sessions](../spark-overview/example-spark-sessions) to get an idea of what sized session to use.\n",
    "\n",
    "For this article, we are using a tiny dataset by Spark standards, and so are using a local session. This also means that you can run this code without having access to a Spark cluster.\n",
    "\n",
    "Note that only one Spark session can be running at once. If a session already exists then a new one will not be created, instead the connection to the existing session will be used.\n",
    "\n",
    "We can confirm that we have a running Spark session by checking that the output of [`spark_connection_is_open()`](https://spark.rstudio.com/packages/sparklyr/latest/reference/spark-connections.html) is `TRUE`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311d1d97",
   "metadata": {},
   "source": [
    "```r\n",
    "sc <- sparklyr::spark_connect(\n",
    "  master = \"local[2]\",\n",
    "  app_name = \"sparklyr-intro\",\n",
    "  config = sparklyr::spark_config())\n",
    "  \n",
    "sparklyr::spark_connection_is_open(sc)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3385e64",
   "metadata": {},
   "source": [
    "### Reading data: `spark_read_csv()`\n",
    "\n",
    "For this article we will look at some open data on animal rescue incidents from the London Fire Brigade. The data are stored as a CSV, although the parquet file format is the most common when using Spark. The reason for using CSV in this article is because it is a familiar file format and allows you to adapt this code easily for your own sample data. See the article on [Reading Data in sparklyr](../pyspark-intro/reading-data-sparklyr) for more information.\n",
    "\n",
    "Often your data will be large and stored using Hadoop, on the Hadoop Distributed File System (HDFS). This example uses a local file, enabling us to get started quickly; see the article on [Data Storage](../spark-overview/data-storage) for more information.\n",
    "\n",
    "To read in from a CSV file, use [`spark_read_csv()`](https://spark.rstudio.com/packages/sparklyr/latest/reference/spark_read_csv.html). The first argument is to supply `sc`, which is the open Spark connection defined from `spark_connect()` previously. The file path is stored in the config file as `rescue_path_csv`. Using `header=TRUE` means that the DataFrame will use the column headers from the CSV as the column names. CSV files do not contain information about the [data types](../spark-overview/data-types), so use `infer_schema=TRUE` which makes Spark scan the file to infer the data types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743be4fc",
   "metadata": {},
   "source": [
    "```r\n",
    "rescue_path = config$rescue_path_csv\n",
    "\n",
    "rescue <- sparklyr::spark_read_csv(\n",
    "    sc,\n",
    "    path=rescue_path, \n",
    "    header=TRUE,\n",
    "    infer_schema=TRUE)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c70348",
   "metadata": {},
   "source": [
    "We can use [`class(rescue)`](https://stat.ethz.ch/R-manual/R-devel/library/base/html/class.html) to see that `rescue` is now a sparklyr DataFrame:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4bbf20",
   "metadata": {},
   "source": [
    "```r\n",
    "class(rescue)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a547f924",
   "metadata": {},
   "source": [
    "### Preview data: `glimpse()`\n",
    "\n",
    "To view the column names, data types and get a short preview of the data use [`glimpse()`](https://pillar.r-lib.org/reference/glimpse.html) from the `pillar` package. These will be transposed in the display, meaning this option is good for previewing DataFrames with a large number of columns.\n",
    "\n",
    "Note that `glimpse()` is an *action* and the DataFrame will get lazily evaluated when this is used. As such, `glimpse()` can take a long time when you have a lot of code. As we are only previewing a small DataFrame that is directly imported from CSV it will be relatively quick here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ea970f",
   "metadata": {},
   "source": [
    "```r\n",
    "pillar::glimpse(rescue)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04f7dd3",
   "metadata": {},
   "source": [
    "Note that when using `glimpse()` the number of rows are given as `??`. Spark uses lazy evaluation and it can return a preview of the data without having to derive the row count.\n",
    "\n",
    "### Implicitly Preview data: `print()`\n",
    "\n",
    "The DataFrame can be implicitly previewed with `print()` or by supplying the DataFrame name without assignment or a function.\n",
    "\n",
    "These are interpreted as *actions*, meaning that all previous *transformations* will be ran on the Spark cluster; often this will have many *transformations*, but here we only have one, reading in the data.\n",
    "\n",
    "By default 10 rows will be displayed; this can be changed by setting `n`, e.g. `n=5`. When there are many columns the output will be truncated, so this option is often not practical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e00d3b",
   "metadata": {},
   "source": [
    "```r\n",
    "rescue\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e607fcff",
   "metadata": {},
   "source": [
    "```r\n",
    "print(rescue, n=5)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85245c13",
   "metadata": {},
   "source": [
    "### Convert to a tibble: `collect()` and `head()`\n",
    "\n",
    "`glimpse()` and `print()` both only give us a partial preview of the rows in the DataFrame, and do not assign the results to a variable. [`collect()`](https://dplyr.tidyverse.org/reference/compute.html) will bring the sparklyr DataFrame into the driver from the cluster as a tibble, and you can therefore take advantage of all the usual tibble and base R functionality. For instance, you may want to [visualise your results](../ancillary-topics/visualisation).\n",
    "\n",
    "In some IDEs, printing a tibble will display a nicely formatted HTML table, so this is another option when previewing data.\n",
    "\n",
    "Be careful: the DataFrame is currently on the *Spark cluster* with lots of memory capacity, whereas tibbles are stored on the *driver*, which will have much less. Trying to use `collect()` and a huge sparklyr DF will not work. If converting to a tibble just to view the data, use `head()` to just bring back a small number of rows. Note that [sparklyr DataFrames are not ordered by default](../spark-concepts/df-order), and `head()` on a sparklyr DataFrame will **not** always return the same rows. This is [discussed in more detail later](sort-data).\n",
    "\n",
    "`collect()` is an *action* and will process the whole plan on the Spark cluster. In this example, it will read the CSV file, return three rows, and then convert the result to the driver as a tibble. Note that the number of rows is now `3` rather than `??`, as tibbles have known dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea419a2",
   "metadata": {},
   "source": [
    "```r\n",
    "rescue_tibble <- rescue %>%\n",
    "    head(3) %>%\n",
    "    sparklyr::collect()\n",
    "    \n",
    "rescue_tibble %>%\n",
    "    print()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6170d68f",
   "metadata": {},
   "source": [
    "As the syntax in sparklyr inherits so much from dplyr new users can sometimes get confused over what type of DataFrame they have. `class()` is useful here. We can confirm that `rescue_tibble` is indeed a tibble:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05347768",
   "metadata": {},
   "source": [
    "```r\n",
    "class(rescue_tibble)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325d2a70",
   "metadata": {},
   "source": [
    "### Select and drop columns: `select()`\n",
    "\n",
    "Often your data will have too many columns that are not relevant, so we can use `select()` to just get the ones that are of interest. In our example, we can reduce the number of columns returned so that the output of `print()` is much neater.\n",
    "\n",
    "[`select()`](https://dplyr.tidyverse.org/reference/select.html) in sparklyr works in the same way as in dplyr; simply provide the names of the columns. There are no need for vectors, lists or quotes around column names. It is recommended to use the pipe (`%>%`), which means the first argument is the result of the previous operation. Using `%>%` makes the code easier to read and also more instinctive, since the only inputs to `select()` are the columns, not the DataFrame.\n",
    "\n",
    "Selecting columns is a *transformation*, and so will only be processed once an *action* is called. As such we are chaining this with `collect()`, which will bring the data into the driver. Remember to use `head()` if collecting the data:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e589f952",
   "metadata": {},
   "source": [
    "```r\n",
    "rescue %>%\n",
    "    sparklyr::select(IncidentNumber, DateTimeOfCall, FinalDescription) %>%\n",
    "    head(5) %>%\n",
    "    sparklyr::collect() %>%\n",
    "    print()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd04b10",
   "metadata": {},
   "source": [
    "`select()` can also be used to drop columns. Simply specify `-` before a column name to remove it. There are a lot of columns related to the location of the animal rescue incidents that we will not use that can be removed with `select(-column)`.\n",
    "\n",
    "Note that we have written over the our previous DataFrame by re-assiging to `rescue`; Spark DFs are *immutable*.\n",
    "\n",
    "We then use `glimpse()` to verify that the columns have been removed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e52d60",
   "metadata": {},
   "source": [
    "```r\n",
    "rescue <- rescue %>%\n",
    "    sparklyr::select(\n",
    "        -WardCode,\n",
    "        -BoroughCode,\n",
    "        -Easting_m,\n",
    "        -Northing_m,\n",
    "        -Easting_rounded,\n",
    "        -Northing_rounded)\n",
    "        \n",
    "pillar::glimpse(rescue)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370e82de",
   "metadata": {},
   "source": [
    "### Get the row count: `sdf_nrow()`\n",
    "\n",
    "Sometimes you data will be small enough that you do not even need to use Spark. As such it is useful to know the row count, and then make the decision on whether to use Spark or just use base R or dplyr. One advantage of base R DataFrames is that there are more packages available than there are with Spark.\n",
    "\n",
    "To get the row count in sparklyr, use [`sdf_nrow()`](https://spark.rstudio.com/packages/sparklyr/latest/reference/sdf_dim.html). This will only work on sparklyr DFs, not tibbles or base R DFs.\n",
    "\n",
    "We saw earlier that unlike tibbles the row count is not automatically determined when the data are read in as a sparklyr DataFrame; remember that `glimpse()` had a row count of `??`. This is an example of *lazy evaluation*. As such, `sdf_nrow()` is an *action* and has to be explicitly called."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28eea0d3",
   "metadata": {},
   "source": [
    "```r\n",
    "rescue %>%\n",
    "    sparklyr::sdf_nrow()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e615563",
   "metadata": {},
   "source": [
    "### Rename columns: `rename()`\n",
    "\n",
    "The source data has the column names in `CamelCase`, but when using R we generally prefer to use `snake_case`.\n",
    "\n",
    "To rename columns, use [`rename()`](https://dplyr.tidyverse.org/reference/rename.html). Like `select()`, usage is the same as dplyr and is actually referenced directly with `dplyr::rename()`. Assign the new column name to be the old one with `=`, e.g `rename(incident_number = IncidentNumber)`. Once again we are re-assigning the DataFrame as it is immutable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a0d2d8",
   "metadata": {},
   "source": [
    "```r\n",
    "rescue <- rescue %>%\n",
    "    dplyr::rename(\n",
    "        incident_number = IncidentNumber,\n",
    "        date_time_of_call = DateTimeOfCall,\n",
    "        animal_group = AnimalGroupParent,\n",
    "        cal_year = CalYear,\n",
    "        total_cost = IncidentNotionalCostGBP,\n",
    "        job_hours = PumpHoursTotal,\n",
    "        engine_count = PumpCount)\n",
    "\n",
    "pillar::glimpse(rescue)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f1622e",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "#### Exercise 1a\n",
    "\n",
    "Rename the following columns in the `rescue` DataFrame:\n",
    "\n",
    "`FinalDescription` --> `description`\n",
    "\n",
    "`PostcodeDistrict` --> `postcode_district`\n",
    "\n",
    "#### Exercise 1b\n",
    "\n",
    "Select these columns and the seven columns that were renamed in the cell above; you should have nine in total. Reassign the result to the `rescue` DataFrame.\n",
    "\n",
    "#### Exercise 1c\n",
    "\n",
    "Preview the structure of the DataFrame.\n",
    "\n",
    "<details>\n",
    "<summary><b>Exercise 1: Solution</b></summary>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de276530",
   "metadata": {},
   "source": [
    "```r\n",
    "# 1a: Use rename. Remember to put the name of the new column first.\n",
    "rescue <- rescue %>%\n",
    "    dplyr::rename(\n",
    "        description = FinalDescription,\n",
    "        postcode_district = PostcodeDistrict)\n",
    "\n",
    "# 1b: Select the nine columns and assign to rescue\n",
    "rescue <- rescue %>%\n",
    "    sparklyr::select(\n",
    "        incident_number,\n",
    "        date_time_of_call,\n",
    "        animal_group,\n",
    "        cal_year,\n",
    "        total_cost,\n",
    "        job_hours,\n",
    "        engine_count,\n",
    "        description,\n",
    "        postcode_district)\n",
    "\n",
    "# 1c Preview with glimpse()\n",
    "# As we have nine columns it is easier to view the transposed output\n",
    "rescue %>%\n",
    "    pillar::glimpse()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f286043",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "### Filter rows: `filter()`\n",
    "\n",
    "Rows of sparklyr DataFrames can be filtered with [`filter()`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.filter.html), which takes a logical condition. This works in an identical way to `dplyr::filter()`. Any column names can be referenced by name without quotes.\n",
    "\n",
    "For instance, if we want to select all the rows where `animal_group` is equal to `Hamster`, we can use `filter(animal_group == \"Hamster\")`. Note the double equals sign used in a condition. We do not want to change the `rescue` DataFrame, so assign it to a new DF, `hamsters`, then preview a few of the columns:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3f199e",
   "metadata": {},
   "source": [
    "```r\n",
    "hamsters <- rescue %>%\n",
    "    sparklyr::filter(animal_group == \"Hamster\")\n",
    "\n",
    "hamsters %>%\n",
    "    sparklyr::select(\n",
    "        incident_number,\n",
    "        animal_group,\n",
    "        cal_year,\n",
    "        total_cost)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdffc88",
   "metadata": {},
   "source": [
    "For multiple conditions putting each condition on a new line makes the code easier to read:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d9c839",
   "metadata": {},
   "source": [
    "```r\n",
    "expensive_olympic_dogs <- rescue %>%\n",
    "    sparklyr::filter(\n",
    "        animal_group == \"Dog\" &\n",
    "        total_cost >= 750 &\n",
    "        cal_year == \"2012\")\n",
    "\n",
    "expensive_olympic_dogs %>%\n",
    "    sparklyr::select(\n",
    "        incident_number,\n",
    "        animal_group,\n",
    "        cal_year,\n",
    "        total_cost)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5015682b",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Create a new DataFrame which consists of all the rows where `animal_group` is equal to `\"Fox\"`, then select `incident_number`, `animal_group`, `cal_year`, and `total_cost` and preview the first ten rows.\n",
    "\n",
    "<details>\n",
    "<summary><b>Exercise 2: Solution</b></summary>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad42636",
   "metadata": {},
   "source": [
    "```r\n",
    "# Use filter(), ensuring that a new DataFrame is created\n",
    "foxes <- rescue %>%\n",
    "    sparklyr::filter(animal_group == \"Fox\")\n",
    "\n",
    "# Preview with head(10)\n",
    "foxes %>%\n",
    "    sparklyr::select(\n",
    "        incident_number,\n",
    "        animal_group,\n",
    "        cal_year,\n",
    "        total_cost) %>%\n",
    "    head(10)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad73ef9",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "### Adding Columns: `mutate()`\n",
    "\n",
    "[`mutate()`](https://dplyr.tidyverse.org/reference/mutate.html) can be used to create new columns and overwrite an existing ones. One `mutate()` statement can be used to create multiple columns, which can make the code easier to read. Just like the other dplyr commands, `sparklyr::mutate()` works in an identical way to `dplyr::mutate()`. Once again, just reference the columns by name, rather than using quotes.\n",
    "\n",
    "The statement to create each column should begin `column_name = `; note that a single `=` is being used for assignment rather than `<-`. Then just set this new column equal to a statement, which will often be derived from other columns. For instance, we do not have a column for how long an incident took in the data, but do have the columns available to derive this:\n",
    "- `job_hours` gives the total number of hours for engines attending the incident, e.g. if 2 engines attended for an hour `job_hours` will be `2`\n",
    "- `engine_count` gives the number of engines in attendance\n",
    "\n",
    "So to get the duration of the incident, which we will call `incident_duration`, we have to divide `job_hours` by `engine_count`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba01f8d",
   "metadata": {},
   "source": [
    "```r\n",
    "rescue <- rescue %>%\n",
    "    sparklyr::mutate(\n",
    "        incident_duration = job_hours / engine_count)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8be5fb",
   "metadata": {},
   "source": [
    "Now preview the data with `head(5) %>% collect()` after selecting a few relevant columns, then print out the DataFrame:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0c9958",
   "metadata": {},
   "source": [
    "```r\n",
    "rescue %>%\n",
    "    sparklyr::select(\n",
    "        incident_number,\n",
    "        animal_group,\n",
    "        incident_duration,\n",
    "        job_hours,\n",
    "        engine_count) %>%\n",
    "    head(5) %>%\n",
    "    sparklyr::collect() %>%\n",
    "    print()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce93abfd",
   "metadata": {},
   "source": [
    "Although the source data here is small, note that previewing the data will still take longer to process than defining the new column. Why? Remember that Spark is built on the concept of **transformations** and **actions**:\n",
    "* **Transformations** are lazily evaluated expressions. These form the set of instructions called the execution plan.  \n",
    "* **Actions** trigger computation to be performed on the cluster and results returned to the driver. It is actions that trigger the execution plan.\n",
    "\n",
    "Multiple transformations can be combined, as we did to preprocess the `rescue` DataFrame above. Only when an action is called, for example `collect()` or implicitly printing the DF are these transformations and action executed on the cluster, after which the results are returned to the driver.\n",
    "\n",
    "### Using Spark SQL Functions with `mutate()`\n",
    "\n",
    "So far, writing sparklyr code has been very similar to using dplyr, the main difference being the consideration of where the DataFrame is being processed: either on the driver in the case of dplyr, or in the Spark cluster with sparklyr.\n",
    "\n",
    "One key difference is that when using `mutate()` you can also make use of Spark functions directly. See the article on [Using Spark functions in sparklyr](../sparklyr-intro/sparklyr-functions) for a full explanation.\n",
    "\n",
    "One example is casting columns from one data type to another, e.g. from string to date. In sparklyr we can use [`to_date()`](https://spark.apache.org/docs/latest/api/sql/index.html#to_date). Note that this is **not** referenced with `::`. We then verify the column type has changed with `glimpse()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498b7a15",
   "metadata": {},
   "source": [
    "```r\n",
    "rescue <- rescue %>%\n",
    "    sparklyr::mutate(date_time_of_call = to_date(date_time_of_call, \"dd/MM/yyyy\"))\n",
    "\n",
    "rescue %>%\n",
    "    sparklyr::select(\n",
    "        incident_number,\n",
    "        date_time_of_call) %>%\n",
    "    pillar::glimpse()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759c64a2",
   "metadata": {},
   "source": [
    "You will need to consult the [documentation](https://spark.apache.org/docs/latest/api/sql/index.html) for Spark SQL functions as they are not exposed to R directly; this can be demonstrated by a lack of available help for them in R. You can also only use them on sparklyr DFs, not base R DFs or tibbles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd17b16",
   "metadata": {},
   "source": [
    "```r\n",
    "?to_date\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759cda87",
   "metadata": {},
   "source": [
    "(sort-data)=\n",
    "### Sorting: `arrange()` and `sdf_sort()`\n",
    "\n",
    "An important Spark concept is that DataFrames are [not ordered by default](../spark-concepts/df-order), unlike a base R DF or tibble, which have an index. Remember that a Spark DataFrame is distributed into partitions, and there is no guarantee of the order of the rows within these partitions, or which partition a particular row is on.\n",
    "\n",
    "There are two ways to sort data in sparklyr: with [`dplyr::arrange()`](https://dplyr.tidyverse.org/reference/arrange.html) and [`sparklyr::sdf_sort()`](https://spark.rstudio.com/packages/sparklyr/latest/reference/sdf_sort.html). Neither are perfect implementations. `arrange()` will only actually be processed if the data is being subset or directly returned, as otherwise the sorting operation gets deemed superfluous by Spark. By default columns are sorted ascending; use [`desc()`](https://dplyr.tidyverse.org/reference/desc.html) to sort descending.\n",
    "\n",
    "`sdf_sort()` is better, but columns can only be sorted ascending and the syntax involves adding the column names as strings in a vector like in base R which makes it less readable. You can sort columns that are entirely numeric descending by creating a new column of negative values, sorting by this with `sdf_sort()`, then dropping it, but this will not work correctly with strings or `NA` values. You could also use an SQL expression for sorting. The most important principle here is consistency; try and use the same syntax as your colleagues to make the code easier to read.\n",
    "\n",
    "Note that sorting the DataFrame is an expensive operation, as the rows move between partitions. This is a key Spark concept called a [*shuffle*](../spark-concepts/shuffling). When you are ready to optimise your Spark code you will want to read the article on Shuffling.\n",
    "\n",
    "To show the ten highest cost incidents, use `arrange(desc(total_cost))`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbf5e13",
   "metadata": {},
   "source": [
    "```r\n",
    "rescue %>%\n",
    "    sparklyr::select(\n",
    "        incident_number,\n",
    "        animal_group,\n",
    "        total_cost) %>%\n",
    "    dplyr::arrange(desc(total_cost)) %>%\n",
    "    head(10)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce22bc28",
   "metadata": {},
   "source": [
    "Horses make up a lot of the more expensive calls, which makes sense, given that they are large animals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ef261f",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "Sort the incidents in terms of their duration, look at the top 10 and the bottom 10. Do you notice anything strange?\n",
    "\n",
    "<details>\n",
    "<summary><b>Exercise 3: Solution</b></summary>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8dd428",
   "metadata": {},
   "source": [
    "```r\n",
    "# To get the top 10, sort the DF descending\n",
    "top10 <- rescue %>%\n",
    "    sparklyr::select(\n",
    "        incident_number,\n",
    "        animal_group,\n",
    "        total_cost,\n",
    "        incident_duration) %>%\n",
    "    dplyr::arrange(desc(incident_duration)) %>%\n",
    "    head(10)\n",
    "\n",
    "top10\n",
    "\n",
    "# The bottom 10 can just be sorted ascending, so in this example we have used sdf_sort\n",
    "# Note that .tail() does not exist in Spark 2.4\n",
    "bottom10 <- rescue %>%\n",
    "    sparklyr::select(\n",
    "        incident_number,\n",
    "        animal_group,\n",
    "        total_cost,\n",
    "        incident_duration) %>%\n",
    "    sparklyr::sdf_sort(c(\"incident_duration\")) %>%\n",
    "    head(10)\n",
    "\n",
    "# When previewing the results, the incident_duration are all null\n",
    "bottom10\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be688a5a",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "### Grouping and Aggregating: `group_by()` and `summarise()`\n",
    "\n",
    "In most cases, we want to get insights into the raw data, for instance, by taking the sum or average of a column, or getting the largest or smallest values. This is key to what the Office for National Statistics does: we release statistics! \n",
    "\n",
    "In sparklyr, [`group_by()`](https://dplyr.tidyverse.org/reference/group_by.html) and [`summarise()`](https://dplyr.tidyverse.org/reference/summarise.html) can be referenced directly from dplyr and these functions work in an identical way. There is one key difference: inside `summmarise()` functions will be interpreted as Spark SQL functions where possible, e.g. [`sum()`](https://spark.apache.org/docs/latest/api/sql/index.html#sum), [`max()`](https://spark.apache.org/docs/latest/api/sql/index.html#max) etc.\n",
    "\n",
    "For instance, to find the average cost by `animal_group` we use [`mean()`](https://spark.apache.org/docs/latest/api/sql/index.html#mean). Remember to assign the result of the aggregation to a column name:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450b4dd4",
   "metadata": {},
   "source": [
    "```r\n",
    "cost_by_animal <- rescue %>%\n",
    "    dplyr::group_by(animal_group) %>%\n",
    "    dplyr::summarise(average_cost = mean(total_cost))\n",
    "\n",
    "cost_by_animal %>%\n",
    "    head(5)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f9900e",
   "metadata": {},
   "source": [
    "Remember that sparklyr DFs are not ordered by unless we specifically do so; here we will sort the `average_cost` descending:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ec4269",
   "metadata": {},
   "source": [
    "```r\n",
    "cost_by_animal %>%\n",
    "    dplyr::arrange(desc(average_cost)) %>%\n",
    "       head(10)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0e39ad",
   "metadata": {},
   "source": [
    "It looks like `Goat` could be an outlier as it is significantly higher than the other higher average cost incidents. We can investigate this in more detail using `filter()`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b3a958",
   "metadata": {},
   "source": [
    "```r\n",
    "goats <- rescue %>%\n",
    "    sparklyr::filter(animal_group == \"Goat\")\n",
    "    \n",
    "goats %>%\n",
    "    sparklyr::sdf_nrow()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a6404c",
   "metadata": {},
   "source": [
    "Just one expensive goat incident! Lets see the description:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59986479",
   "metadata": {},
   "source": [
    "```r\n",
    "goat <- goats %>%\n",
    "    sparklyr::select(incident_number, animal_group, description) %>%\n",
    "    sparklyr::collect()\n",
    "\n",
    "goat\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5753936",
   "metadata": {},
   "source": [
    "Note that although we used `collect()` we did not subset the data with `head()`. This is because we know the row count is tiny, and so there was no danger of overloading the driver with too much data.\n",
    "\n",
    "### Reading data from a Parquet file: `spark_read_parquet()`\n",
    "\n",
    "The next section covers how to join data in Spark, but before we do, we need to read in another dataset. In our rescue data, we have a column for the postcode district, which represents the first part of the postcode. We have data for the population by postcode in another dataset, `population`.\n",
    "\n",
    "This data are stored as a parquet file. Parquet files the most efficient way to store data when using Spark. They are compressed and so take up much less storage space, and reading parquet files with Spark is many times quicker than reading CSVs. The drawback is that they are not human readable, although you can store them as a Hive table which means they can easily be interrogated with SQL. See the article on Parquet files for more information.\n",
    "\n",
    "The syntax for reading in a parquet file is similar to a CSV: [`spark_read_parquet()`](https://spark.rstudio.com/packages/sparklyr/latest/reference/spark_read_parquet.html). Specify `sc` first, then the `path`. There is no need for the `header` or `infer_schema` argument as unlike CSVs parquet files already have the schema defined. We can then preview the data with `glimpse()`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3bd696",
   "metadata": {},
   "source": [
    "```r\n",
    "population_path <- config$population_path\n",
    "population <- sparklyr::spark_read_parquet(sc, population_path)\n",
    "\n",
    "pillar::glimpse(population)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defef20c",
   "metadata": {},
   "source": [
    "### Joining Data `left_join()`\n",
    "\n",
    "Now we have read the population data in, we can join it to the rescue data to get the population by postcode. Left joining in sparklyr uses the dplyr syntax of [`left_join()`](https://spark.rstudio.com/packages/sparklyr/latest/reference/join.tbl_spark.html). Other joins use similar syntax, e.g. [`right_join()`](https://spark.rstudio.com/packages/sparklyr/latest/reference/join.tbl_spark.html), [`inner_join()`](https://spark.rstudio.com/packages/sparklyr/latest/reference/join.tbl_spark.html).\n",
    "\n",
    "This article assumes that you are familiar with joins. Those who know SQL will be familiar with this term, although pandas and R users sometimes use the term *merge*. If you do not know how a join works, please read about [joins in SQL](https://en.wikipedia.org/wiki/Join_(SQL)) first; the principles are the same in Spark. Joins are expensive in Spark as they involve shuffling the data and this can make larger joins slow. See the article on [Optimising Joins](../spark-concepts/join-concepts) for more information on how to make them more efficient.\n",
    "\n",
    "Assuming that you are using the pipe notation for the DataFrame on the left hand side of the join, the other \n",
    "\n",
    "`left_join()` has three arguments that you will use:\n",
    "- `x`: the left hand side of the join; this will most frequently be piped in\n",
    "- `y`: the right hand side of the join; if using pipes this is the first argument\n",
    "- `by`: which specifies the mapping. Here we have a common column name and so can simply supply the column name. Note that we use a string for the column name. If joining on multiple columns, use a vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431f92fe",
   "metadata": {},
   "source": [
    "```r\n",
    "rescue_with_pop <- rescue %>%\n",
    "    sparklyr::left_join(y=population, by=\"postcode_district\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9203310b",
   "metadata": {},
   "source": [
    "Once again, note how quick this code runs. This is because although a join is an expensive operation, we have only created the plan at this point. We need an action to run the plan and return a result; sort the joined DataFrame, subset the columns and then use `head(5)` to implicitly print:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ff404b",
   "metadata": {},
   "source": [
    "```r\n",
    "rescue_with_pop <- rescue_with_pop %>%\n",
    "    sparklyr::sdf_sort(c(\"incident_number\")) %>%\n",
    "    sparklyr::select(incident_number, animal_group, postcode_district, population)\n",
    "\n",
    "rescue_with_pop %>%\n",
    "    head(5)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8215a3d7",
   "metadata": {},
   "source": [
    "### Writing data: file choice\n",
    "\n",
    "In this article so far, we have been calling actions to preview the data, bringing back only a handful of rows each time. This is useful when developing and debugging code, but in production pipelines you will want to write the results.\n",
    "\n",
    "The format in which the results are written out depends on what you want to do next with the data:\n",
    "- If the data are intended to be human readable, e.g. as the basis for a presentation, or as a publication on the ONS website, then you will likely want to output the data as a CSV\n",
    "- If the data are intended to be used as an input to another Spark process, then use parquet or a Hive table.\n",
    "\n",
    "There are other use cases, e.g. JSON can be useful if you want the results to analysed with a different programming language, although here we only focus on CSV and parquet. See the article on [Writing Data](../spark-functions/writing-data) for more information.\n",
    "\n",
    "### Write to a parquet: `spark_write_parquet()`\n",
    "\n",
    "To write out our DataFrame as a parquet file, use [`spark_write_parquet()`](https://spark.rstudio.com/packages/sparklyr/latest/reference/spark_write_parquet.html). This has two compulsory arguments: `x`, the DataFrame to be written, and `path`, the path to the file. If using pipes for the DataFrame then you will only need the file path.\n",
    "\n",
    "The key difference between writing out data with Spark and writing out data with R is that the data will be distributed, which means that multiple files will be created, stored in a parent folder. Spark can read in these parent folders as one DataFrame. There will be one file written out per partition of the DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f60b0ca",
   "metadata": {},
   "source": [
    "```r\n",
    "output_path_parquet <- config$rescue_with_pop_path_parquet\n",
    "\n",
    "rescue_with_pop %>%\n",
    "    sparklyr::spark_write_parquet(output_path_parquet)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3dfef4",
   "metadata": {},
   "source": [
    "It is worth looking at the raw data that is written out to see that it has been stored in several files in a parent folder.\n",
    "\n",
    "When reading the data in, Spark will treat every individual file as a partition. See the article on [Managing Partitions](../spark-concepts/partitions) for more information.\n",
    "\n",
    "### Write to a CSV: `spark_write_csv()` and `sdf_coalesce()`\n",
    "\n",
    "CSVs will also be written out in a distributed manner as multiple files. While this is desirable in a parquet, it is not very useful with CSV, as the main benefit is to make them human readable. First, write out the data with [`spark_write_csv()`](https://spark.rstudio.com/packages/sparklyr/latest/reference/spark_write_csv.html), using the path defined in the config:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72619f9e",
   "metadata": {},
   "source": [
    "```r\n",
    "output_path_csv <- config$rescue_with_pop_path_csv\n",
    "\n",
    "rescue_with_pop %>%\n",
    "    sparklyr::spark_write_csv(output_path_csv, header=TRUE)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c59f688",
   "metadata": {},
   "source": [
    "Again, look at the raw data in a file browser. You can see that it has written out a folder called `rescue_with_pop.csv`, with multiple files inside. Each of these on their own is a legitimate CSV file, with the correct headers.\n",
    "\n",
    "To reduce the number of partitions, pipe the data into [`sdf_coalesce(partitions)`](https://spark.rstudio.com/packages/sparklyr/latest/reference/sdf_coalesce.html); this will combine existing partitions. Setting `partitions` to `1` will put all of the data on the same partition. \n",
    "\n",
    "As the file will already exist, we need to tell Spark to overwrite the existing file. Use `mode=\"overwrite\"` to do this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141459cf",
   "metadata": {},
   "source": [
    "```r\n",
    "rescue_with_pop %>%\n",
    "    sparklyr::sdf_coalesce(1) %>%\n",
    "    sparklyr::spark_write_csv(output_path_csv, header=TRUE, mode=\"overwrite\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e3d621",
   "metadata": {},
   "source": [
    "Checking the file again, you can see that although the folder still exists, it will contain only one CSV file.\n",
    "\n",
    "### Removing files\n",
    "\n",
    "Spark has no native way of removing files, so either use the standard R methods, or delete them manually through a file browser. If on a local file system, use [`unlink()`](https://stat.ethz.ch/R-manual/R-patched/library/base/html/unlink.html) to delete a file, setting `recursive=TRUE` to remove a directory. If using HDFS or similar, then use [`system()`](https://stat.ethz.ch/R-manual/R-devel/library/base/html/system.html). Be careful when using the `system()` command as you will not get a warning before deleting files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be39e573",
   "metadata": {},
   "source": [
    "```r\n",
    "delete_file <- function(file_path){\n",
    "    cmd <- paste0(\"hdfs dfs -rm -r -skipTrash \",\n",
    "                  file_path)\n",
    "    \n",
    "    system(cmd,\n",
    "           ignore.stdout=TRUE,\n",
    "           ignore.stderr=TRUE)\n",
    "}\n",
    "\n",
    "delete_file(output_path_parquet)\n",
    "delete_file(output_path_csv)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45dc8cf3",
   "metadata": {},
   "source": [
    "### Further Resources\n",
    "\n",
    "Spark at the ONS Code and Articles:\n",
    "- [R Code used in this article](https://github.com/best-practice-and-impact/ons-spark/blob/main/ons-spark/raw-notebooks/sparklyr-intro/r_input.R)\n",
    "- [Introduction to PySpark](../pyspark-intro/pyspark-intro)\n",
    "- [Avoiding Module Import Conflicts](../ancillary-topics/module-imports)\n",
    "- [Guidance on Spark Sessions](../spark-overview/spark-session-guidance)\n",
    "- [Example Spark Sessions](../spark-overview/example-spark-sessions)\n",
    "- [Reading Data in sparklyr](../sparklyr-intro/reading-data-sparklyr)\n",
    "- [Data Storage](../spark-overview/data-storage)\n",
    "- [Data Types in Spark](../spark-overview/data-types)\n",
    "- [Spark and Visualisation](../ancillary-topics/visualisation)\n",
    "- [Spark DataFrames Are Not Ordered](../spark-concepts/df-order)\n",
    "- [Using Spark functions in sparklyr](../sparklyr-intro/sparklyr-functions)\n",
    "- [Shuffling](../spark-concepts/shuffling)\n",
    "- [Optimising Joins](../spark-concepts/join-concepts)\n",
    "- [Writing Data](../spark-functions/writing-data)\n",
    "- [Managing Partitions](../spark-concepts/partitions)\n",
    "\n",
    "[sparklyr](https://spark.rstudio.com/packages/sparklyr/latest/reference/) and [tidyverse](https://www.tidyverse.org/) Documentation:\n",
    "- [`select()`](https://dplyr.tidyverse.org/reference/select.html)\n",
    "- [`mutate()`](https://dplyr.tidyverse.org/reference/mutate.html)\n",
    "- [`filter()`](https://dplyr.tidyverse.org/reference/filter.html)\n",
    "- [`summarise()`](https://dplyr.tidyverse.org/reference/summarise.html)\n",
    "- [Spark connections](https://spark.rstudio.com/packages/sparklyr/latest/reference/spark-connections.html): covers `spark_connect()` and `spark_connection_is_open()`\n",
    "- [`spark_config()`](https://spark.rstudio.com/packages/sparklyr/latest/reference/spark_config.html)\n",
    "- [`glimpse()`](https://pillar.r-lib.org/reference/glimpse.html)\n",
    "- [`collect()`](https://dplyr.tidyverse.org/reference/compute.html)\n",
    "- [`sdf_nrow()`](https://spark.rstudio.com/packages/sparklyr/latest/reference/sdf_dim.html)\n",
    "- [`rename()`](https://dplyr.tidyverse.org/reference/rename.html)\n",
    "- [`arrange()`](https://dplyr.tidyverse.org/reference/arrange.html)\n",
    "- [`sdf_sort()`](https://spark.rstudio.com/packages/sparklyr/latest/reference/sdf_sort.html)\n",
    "- [`desc()`](https://dplyr.tidyverse.org/reference/desc.html)\n",
    "- [`group_by()`](https://dplyr.tidyverse.org/reference/group_by.html)\n",
    "- [Joins](https://spark.rstudio.com/packages/sparklyr/latest/reference/join.tbl_spark.html): covers all joins, including `left_join()`, `right_join()` and `inner_join()`\n",
    "- [`spark_write_parquet()`](https://spark.rstudio.com/packages/sparklyr/latest/reference/spark_write_parquet.html)\n",
    "- [`spark_write_csv()`](https://spark.rstudio.com/packages/sparklyr/latest/reference/spark_write_csv.html)\n",
    "- [`sdf_coalesce()`](https://spark.rstudio.com/packages/sparklyr/latest/reference/sdf_coalesce.html)\n",
    "\n",
    "Base R Documentation:\n",
    "- [`class()`](https://stat.ethz.ch/R-manual/R-devel/library/base/html/class.html) \n",
    "- [`unlink()`](https://stat.ethz.ch/R-manual/R-patched/library/base/html/unlink.html)\n",
    "- [`system()`](https://stat.ethz.ch/R-manual/R-devel/library/base/html/system.html)\n",
    "\n",
    "Spark SQL Functions Documentation:\n",
    "- [`to_date`](https://spark.apache.org/docs/latest/api/sql/index.html#to_date)\n",
    "- [`sum`](https://spark.apache.org/docs/latest/api/sql/index.html#sum)\n",
    "- [`max`](https://spark.apache.org/docs/latest/api/sql/index.html#max)\n",
    "- [`mean`](https://spark.apache.org/docs/latest/api/sql/index.html#mean)\n",
    "\n",
    "Other Links:\n",
    "- [Introduction to dplyr](https://dplyr.tidyverse.org/articles/dplyr.html)\n",
    "- [R for Data Science](https://r4ds.had.co.nz/)\n",
    "- [Databricks: Learning Spark](https://pages.databricks.com/rs/094-YMS-629/images/LearningSpark2.0.pdf)\n",
    "- [SparkR](https://spark.apache.org/docs/latest/sparkr.html): not used at the ONS\n",
    "\n",
    "#### Acknowledgements\n",
    "\n",
    "Thanks to Karina Marks, Chris Musselle and Beth Ashlee for creating the initial version of this article."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
