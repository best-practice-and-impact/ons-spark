{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Cramér's V from a Spark DataFrame\n",
    "\n",
    "### What is Cramér's V?\n",
    "Cramér's V is a statistical measure of an association between two nominal variables, giving a value between 0 and 1 inclusive. Here 0 would indicate no association and 1 indicates a strong association between the two variables. It is based on Pearson's chi-square statistics.\n",
    "\n",
    "We calculate Cramér's V as follows:\n",
    "\n",
    "$$ \\text{Cramer's V} = \\sqrt{\\dfrac{\\dfrac{\\chi^2}{n}}{\\min (c-1,r-1)}}, $$ \n",
    "where:\n",
    "- $\\chi^2$ is the Chi-squared statistic,\n",
    "- $n$ is the number of samples,\n",
    "- $r$ is the number of rows,\n",
    "- $c$ is the number of columns.\n",
    "\n",
    "In some literature you may see the Phi coefficient used ($\\phi$), where $\\phi^2 = \\chi^2/n$.\n",
    "\n",
    "### Cramér's V in Spark:\n",
    "Although there is not an in built method for calculating this statistic in base python, is it reasonably straightforward using `numpy` and `scipy.stats` packages. An example of this can be found [online here](https://www.statology.org/cramers-v-in-python/).\n",
    "A similar example for R is linked [here](https://www.statology.org/cramers-v-in-r/).\n",
    "\n",
    "To calculate the Cramér V statistic, we will need to first calculate the $\\chi^2$ statistic. In python we will utilise `scipy.stats.chi2_contingency` and `chisq.test` in R. Both these functions will take a matrix like input of a contingency table / pair-wise frequency table. Both Pyspark and SparklyR have inbuilt functions which can produce these tables (`crosstab`/`sdf_crosstab`) as we will see shortly.\n",
    "\n",
    "Due to Pyspark and SparklyR's differences to classical python and R, we need to consider how we can calculate Cramér's V when using Spark DataFrames.\n",
    "First we will import the needed packages, start a spark session and load the rescue data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "import scipy.stats as stats\n",
    "\n",
    "\n",
    "spark = (SparkSession.builder.master(\"local[2]\")\n",
    "         .appName(\"cramer-v\")\n",
    "         .getOrCreate())\n",
    "\n",
    "\n",
    "with open(\"../../../config.yaml\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "    \n",
    "\n",
    "rescue_path = config[\"rescue_clean_path\"]\n",
    "rescue = spark.read.parquet(rescue_path)\n",
    "\n",
    "rescue = (rescue.withColumnRenamed('animal_group','animal_type')\n",
    "                .withColumnRenamed('postcodedistrict','postcode_district')\n",
    "          )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "library(sparklyr)\n",
    "library(dplyr)\n",
    "\n",
    "default_config <- sparklyr::spark_config()\n",
    "\n",
    "sc <- sparklyr::spark_connect(\n",
    "    master = \"local[2]\",\n",
    "    app_name = \"cramer-v\",\n",
    "    config = default_config)\n",
    "\n",
    "config <- yaml::yaml.load_file(\"ons-spark/config.yaml\")\n",
    "rescue <- sparklyr::spark_read_parquet(sc, config$rescue_clean_path, header=TRUE, infer_schema=TRUE)\n",
    "rescue <- rescue %>% dplyr::rename(\n",
    "                    animal_type = animal_group,\n",
    "                    postcode_district = postcodedistrict)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As Cramér V is a measure of how two variables are associated, it makes sense for us to select two variables which we either believe will or will not have some level of association. For our first example we will select the `cal_year` and `animal_type` columns. following this we will compare `postcode_district` and `animal_type`.\n",
    "\n",
    "#### Cramér's V Example 1: `cal_year` and `animal_type`\n",
    "\n",
    "Using either `.crosstab()` or `sdf_crosstab()`, we can calculate a pair-wise frequency table of the `id` and `value` columns (a.k.a. contingency table). We will generate this table and convert it to a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cal_year_animal_type</th>\n",
       "      <th>Bird</th>\n",
       "      <th>Budgie</th>\n",
       "      <th>Bull</th>\n",
       "      <th>Cat</th>\n",
       "      <th>Cow</th>\n",
       "      <th>Deer</th>\n",
       "      <th>Dog</th>\n",
       "      <th>Ferret</th>\n",
       "      <th>Fish</th>\n",
       "      <th>...</th>\n",
       "      <th>Pigeon</th>\n",
       "      <th>Rabbit</th>\n",
       "      <th>Sheep</th>\n",
       "      <th>Snake</th>\n",
       "      <th>Squirrel</th>\n",
       "      <th>Tortoise</th>\n",
       "      <th>Unknown - Animal Rescue From Water - Farm Animal</th>\n",
       "      <th>Unknown - Domestic Animal Or Pet</th>\n",
       "      <th>Unknown - Heavy Livestock Animal</th>\n",
       "      <th>Unknown - Wild Animal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014</td>\n",
       "      <td>110</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>298</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013</td>\n",
       "      <td>85</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>313</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>93</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018</td>\n",
       "      <td>126</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>305</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>91</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015</td>\n",
       "      <td>106</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>263</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>88</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011</td>\n",
       "      <td>120</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>309</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  cal_year_animal_type  Bird  Budgie  Bull  Cat  Cow  Deer  Dog  Ferret  Fish  \\\n",
       "0                 2014   110       0     0  298    1     5   90       0     0   \n",
       "1                 2013    85       0     0  313    0     7   93       0     0   \n",
       "2                 2018   126       1     0  305    0    16   91       1     0   \n",
       "3                 2015   106       0     0  263    0     6   88       1     0   \n",
       "4                 2011   120       1     0  309    0     9  103       2     0   \n",
       "\n",
       "           ...            Pigeon  Rabbit  Sheep  Snake  Squirrel  Tortoise  \\\n",
       "0          ...                 1       3      0      0         2         0   \n",
       "1          ...                 1       0      0      2         7         0   \n",
       "2          ...                 1       2      0      1         5         0   \n",
       "3          ...                 0       0      1      0         5         1   \n",
       "4          ...                 0       0      0      0         4         0   \n",
       "\n",
       "   Unknown - Animal Rescue From Water - Farm Animal  \\\n",
       "0                                                 1   \n",
       "1                                                 0   \n",
       "2                                                 0   \n",
       "3                                                 0   \n",
       "4                                                 0   \n",
       "\n",
       "   Unknown - Domestic Animal Or Pet  Unknown - Heavy Livestock Animal  \\\n",
       "0                                29                                 0   \n",
       "1                                19                                 2   \n",
       "2                                 7                                 2   \n",
       "3                                29                                 0   \n",
       "4                                13                                 8   \n",
       "\n",
       "   Unknown - Wild Animal  \n",
       "0                      7  \n",
       "1                     12  \n",
       "2                      5  \n",
       "3                      7  \n",
       "4                      0  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_spark = rescue.crosstab('cal_year','animal_type')\n",
    "freq_pandas = freq_spark.toPandas()\n",
    "freq_pandas.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "freq_spark <- sparklyr::sdf_crosstab(rescue, 'cal_year', 'animal_type') \n",
    "glimpse(freq_spark)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have converted out data into a contingency table, we need to be careful about converting this into a matrix or array type variable. If we do this without considering the `CalYear_animal_type` column, we will end up with some string within our matrix. We would spot this issue when moving forward to use `.chi2_contingency()`, as we would get a `TypeError` raised. We need to find some way of dealing with our column, such that it is not converted when changing our pandas DataFrame into a numpy array. We do this by setting the `cal_year_animal_type` column as the index of our pandas dataframe, as neither the index or column headers are converted into a numpy array, just the internal values are extracted.\n",
    "\n",
    "A type issue is also raised in R, however we will deal with this slightly differently. Instead of changing the index of our DataFrame, we will simply drop the `cal_year_animal_type` column. This allows us to pass the frequency table directly into the `chisq.test()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_pandas_reindexed = freq_pandas.set_index('cal_year_animal_type')\n",
    "freq_numpy = np.array(freq_pandas_reindexed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "freq_r <- freq_spark %>% collect() \n",
    "freq_r <- subset(freq_r, select = -cal_year_animal_type)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are wanting to calculate Cramér's V for two examples, we will define a function to perform the $\\chi^2$ test and calculate the statistic. We only need the $\\chi^2$ statistic, as such we extract this from the `chi2_contingency`/`chisq.test` functions using `[0]` in python and `$statistic` in R. `chisq.test` has additional text included in the output, to remove this we cast this output as a double, keeping only the numerical value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cramer_v(freq_numpy):\n",
    "    \"\"\"\n",
    "    Returns the Cramer's V Statistic for the given pair-wise frequency table\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    freq_numpy : np.array\n",
    "        pair-wise frequency table / contingency table as a numpy array. \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cramer_v_statistic :  float\n",
    "        the Cramer's V statistic for the given contingency table\n",
    "    \"\"\"\n",
    "    # Chi-squared test statistic, sample size, and minimum of rows and columns\n",
    "    chi_sqrd = stats.chi2_contingency(freq_numpy, correction=False)[0]\n",
    "    n = np.sum(freq_numpy)\n",
    "    min_dim = min(freq_numpy.shape)-1\n",
    "\n",
    "    #calculate Cramer's V \n",
    "    cramer_v_statistic = np.sqrt((chi_sqrd/n) / min_dim)\n",
    "    return cramer_v_statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "get_cramer_v <- function(freq_r){\n",
    "  # Get cramer V statistic\n",
    "  # Returns the Cramer's V Statistic for the given\n",
    "  # pair-wise frequency table\n",
    "  # \n",
    "  # Param: \n",
    "  # freq_r: dataframe. pair-wise frequency / contingency table not containing strings\n",
    "  #\n",
    "  # Return:\n",
    "  # cramer_v_statistic: numeric. The Cramer's V statistic \n",
    "  # \n",
    "\n",
    "  # Chi-squared test statistic, sample size, and minimum of rows and columns\n",
    "  chi_sqrd <- chisq.test(freq_r, correct = FALSE)$statistic\n",
    "  chi_sqrd <- as.double(chi_sqrd)\n",
    "  n <- sum(freq_r)\n",
    "  min_dim <- min(dim(freq_r)) - 1\n",
    "\n",
    "  # calculate Cramer's V \n",
    "  cramer_v_statistic <- sqrt((chi_sqrd/n) / min_dim)\n",
    "  return(cramer_v_statistic)\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the preprocessing and consideration of column names, we can now apply the `get_cramer_v()` function to our arrays "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.086908868435997877"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_cramer_v(freq_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "get_cramer_v(freq_r)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now get an Cramér V of 0.0869, telling us that there is little to no association between `cal_year` and `animal_type`.\n",
    "This example also validates both Python and R methods against each other, as we get identical outputs.\n",
    "\n",
    "#### Cramér's V Example 2: `postcode_district` and `animal_type`\n",
    "\n",
    "Using the earlier defined function, just need to repeat the cross tabbing exercise before applying our function. This time we will select `postcode_district` instead of `cal_year`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29083878195595769"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_spark = rescue.crosstab('postcode_district','animal_type')\n",
    "freq_pandas = freq_spark.toPandas()\n",
    "freq_pandas_reindexed = freq_pandas.set_index('postcode_district_animal_type')\n",
    "freq_numpy = np.array(freq_pandas_reindexed)\n",
    "get_cramer_v(freq_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "freq_spark <- sparklyr::sdf_crosstab(rescue, 'postcode_district', 'animal_type') \n",
    "freq_r <- freq_spark %>% collect() \n",
    "freq_r <- subset(freq_r, select = -postcode_district_animal_type)\n",
    "get_cramer_v(freq_r)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we get a cramér V value of 0.29, suggesting a slight association between `postcode_district` and `animal_type`.\n",
    "\n",
    "### Potential Issue with `.crosstab` or `sdf_crosstab()`\n",
    "During the testing for this page, we noted a few common issues which may arise when attempting to calculate Cramér's V statistic. Specifically this issue is linked to `.crosstab` and `sdf_crosstab()` functions for data which has not been fully pre-processed.\n",
    "\n",
    "For this example we take the unprocessed rescue dataset. Here in `animal_type`, we have values for cat with both a upper and lower case first letter. Here we will use the `tryCatch()` function to handle the error message, this is similar to pythons `try: except:` statements (For more info see [`Error handing in R`](https://cran.r-project.org/web/packages/tryCatchLog/vignettes/tryCatchLog-intro.html) or [`Errors and Exceptions (Python)`](https://docs.python.org/3/tutorial/errors.html)).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/cloudera/parcels/CDH/lib/spark/python/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o479.crosstab.\n",
      ": org.apache.spark.sql.AnalysisException: Reference 'Cat' is ambiguous, could be: Cat, Cat.;\n",
      "\tat org.apache.spark.sql.catalyst.expressions.package$AttributeSeq.resolve(package.scala:259)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveQuoted(LogicalPlan.scala:121)\n",
      "\tat org.apache.spark.sql.Dataset.resolve(Dataset.scala:221)\n",
      "\tat org.apache.spark.sql.Dataset.col(Dataset.scala:1268)\n",
      "\tat org.apache.spark.sql.DataFrameNaFunctions.org$apache$spark$sql$DataFrameNaFunctions$$fillCol(DataFrameNaFunctions.scala:443)\n",
      "\tat org.apache.spark.sql.DataFrameNaFunctions$$anonfun$7.apply(DataFrameNaFunctions.scala:502)\n",
      "\tat org.apache.spark.sql.DataFrameNaFunctions$$anonfun$7.apply(DataFrameNaFunctions.scala:492)\n",
      "\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
      "\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
      "\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n",
      "\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)\n",
      "\tat org.apache.spark.sql.DataFrameNaFunctions.fillValue(DataFrameNaFunctions.scala:492)\n",
      "\tat org.apache.spark.sql.DataFrameNaFunctions.fill(DataFrameNaFunctions.scala:179)\n",
      "\tat org.apache.spark.sql.DataFrameNaFunctions.fill(DataFrameNaFunctions.scala:163)\n",
      "\tat org.apache.spark.sql.DataFrameNaFunctions.fill(DataFrameNaFunctions.scala:140)\n",
      "\tat org.apache.spark.sql.execution.stat.StatFunctions$.crossTabulate(StatFunctions.scala:225)\n",
      "\tat org.apache.spark.sql.DataFrameStatFunctions.crosstab(DataFrameStatFunctions.scala:215)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-13-6eb2220a558e>\", line 12, in <module>\n",
      "    rescue_raw.crosstab('cal_year','animal_type')\n",
      "  File \"/opt/cloudera/parcels/CDH/lib/spark/python/pyspark/sql/dataframe.py\", line 1945, in crosstab\n",
      "    return DataFrame(self._jdf.stat().crosstab(col1, col2), self.sql_ctx)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/opt/cloudera/parcels/CDH/lib/spark/python/pyspark/sql/utils.py\", line 69, in deco\n",
      "    raise AnalysisException(s.split(': ', 1)[1], stackTrace)\n",
      "pyspark.sql.utils.AnalysisException: \"Reference 'Cat' is ambiguous, could be: Cat, Cat.;\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import traceback\n",
    "\n",
    "rescue_path_csv = config[\"rescue_path_csv\"]\n",
    "rescue_raw = spark.read.csv(rescue_path_csv, header=True, inferSchema=True)\n",
    "\n",
    "rescue_raw = (rescue_raw.withColumnRenamed('AnimalGroupParent','animal_type')\n",
    "                .withColumnRenamed('IncidentNumber','incident_number')\n",
    "                .withColumnRenamed('CalYear','cal_year')\n",
    "          )\n",
    "\n",
    "try:\n",
    "      rescue_raw.crosstab('cal_year','animal_type')\n",
    "except Exception:\n",
    "  print(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "rescue_raw <- sparklyr::spark_read_csv(sc, config$rescue_path_csv, header=TRUE, infer_schema=TRUE)\n",
    "rescue_raw <- rescue_raw %>%\n",
    "    dplyr::rename(\n",
    "        incident_number = IncidentNumber,\n",
    "        animal_type = AnimalGroupParent,\n",
    "        cal_year = CalYear)\n",
    "\n",
    "tryCatch(\n",
    "  {\n",
    "    sparklyr::sdf_crosstab(rescue_raw, 'cal_year', 'animal_type')\n",
    "  },\n",
    "  error = function(e){\n",
    "    message('Error message from Spark')\n",
    "    print(e)\n",
    "  }\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the error message, we can see that `Cat` is ambigious. When we look closer at the distinct values within the `animal_type` column, we will see there is both `cat` and `Cat` present. This is something to be aware of if you wish to use either crosstab function in the future.\n",
    "\n",
    "*Note* the exact error message you recive may depend on the version of Pyspark or sparklyR you are using, but this still relates to values within the `animal_type` column as mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|         animal_type|\n",
      "+--------------------+\n",
      "|                Bird|\n",
      "|              Budgie|\n",
      "|                Bull|\n",
      "|                 Cat|\n",
      "|                 Cow|\n",
      "|                Deer|\n",
      "|                 Dog|\n",
      "|              Ferret|\n",
      "|                Fish|\n",
      "|                 Fox|\n",
      "|                Goat|\n",
      "|             Hamster|\n",
      "|            Hedgehog|\n",
      "|               Horse|\n",
      "|                Lamb|\n",
      "|              Lizard|\n",
      "|              Pigeon|\n",
      "|              Rabbit|\n",
      "|               Sheep|\n",
      "|               Snake|\n",
      "|            Squirrel|\n",
      "|            Tortoise|\n",
      "|Unknown - Animal ...|\n",
      "|Unknown - Domesti...|\n",
      "|Unknown - Heavy L...|\n",
      "|Unknown - Wild An...|\n",
      "|                 cat|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rescue_raw.select('animal_type').distinct().orderBy('animal_type',ascending = True).show(27)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "rescue_raw %>% sparklyr::select(animal_type) %>% distinct() %>% dplyr::arrange(animal_type) %>% print(n=27)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is something to consider when attempting to calculate Cramér's V or a $\\chi^2$ statistic from a spark dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further resources\n",
    "\n",
    "PySpark Documentation:\n",
    "- [`.crosstab()`](https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.DataFrame.crosstab.html)\n",
    "\n",
    "Python Documentation:\n",
    "- [`chi2_contingency()`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chi2_contingency.html)\n",
    "- [`array()`](https://numpy.org/doc/stable/reference/generated/numpy.array.html)\n",
    "- [`Errors and Exceptions`](https://docs.python.org/3/tutorial/errors.html)\n",
    "\n",
    "sparklyr Documentation:\n",
    "- [`sdf_crosstab()`](https://www.rdocumentation.org/packages/sparklyr/versions/1.3.1/topics/sdf_crosstab)\n",
    "\n",
    "R Documention:\n",
    "- [`chisq.test()`](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/chisq.test)\n",
    "- [`Error handing in R`](https://cran.r-project.org/web/packages/tryCatchLog/vignettes/tryCatchLog-intro.html)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
