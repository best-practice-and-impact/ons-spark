{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a96c63f",
   "metadata": {},
   "source": [
    "## Sampling: an overview\n",
    "\n",
    "Sampling is something that you may want to do during development or initial analysis of data, as with a smaller amount of data your code will run faster and requires less memory to process. \n",
    "\n",
    "There are many different ways that a dataframe can be sampled, the two main types covered in this page are:\n",
    "1) **simple random sampling**: [`.sample()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.sample.html) in Pyspark and [`sdf_sample()`](https://spark.rstudio.com/packages/sparklyr/latest/reference/sdf_sample.html) in SparklyR and\n",
    "2) **stratified sampling**: [`.sampleBy()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.sampleBy.html) in Pyspark. There is currently no way to do stratified sampling in SparklyR when using version 2.4.0 (spark vesion > 3.0.0 is required).\n",
    "\n",
    "Although, these two methods are the focus of this page there are numerous methods that can be used for sampling that will not be covered here, such as systematic sampling and cluster sampling. \n",
    "\n",
    "Both `.sample()` and `.sampleBy()` in Pyspark use the same base functions for sampling with and without replacement. For sampling without replacement PySpark implements a uniform sampling method using random number generation. A row will be added to the sample if the randomly generated number is smaller than the fraction input and each row has an equal probability of being sampled. Whereas for sampling with replacement numbers are generated from a Poisson sample. A link to the alogirithm breakdown can be found [here](https://github.com/apache/spark/blob/master/python/pyspark/rddsampler.py).\n",
    "\n",
    "It is important to note that sampling in Spark returns an approximate fraction of the data, rather than an exact one. The reason for this is explained in the [sample](#sampling-sample-and-sdf_sample) section.\n",
    "\n",
    "#### Creating spark session and loading data\n",
    "First, set up the Spark session, read the Animal Rescue data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c82e0509",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[2]\").appName(\"sampling\").getOrCreate()\n",
    "\n",
    "with open(\"../../../config.yaml\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "    \n",
    "rescue_path = config[\"rescue_path_csv\"]\n",
    "# rescue_path = \"../../data/animal_rescue.csv\"\n",
    "rescue = spark.read.csv(rescue_path, header=True, inferSchema=True)\n",
    "rescue = rescue.withColumnRenamed('AnimalGroupParent','animal_type')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ca7e13",
   "metadata": {},
   "source": [
    "```r\n",
    "library(sparklyr)\n",
    "\n",
    "default_config <- sparklyr::spark_config()\n",
    "\n",
    "sc <- sparklyr::spark_connect(\n",
    "    master = \"local[2]\",\n",
    "    app_name = \"sampling\",\n",
    "    config = default_config)\n",
    "\n",
    "config <- yaml::yaml.load_file(\"ons-spark/config.yaml\")\n",
    "\n",
    "rescue <- sparklyr::spark_read_csv(sc, config$rescue_path_csv, header=TRUE, infer_schema=TRUE)\n",
    "rescue <- rescue %>% sparklyr::mutate(animal_type = AnimalGroupParent)\n",
    "rescue <- select(rescue,-AnimalGroupParent)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6002c3fc",
   "metadata": {},
   "source": [
    "To fully test how Spark sampling functions are impacted by partitions we will also make use of a skewed dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "734e8e19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skewed_df = spark.range(1e6).withColumn(\"skew_col\",F.when(F.col('id') < 100, 'A')\n",
    "                                        .when(F.col('id') < 1000, 'B')\n",
    "                                        .when(F.col('id') < 10000, 'C')\n",
    "                                        .when(F.col('id') < 100000, 'D')\n",
    "                                        .otherwise('E')\n",
    "                                        )\n",
    "\n",
    "skewed_df = skewed_df.repartition('skew_col')\n",
    "\n",
    "skewed_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21f3a70",
   "metadata": {},
   "source": [
    "```r\n",
    "skewed_df <- sparklyr::sdf_seq(sc,from = 1, to = 1e6) %>%\n",
    "          sparklyr::mutate(skew_col = case_when(\n",
    "          id <= 100 ~ 'A',\n",
    "          id <= 1000 ~ 'B',\n",
    "          id <= 10000 ~ 'C',\n",
    "          id <= 100000 ~ 'D',\n",
    "          .default = 'E'))\n",
    "\n",
    "skewed_df <- skewed_df %>% sparklyr::sdf_repartition(partition_by = 'skew_col')\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88acaf8",
   "metadata": {},
   "source": [
    "### Sampling: `.sample()` and `sdf_sample()`\n",
    "\n",
    "By using `.sample()` in Pyspark and `sdf_sample()` in SparklyR you take a sampled subset of the original dataframe by setting a `seed`, a `fraction` and whether replacement is required. For the latter argument, in PySpark you use `withReplacement =`, if this is not set then the default answer is `False`, on the other hand in SparklyR you use `replacement =` and if not set the default answer = `True`. For both functions, `seed` is an optional argument which defaults to a random seed. The `fraction` however is compulsory and must be a value betwen 0.0 and 1.0; so, if we want to obtain a 20% sample we would use `fraction = 0.2`. \n",
    "\n",
    "For these two functions it is advised to specify the arguments explicitly. One reason being that in Pyspark `fraction` must come after `withReplacement` whereas when you use `sdf_sample()` in SparklyR `fraction` must be listed first. If you use both languages it is easy to make this mistake.\n",
    "\n",
    "From the [PySpark documentation](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.sample.html), we see that a uniform sampling method is used for `.sample()`. Here each each row is equally likely to be sampled. \n",
    "We should check that this is indeed occuring for an evenly distrubuted dataframe across a number of partitions (`rescue`) and a skewed dataset (`skew_df`). As both `.sample` and `sdf_sample()` implement uniform sampling, an *approximate* sample is returned, so you will get either slightly more or less than the specified fraction you originally input.\n",
    "\n",
    "#### Sampling without replacement\n",
    "\n",
    "We will perform checks on the `.sample()` function to determine how this will impact our skewed distribution. We will also check how the partitions impact the end distribution.\n",
    "\n",
    "First we will compare the distrubution of our skewed dataset before our sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4f809a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in original DF: 5898\n",
      "Total rows in sampled DF: 548\n",
      "Fraction of rows sampled 0.0929128518141743\n"
     ]
    }
   ],
   "source": [
    "rescue_sample = rescue.sample(withReplacement=False, fraction=0.1)\n",
    "print('Total rows in original DF:',rescue.count())\n",
    "print('Total rows in sampled DF:',rescue_sample.count())\n",
    "print('Fraction of rows sampled',rescue_sample.count()/rescue.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c888a3e",
   "metadata": {},
   "source": [
    "```r\n",
    "rescue_sample <- rescue %>% sparklyr::sdf_sample(fraction=0.1, replacement=FALSE)\n",
    "rescue_sample %>% sparklyr::sdf_nrow()\n",
    "\n",
    "print(paste0(\"Total rows in original DF: \", sparklyr::sdf_nrow(rescue)))\n",
    "print(paste0(\"Total rows in sampled DF: \", sparklyr::sdf_nrow(rescue_sample)))\n",
    "print(paste0(\"Fraction of rows sampled: \", sparklyr::sdf_nrow(rescue_sample)/sparklyr::sdf_nrow(rescue)))\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d53926",
   "metadata": {},
   "source": [
    "You can also set a seed, in a similar way to how random numbers generators work. This enables replication, which is useful in Spark given that the DataFrame will otherwise be re-sampled every time an action is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8b9b7be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 1 count: 593\n",
      "Seed 2 count: 593\n"
     ]
    }
   ],
   "source": [
    "rescue_sample_seed_1 = rescue.sample(withReplacement=None,\n",
    "                      fraction=0.1,\n",
    "                      seed=99)\n",
    "\n",
    "rescue_sample_seed_2 = rescue.sample(withReplacement=None,\n",
    "                      fraction=0.1,\n",
    "                      seed=99)\n",
    "\n",
    "print(f\"Seed 1 count: {rescue_sample_seed_1.count()}\")\n",
    "print(f\"Seed 2 count: {rescue_sample_seed_2.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e125cd2a",
   "metadata": {},
   "source": [
    "```r\n",
    "\n",
    "rescue_sample_seed_1 <- rescue %>% sparklyr::sdf_sample(fraction=0.1, seed=99)\n",
    "rescue_sample_seed_2 <- rescue %>% sparklyr::sdf_sample(fraction=0.1, seed=99)\n",
    "\n",
    "print(paste0(\"Seed 1 count: \", rescue_sample_seed_1 %>% sparklyr::sdf_nrow()))\n",
    "print(paste0(\"Seed 2 count: \", rescue_sample_seed_2 %>% sparklyr::sdf_nrow()))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9393c102",
   "metadata": {},
   "source": [
    "We can see that both samples have returned the same number of rows due to the identical seed.\n",
    "\n",
    "Another way of replicating results is with [persisting](https://best-practice-and-impact.github.io/ons-spark/spark-concepts/cache.html#persist). [Caching](https://best-practice-and-impact.github.io/ons-spark/spark-concepts/cache.html) or [checkpointing](https://best-practice-and-impact.github.io/ons-spark/spark-concepts/checkpoint-staging.html#checkpoint) the DataFrame will avoid recalculation of the DF within the same Spark session. Writing out the DF to a Hive table or parquet enables it to be used in subsequent Spark sessions. See the chapter on persisting for more detail.\n",
    "\n",
    "#### Does `.sample()` preserve the distribution, regardless of partitions?\n",
    "\n",
    "We also wish to perform checks on the `.sample()` function to determine how this will be impacted when the original dataframe has a large skew across partitions. \n",
    "Additionally we will verify that the original distribution is preserved when sampling without replacement.\n",
    "First we group the data by `skew_col` and caclulate how much of the dataframe each column represents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81c6c94e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------------------+\n",
      "|skew_col|row_count|percentage_of_dataframe|\n",
      "+--------+---------+-----------------------+\n",
      "|       A|      100|                   0.01|\n",
      "|       B|      900|                   0.09|\n",
      "|       C|     9000|     0.8999999999999999|\n",
      "|       D|    90000|                    9.0|\n",
      "|       E|   900000|                   90.0|\n",
      "+--------+---------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(skewed_df.groupBy('skew_col')\n",
    "    .agg(F.count('skew_col').alias('row_count'))\n",
    "    .withColumn('percentage_of_dataframe',F.col('row_count')/skewed_df.count()*100)\n",
    "    .sort('skew_col')\n",
    "    .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50678132",
   "metadata": {},
   "source": [
    "```r\n",
    "n_rows <- skewed_df %>% sparklyr::sdf_nrow()\n",
    "skewed_df %>%\n",
    "        dplyr::group_by(skew_col) %>%\n",
    "        dplyr::count(skew_col,name = 'row_count') %>%\n",
    "        sparklyr::mutate(percentage_of_dataframe = row_count/n_rows*100) %>%\n",
    "        sdf_sort('skew_col')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4417ce60",
   "metadata": {},
   "source": [
    "As expected group `E` makes up 90% of the dataframe. \n",
    "Now we will sample 10% of the dataframe and assess the distribution of the sampled dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2612383e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------------------+\n",
      "|skew_col|row_count|percentage_of_dataframe|\n",
      "+--------+---------+-----------------------+\n",
      "|       A|       10|   0.010004301849795413|\n",
      "|       B|       91|    0.09103914683313824|\n",
      "|       C|      886|     0.8863811438918736|\n",
      "|       D|     8984|        8.9878647818562|\n",
      "|       E|    89986|        90.024710625569|\n",
      "+--------+---------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "skewed_sample = skewed_df.sample(fraction= 0.1, withReplacement= False)\n",
    "\n",
    "(skewed_sample.groupBy('skew_col')\n",
    "    .agg(F.count('skew_col').alias('row_count'))\n",
    "    .withColumn('percentage_of_dataframe',F.col('row_count')/skewed_sample.count()*100)\n",
    "    .sort('skew_col')\n",
    "    .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d30391",
   "metadata": {},
   "source": [
    "```r\n",
    "skewed_sample <- skewed_df %>% sparklyr::sdf_sample(fraction= 0.1, replacement= FALSE)\n",
    "\n",
    "n_rows_sample <- skewed_sample %>% sparklyr::sdf_nrow()\n",
    "\n",
    "skewed_sample %>%\n",
    "        dplyr::group_by(skew_col) %>%\n",
    "        dplyr::count(skew_col,name = 'row_count') %>%\n",
    "        sparklyr::mutate(percentage_of_dataframe = row_count/n_rows_sample*100) %>%\n",
    "        sdf_sort('skew_col')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0b6599",
   "metadata": {},
   "source": [
    "From the above example, it looks like the original distribution is preserved.\n",
    "We will now re-run the above sampling, but we first repartition our skewed dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cae450cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------------------+\n",
      "|skew_col|row_count|percentage_of_dataframe|\n",
      "+--------+---------+-----------------------+\n",
      "|       A|       14|   0.014039028499227854|\n",
      "|       B|       75|    0.07520908124586351|\n",
      "|       C|      895|     0.8974950362006376|\n",
      "|       D|     8936|       8.96091133350715|\n",
      "|       E|    89802|      90.05234552054712|\n",
      "+--------+---------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "equal_partitions_df = skewed_df.repartition(20)\n",
    "\n",
    "equal_partitions_sample = equal_partitions_df.sample(fraction=0.1, withReplacement=False)\n",
    "\n",
    "(equal_partitions_sample.groupBy('skew_col')\n",
    "    .agg(F.count('skew_col').alias('row_count'))\n",
    "    .withColumn('percentage_of_dataframe',F.col('row_count')/equal_partitions_sample.count()*100)\n",
    "    .sort('skew_col')\n",
    "    .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6355cc6",
   "metadata": {},
   "source": [
    "```r\n",
    "equal_partitions_df  <- skewed_df %>% sparklyr::sdf_repartition(20)\n",
    "\n",
    "equal_partitions_sample <- equal_partitions_df %>% sparklyr::sdf_sample(fraction= 0.1, replacement= FALSE)\n",
    "\n",
    "n_rows_sample_equal <- equal_partitions_sample %>% sparklyr::sdf_nrow()\n",
    "\n",
    "equal_partitions_sample %>%\n",
    "        dplyr::group_by(skew_col) %>%\n",
    "        dplyr::count(skew_col,name = 'row_count') %>%\n",
    "        sparklyr::mutate(percentage_of_dataframe = row_count/n_rows_sample_equal*100) %>%\n",
    "        sdf_sort('skew_col')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bb0f92",
   "metadata": {},
   "source": [
    "From the above examples we can see that we get similar samples regardless of how the data is partitioned, where each row within the dataframe is equally likely to be added to the sample.\n",
    "Although one sample has been shown here, this has been tested using multiple random samples.\n",
    "\n",
    "#### Sampling with Replacement\n",
    "We have constructed a small example for sampling with replacement. Here we count the number of times the unique `IncidentNumber` occurs within the sampled dataframe.\n",
    "This will show how many times each `IncidentNumber` occurs within the sample, verifiying we are sampling with replacement as these rows have been sampled multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6fa7c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "| IncidentNumber|count|\n",
      "+---------------+-----+\n",
      "|       70935101|    2|\n",
      "|       15682091|    2|\n",
      "|133403-01102016|    2|\n",
      "|       40271121|    2|\n",
      "|       63575131|    2|\n",
      "+---------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "replacement_sample = rescue.sample(fraction = 0.1, withReplacement = True, seed = 20)\n",
    "\n",
    "(replacement_sample.groupBy('IncidentNumber')\n",
    "                     .agg(F.count('IncidentNumber').alias('count'))\n",
    "                     .orderBy('count',ascending = False)\n",
    "                     .show(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9050a437",
   "metadata": {},
   "source": [
    "```r\n",
    "replacement_sample = rescue %>% sparklyr::sdf_sample(fraction=0.1, replacement=TRUE, seed = 20)\n",
    "replacement_sample %>% sparklyr::sdf_nrow()\n",
    "\n",
    "replacement_sample %>%\n",
    "    dplyr::group_by(IncidentNumber) %>%\n",
    "    dplyr::count(IncidentNumber)%>%\n",
    "    dplyr::arrange(desc(n))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c9e60b",
   "metadata": {},
   "source": [
    "The above examples show that a number of rows occur twice within the sample. While we have not presented the details here, sampling with replacement does still preserve the partitions and original distribution of the original dataframe as shown above for sampling without replacement.\n",
    "\n",
    "### Stratified samples: `.sampleBy()`\n",
    "\n",
    "A stratified sample can be taken with [`.sampleBy()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.sampleby.html) in PySpark. This takes a column, `col`, to sample by, and a dictionary of weights, `fractions`.\n",
    "\n",
    "In common with other sampling methods this does not return an exact proportion and you can also optionally set a seed.\n",
    "\n",
    "Note that there is no native sparklyr implementation for stratified sampling, although there is a method for weighted sampling, [`sdf_weighted_sample()`](https://spark.rstudio.com/packages/sparklyr/latest/reference/sdf_weighted_sample.html).\n",
    "\n",
    "In PySpark, to return $5\\%$ of cats, $10\\%$ of dogs and $50\\%$ of hamsters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "129f5333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+\n",
      "|animal_type|row_count|\n",
      "+-----------+---------+\n",
      "|        Cat|      140|\n",
      "|        Dog|      120|\n",
      "|    Hamster|        7|\n",
      "+-----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weights = {\"Cat\": 0.05, \"Dog\": 0.1, \"Hamster\": 0.5}\n",
    "stratified_sample = rescue.sampleBy(\"animal_type\", fractions=weights)\n",
    "stratified_sample_count = (stratified_sample\n",
    "                           .groupBy(\"animal_type\")\n",
    "                           .agg(F.count(\"animal_type\").alias(\"row_count\"))\n",
    "                           .orderBy(\"animal_type\"))\n",
    "stratified_sample_count.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de7f8b9",
   "metadata": {},
   "source": [
    "We can quickly compare the number of rows for each animal to the expected to confirm that they are approximately equal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43f9595c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+------+-------------+---------+\n",
      "|animal_type|count|weight|expected_rows|row_count|\n",
      "+-----------+-----+------+-------------+---------+\n",
      "|        Cat| 2909|  0.05|        145.0|      140|\n",
      "|        Dog| 1008|   0.1|        101.0|      120|\n",
      "|    Hamster|   14|   0.5|          7.0|        7|\n",
      "+-----------+-----+------+-------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weights_df = spark.createDataFrame(list(weights.items()), schema=[\"animal_type\", \"weight\"])\n",
    "\n",
    "(rescue\n",
    "    .groupBy(\"animal_type\").count()\n",
    "    .join(weights_df, on=\"animal_type\", how=\"inner\")\n",
    "    .withColumn(\"expected_rows\", F.round(F.col(\"count\") * F.col(\"weight\"), 0))\n",
    "    .join(stratified_sample_count, on=\"animal_type\", how=\"left\")\n",
    "    .orderBy(\"animal_type\")\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c64aff6",
   "metadata": {},
   "source": [
    "#### An example using the skewed dataset:\n",
    "\n",
    "Using `.sampleBy()` to sample a skewed dataset is perhaps better than using `.sample()` as you can specify the fractions of each strata within your sample to ensure that the strata in the sample are representative of the overall population. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b48e92a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------------------+\n",
      "|skew_col|row_count|percentage_of_dataframe|\n",
      "+--------+---------+-----------------------+\n",
      "|       A|       19|   0.006711883566482973|\n",
      "|       B|       94|    0.03320616080259997|\n",
      "|       C|     4507|     1.5921294333757243|\n",
      "|       D|     8999|      3.178960011304225|\n",
      "|       E|   269461|      95.18899251095097|\n",
      "+--------+---------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sk_weights = {\"A\":0.2, \"B\": 0.1, \"C\": 0.5, \"D\":0.1, \"E\":0.3}\n",
    "stratified_sk_sample = skewed_df.sampleBy(\"skew_col\", fractions=sk_weights)\n",
    "\n",
    "(stratified_sk_sample\n",
    "    .groupBy('skew_col')\n",
    "    .agg(F.count('skew_col').alias('row_count'))\n",
    "    .withColumn('percentage_of_dataframe',F.col('row_count')/stratified_sk_sample.count()*100)\n",
    "    .sort('skew_col')\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f830a0eb",
   "metadata": {},
   "source": [
    "**Conclusion**: the .sampleBy() function will produce similar results independant of partitions!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacbf6c7",
   "metadata": {},
   "source": [
    "### Information on alternate sampling methods\n",
    "\n",
    "### More details on sampling\n",
    "\n",
    "The following section gives more detail on sampling and how it is processed on the Spark cluster. It is not compulsory reading, but may be of interest to some Spark users.\n",
    "\n",
    "#### Returning an exact sample\n",
    "\n",
    "We have demonstrated above that `.sample()`/`sdf_sample()` return an approximate fraction, not an exact one. This is because every row is independently assigned a probability equal to the `fraction` of being included in the sample, e.g. with `fraction=0.2` every row has a $20\\%$ probability of being in the sample. The number of rows returned in the sample therefore follows the binomial distribution.\n",
    "\n",
    "The advantage of the sample being calculated in this way is that it is processed as a *narrow transformation*, which is more efficient than a *wide transformation*.\n",
    "\n",
    "To return an exact sample, one method is to calculate how many rows are required in the sample, create a new column of random numbers and sort by it, and use [`.limit()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.limit.html) in PySpark or [`head()`](https://stat.ethz.ch/R-manual/R-devel/library/utils/html/head.html) in sparklyr. This requires an action and a wide transformation, and so will take longer to process than using `.sample()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca7fd820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "590"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fraction = 0.1\n",
    "row_count = round(rescue.count() * fraction)\n",
    "row_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96fd2b2",
   "metadata": {},
   "source": [
    "```r\n",
    "fraction <- 0.1\n",
    "row_count <- round(sparklyr::sdf_nrow(rescue) * fraction)\n",
    "row_count\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9051de0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "590"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rescue.withColumn(\"rand_no\", F.rand()).orderBy(\"rand_no\").limit(row_count).drop(\"rand_no\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9dec8c",
   "metadata": {},
   "source": [
    "```r\n",
    "\n",
    "rescue %>%\n",
    "    sparklyr::mutate(rand_no = rand()) %>%\n",
    "    dplyr::arrange(rand_no) %>%\n",
    "    head(row_count) %>%\n",
    "    sparklyr::select(rand_no) %>%\n",
    "    sparklyr::sdf_nrow()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9417df7e",
   "metadata": {},
   "source": [
    "#### Returning an exact sample - Stratified Sampling\n",
    "If you are wishing to use a form of stratified sampling where an exact number of samples are needed per strata, this can be implemented using a window function in either PySpark or SparklyR. More details and examples using window functions can be found in the [window function page](https://best-practice-and-impact.github.io/ons-spark/spark-functions/window-functions.html). We found that the simplest way of returning an exact number of samples per strata is to create a column for each strata with the number of samples required. To simplify this example we have reduced the number of animals in the `animal_type` column to 5: Bird; Cat; Dog; Fox; and Other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35c692fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|animal_type|count|\n",
      "+-----------+-----+\n",
      "|        Cat| 2909|\n",
      "|       Bird| 1100|\n",
      "|        Dog| 1008|\n",
      "|      Other|  643|\n",
      "|        Fox|  238|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing to simplify the number of animals in the `animal_type` column\n",
    "simplified_animal_types = rescue.withColumn('animal_type',F.when(~F.col('animal_type').isin(['Cat','Bird','Dog','Fox']),'Other')\n",
    "                                              .otherwise(F.col('animal_type')))\n",
    "\n",
    "# Counting the number of animals in each group\n",
    "simplified_animal_types.groupBy('animal_type').agg(F.count('animal_type').alias('count')).sort('count',ascending = False).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a73d57",
   "metadata": {},
   "source": [
    "```r\n",
    "# Preprocessing to simplify the number of animals in the `animal_type` column\n",
    "simplified_animal_types <- rescue %>% sparklyr::mutate(animal_type = case_when(\n",
    "                                                        !animal_type %in% c('Cat','Bird','Dog','Fox') ~ 'Other',\n",
    "                                                        .default = animal_type))\n",
    "\n",
    "# Counting the number of animals in each group\n",
    "simplified_animal_types %>%\n",
    "        dplyr::group_by(animal_type) %>%\n",
    "        dplyr::count(animal_type,name = 'row_count') %>%\n",
    "        sdf_sort('animal_type')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3904c2",
   "metadata": {},
   "source": [
    "Next we create our column which has the required number of samples per strata. This can be done simply using `F.when` statements. This may not be viable when sampling across a lot of variables as a `when` statement is needed for each distinct value. An alternate method can be done using UDFs or a mapping function (An example of this can be found further down the page in the [Simplifying the required sample column](#simplifying-the-required-sample-column) section). The efficiency of these methods has not been thoroughly tested for large datasets and a large number of strata, but we expect that UDFs would be less efficient vs Pyspark mappings and therefore mappings are the recommended method. For more information on UDFs and their efficiency, see the [Pandas UDFs](../ancillary-topics/pandas-udfs.ipynb) page. Alongside the `sample_size` column, we will also give each row a randomly generated number from a uniform distribution which is used to order the rows per strata.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2446ceb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample_size column for each strata \n",
    "simplified_animal_types = (simplified_animal_types.withColumn('sample_size',F.when(F.col('animal_type') == 'Bird',15)\n",
    "                                                .when(F.col('animal_type') == 'Cat',20)\n",
    "                                                .when(F.col('animal_type') == 'Dog',10)\n",
    "                                                .when(F.col('animal_type') == 'Fox',2)\n",
    "                                                .when(F.col('animal_type') == 'Other',5)\n",
    "                                                .otherwise(0))\n",
    "                                                .withColumn('random_number',F.rand())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434cdb97-adfe-4be4-a7af-8a6dbe62e82c",
   "metadata": {},
   "source": [
    "```r\n",
    "simplified_animal_types <- simplified_animal_types %>% \n",
    "                            sparklyr::mutate(sample_size = case_when(\n",
    "                                            animal_type == 'Bird' ~ 15,\n",
    "                                            animal_type == 'Cat' ~ 20,\n",
    "                                            animal_type == 'Dog' ~ 10,\n",
    "                                            animal_type == 'Fox' ~ 2,\n",
    "                                            animal_type == 'Other' ~ 5,\n",
    "                                            .default = 0)) %>%\n",
    "                            sparklyr::mutate(random_number = rand())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4d35ff",
   "metadata": {},
   "source": [
    "Using a window function, we create a column `strata_rank` which will rank each of the random numbers from `random_number` column in order. From this, we create a final column `sampled` which will contain a `1` if the ranked value of a random number is less than or equal to the required sample size for each strata. Otherwise it will contain a `0`. To obtain our final sample, we simply filter on the `sampled` column and extract all rows where this equals `1`. We can check the number of entries per sample by using a aggregating function (`agg`) and a `groupBy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a47429fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+-----+\n",
      "|animal_type|sample_size|count|\n",
      "+-----------+-----------+-----+\n",
      "|        Cat|         20|   20|\n",
      "|       Bird|         15|   15|\n",
      "|        Dog|         10|   10|\n",
      "|      Other|          5|    5|\n",
      "|        Fox|          2|    2|\n",
      "+-----------+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "# use Window function to rank based on size of random number\n",
    "simplified_animal_types = simplified_animal_types.withColumn(\"strata_rank\",F.rank().over(Window.partitionBy('animal_type').orderBy('random_number')))\n",
    "# Set sampled column to 1 if rank is lower than sample_size, 0 otherwise\n",
    "simplified_animal_types = (simplified_animal_types.withColumn('sampled',F.when((F.col('strata_rank') <= F.col('sample_size')), 1)\n",
    "                                    .otherwise(0)))\n",
    " \n",
    "# Take our sample by filtering where `sampled` is 1\n",
    "sample = simplified_animal_types.filter(F.col('sampled') == 1)\n",
    "# Count rows in sample\n",
    "sample.groupBy('animal_type','sample_size').agg(F.count('animal_type').alias('count')).sort('count',ascending = False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f12b0b3",
   "metadata": {},
   "source": [
    "```r\n",
    "\n",
    "simplified_animal_types <- simplified_animal_types %>% \n",
    "              dplyr::group_by(animal_type) %>%\n",
    "              sparklyr::mutate(strata_rank = rank(desc(random_number))) %>%\n",
    "              dplyr::ungroup()\n",
    "\n",
    "simplified_animal_types <- simplified_animal_types %>% sparklyr::mutate(sampled = case_when(\n",
    "              strata_rank <= sample_size ~ 1,\n",
    "              .default = 0))\n",
    "\n",
    "sampled <- simplified_animal_types %>% sparklyr::filter(sampled == 1)\n",
    "\n",
    "sampled %>%\n",
    "        dplyr::group_by(animal_type,sample_size) %>%\n",
    "        dplyr::count(animal_type,name = 'row_count') %>%\n",
    "        sdf_sort('animal_type')\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8040965",
   "metadata": {},
   "source": [
    "It should be pointed out that using a Window function and partitioning by a strata can lead to issues in datasets which have large skews. If one strata is significantly larger than others, this could also cause memory overflow issues on the executors resulting in spark sessions crashing. For more info see the [partitioning page](../spark-concepts/partitions).\n",
    "##### Simplifying the required sample column\n",
    "We can simplify the creation of the required sample column by using dictionaries (python) or a dataframe and parse expressions (R). This does require the use of the `itertools` package and `chain()` function to map the values within the column for python. A similar work around is also possible in R by using `dplyr::tibble()` and `parse_exprs()`. Both methods can be used to easily set the `sample_size` column when dealing with a large number of strata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da1e9eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+-----+\n",
      "|animal_type|sample_size|count|\n",
      "+-----------+-----------+-----+\n",
      "|       Bird|         15| 1100|\n",
      "|        Cat|         20| 2909|\n",
      "|        Dog|         10| 1008|\n",
      "|        Fox|          2|  238|\n",
      "|      Other|          5|  643|\n",
      "+-----------+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "# Create dictionary\n",
    "strata_dictionary = {'Bird':15, 'Cat':20, 'Dog':10, 'Fox': 2, 'Other':5}\n",
    "\n",
    "\n",
    "mapping_example = rescue.withColumn('animal_type',F.when(~F.col('animal_type').isin(['Cat','Bird','Dog','Fox']),'Other')\n",
    "                                              .otherwise(F.col('animal_type')))\n",
    "\n",
    "mapping_expression = F.create_map([F.lit(x) for x in chain(*strata_dictionary.items())]) \n",
    "mapping_example = mapping_example.withColumn(\"sample_size\", \n",
    "              mapping_expression[F.col(\"animal_type\")])\n",
    "\n",
    "mapping_example.groupBy('animal_type','sample_size').agg(F.count('animal_type').alias('count')).sort('animal_type').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "# Create our reference dataframe\n",
    "strata_reference <- dplyr::tibble(type = c(\"Bird\", \"Dog\", \"Cat\", \"Fox\", \"Other\"),\n",
    "                                sample_size = c(105, 20, 20, 2, 5))\n",
    "\n",
    "# Generate sample_size column\n",
    "tibble_example <- rescue %>% sparklyr::mutate(animal_type = case_when(\n",
    "                                                        !animal_type %in% c('Cat','Bird','Dog','Fox') ~ 'Other',\n",
    "                                                        .default = animal_type))\n",
    "\n",
    "tibble_example <- tibble_example %>% \n",
    "                        sparklyr::mutate(sample_size = case_when(!!!rlang::parse_exprs(paste0(\"animal_type == \\\"\",\n",
    "                                                                    strata_reference$type, \n",
    "                                                                    \"\\\" ~ \",\n",
    "                                                                    strata_reference$sample_size,\n",
    "                                                                    collapse = \"\\n\"))))\n",
    "\n",
    "# Check sample size column has been generated correcty\n",
    "tibble_example %>% \n",
    "            dplyr::group_by(animal_type,sample_size) %>%\n",
    "            dplyr::count(animal_type,name = 'row_count') %>%\n",
    "            sdf_sort('animal_type')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b65321",
   "metadata": {},
   "source": [
    "#### Partitioning\n",
    "\n",
    "Even though your sample DF will be smaller than the original DF the number of partitions will remain the same. For example, by default your original DF will have 200 partitions, but your sample DF may only be a 10 % fraction of the original DF and therefore does not need to be partitioned in 200 parts. Too many partitions can be expensive for Spark as it causes excessive overhead for managing smaller tasks; please see our page on [partitions](https://best-practice-and-impact.github.io/ons-spark/spark-concepts/partitions.html) for more information.\n",
    "\n",
    "To reduce the number of partitions in your sample you can use .coalesce() in PySpark or sdf_coalesce() in SparklyR. An example of reducing partitions to 20 in PySpark is given here: `df.sample(fraction=0.1).coalesce(20)`.\n",
    "\n",
    "#### Sampling consistently by filtering the data\n",
    "\n",
    "If the primary reason for using sampling is to process less data during development then an alternative is to filter the data and specify a condition which gives approximately the desired number of rows. This will give consistent results, which may or may not be desirable. For instance, in the Animal Rescue data we could use two years data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb1757cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1142"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rescue.filter(F.col(\"CalYear\").isin(2012, 2017)).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f25385",
   "metadata": {},
   "source": [
    "```r\n",
    "rescue %>%\n",
    "    sparklyr::filter(CalYear == 2012 | CalYear == 2017) %>%\n",
    "    sparklyr::sdf_nrow()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8518062b",
   "metadata": {},
   "source": [
    "The disadvantage of this method is that you may have data quality issues in the original DF that will not be encountered, whereas these may be discovered with `.sample()`. Using unit testing and test driven development can mitigate the risk of these issues.\n",
    "\n",
    "\n",
    "#### Splitting a DF: `.randomSplit()` and `sdf_random_split()`\n",
    "\n",
    "Every row in the DF will be allocated to one of the split DFs. In common with the other sampling methods the exact size of each split may vary. An optional seed can also be set.\n",
    "\n",
    "For instance, to split the animal rescue data into three DFs with a weighting of $50\\%$, $40\\%$ and $10\\%$ in PySpark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "78681f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split1: 2897\n",
      "Split2: 2400\n",
      "Split3: 601\n"
     ]
    }
   ],
   "source": [
    "split1, split2, split3 = rescue.randomSplit([0.5, 0.4, 0.1])\n",
    "\n",
    "print(f\"Split1: {split1.count()}\")\n",
    "print(f\"Split2: {split2.count()}\")\n",
    "print(f\"Split3: {split3.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659004d2",
   "metadata": {},
   "source": [
    "```r\n",
    "splits <- rescue %>% sparklyr::sdf_random_split(\n",
    "    split1 = 0.5,\n",
    "    split2 = 0.4,\n",
    "    split3 = 0.1)\n",
    "\n",
    "print(paste0(\"Split1: \", sparklyr::sdf_nrow(splits$split1)))\n",
    "print(paste0(\"Split2: \", sparklyr::sdf_nrow(splits$split2)))\n",
    "print(paste0(\"Split3: \", sparklyr::sdf_nrow(splits$split3)))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf89cef",
   "metadata": {},
   "source": [
    "Check that the count of the splits equals the total row count:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4970c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF count: 5898\n",
      "Split count total: 5898\n"
     ]
    }
   ],
   "source": [
    "print(f\"DF count: {rescue.count()}\")\n",
    "print(f\"Split count total: {split1.count() + split2.count() + split3.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f6ee24",
   "metadata": {},
   "source": [
    "```r\n",
    "print(paste0(\"DF count: \", sparklyr::sdf_nrow(rescue)))\n",
    "print(paste0(\"Split count total: \",\n",
    "             sparklyr::sdf_nrow(splits$split1) +\n",
    "             sparklyr::sdf_nrow(splits$split2) +\n",
    "             sparklyr::sdf_nrow(splits$split3)))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29a65fa",
   "metadata": {},
   "source": [
    "#### Splitting via `monotonically_increasing_id()` or `sdf_with_unique_id()`\n",
    "Finally we will cover an alternate method of splitting a dataframe by using the `monotonically_increasing_id()` Pyspark function or `sdf_with_unique_id()` SparklyR function.\n",
    "This will add a unique id number to each row which is larger than the previous id. \n",
    "Within a partition, rows will be numbered sequentially starting at 0 for the first row in the first partition, with large increases in row id occuring between partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3a4f0052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1966 1966 1966\n"
     ]
    }
   ],
   "source": [
    "rescue_id = rescue.repartition(20)\n",
    "\n",
    "rescue_id = (rescue_id\n",
    "             .withColumn('row_id', F.monotonically_increasing_id())\n",
    "             .withColumn('group_number', F.col('row_id')%3)\n",
    ")\n",
    "\n",
    "rescue_subsample_1 = rescue_id.filter(F.col('group_number') == 0)\n",
    "rescue_subsample_2 = rescue_id.filter(F.col('group_number') == 1)\n",
    "rescue_subsample_3 = rescue_id.filter(F.col('group_number') == 2)\n",
    "\n",
    "print(rescue_subsample_1.count(),\n",
    "    rescue_subsample_2.count(),\n",
    "    rescue_subsample_3.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b332f46",
   "metadata": {},
   "source": [
    "```r\n",
    "rescue_id <- rescue %>% sparklyr::sdf_repartition(20)\n",
    "\n",
    "rescue_id <- rescue_id %>%\n",
    "                  sparklyr::sdf_with_unique_id(id = \"id\") %>%\n",
    "                  sparklyr::mutate(group_number = id%%3)\n",
    "\n",
    "rescue_subsample_1 <- rescue_id %>% filter(group_number == 0)\n",
    "rescue_subsample_2 <- rescue_id %>% filter(group_number == 1)\n",
    "rescue_subsample_3 <- rescue_id %>% filter(group_number == 2)\n",
    "\n",
    "cat(rescue_subsample_1 %>% sparklyr::sdf_nrow(),\n",
    "rescue_subsample_2 %>% sparklyr::sdf_nrow(),\n",
    "rescue_subsample_3 %>% sparklyr::sdf_nrow())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f25be03",
   "metadata": {},
   "source": [
    "### Further Resources\n",
    "\n",
    "Spark at the ONS Articles:\n",
    "- [Persisting in Spark](https://best-practice-and-impact.github.io/ons-spark/spark-concepts/persistence.html)\n",
    "- [Caching](https://best-practice-and-impact.github.io/ons-spark/spark-concepts/cache.html)\n",
    "- [Checkpoint](https://best-practice-and-impact.github.io/ons-spark/spark-concepts/checkpoint-staging.html#checkpoint)\n",
    "- [Partitions](https://best-practice-and-impact.github.io/ons-spark/spark-concepts/partitions.html)\n",
    "- [Pandas UDFs](../ancillary-topics/pandas-udfs.ipynb)\n",
    "\n",
    "PySpark Documentation:\n",
    "- [`.sample()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.sample.html)\n",
    "- [`.limit()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.limit.html)\n",
    "- [`.coalesce()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.coalesce.html)\n",
    "- [`.randomSplit()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.randomSplit.html)\n",
    "- [`.sampleBy()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.sampleBy.html)\n",
    "- [`.monotonically_increasing_id()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.monotonically_increasing_id.html)\n",
    "\n",
    "sparklyr Documentation:\n",
    "- [`sdf_sample()`](https://spark.rstudio.com/packages/sparklyr/latest/reference/sdf_sample.html)\n",
    "- [`sdf_coalesce()`](https://spark.rstudio.com/packages/sparklyr/latest/reference/sdf_coalesce.html)\n",
    "- [`sdf_random_split()`](https://spark.rstudio.com/packages/sparklyr/latest/reference/sdf_random_split.html)\n",
    "- [`sdf_weighted_sample()`](https://spark.rstudio.com/packages/sparklyr/latest/reference/sdf_weighted_sample.html)\n",
    "- [`sdf_with_unique_id()`](https://spark.rstudio.com/packages/sparklyr/latest/reference/sdf_with_unique_id.html#sdf_with_unique_id)\n",
    "\n",
    "R Documentation:\n",
    "- [`head()`](https://stat.ethz.ch/R-manual/R-devel/library/utils/html/head.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
