{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitions when Unioning or Binding DataFrames (working title)\n",
    "\n",
    "You can append two DataFrames in PySpark that have the same schema with the [`.union()`](https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.union) operation. In sparklyr, the equivalent operation is [`sdf_bind_rows()`](https://spark.rstudio.com/reference/sdf_bind.html) and R users will often refer to appending two DataFrames as *binding*.\n",
    "\n",
    "The `.union()` function in PySpark and the `sdf_bind_rows()` function in sparklyr are both equivalent to `UNION ALL` in SQL. A regular `UNION` operation in SQL will remove duplicates, whereas `.union()` in PySpark and `sdf_bind_rows()` in sparklyr will not.\n",
    "\n",
    "Unlike joining two DataFrames, `.union()`/`sdf_bind_rows()`  does not involve a full shuffle, as the data does not move between partitions. Instead, the number of partitions in the unioned DataFrame is equal to the sum of the number of partitions in the two source DataFrames, i.e. if you union a DataFrame consisting of 100 partitions and one consisting of 50 partitions, your unioned DataFrame will have 150 partitions.\n",
    "\n",
    "To avoid excessive number of partitions, you can use [`.coalesce()`](https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.coalesce) or [`.repartition()`](https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.repartition) to reduce the number of partitions (use [`sdf_coalesce()`](https://spark.rstudio.com/reference/sdf_coalesce.html) or [`sdf_repartition()`](https://spark.rstudio.com/reference/sdf_repartition.html)\n",
    " in sparklyr). The DataFrame will also get repartitioned when a *wide transformation* is applied to the DataFrame (also called a shuffle), e.g. with a `.groupBy()` or `.orderBy()`.\n",
    " \n",
    "It is also worth being aware that storing data on HDFS as many small files is inefficient, both in terms of how the data is stored and in reading it in. Unioning many DataFrames and then writing straight to HDFS can be a cause of this issue. For more information on the small file issue, see the Section on Coalcessing partitions **NATHAN**\n",
    "\n",
    "\n",
    "\n",
    "First we start a Spark session, read the Animal Rescue data, group by animal and year, and then count. The grouping and aggregation will cause a shuffle, meaning that the DataFrame will have 12 partitions, as we set `spark.sql.shuffle.partitions` to `12` in the `SparkSession.builder`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>animal_group</th>\n",
       "      <th>cal_year</th>\n",
       "      <th>animal_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bird</td>\n",
       "      <td>2010</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hamster</td>\n",
       "      <td>2011</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Unknown - Domestic Animal Or Pet</td>\n",
       "      <td>2012</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Unknown - Heavy Livestock Animal</td>\n",
       "      <td>2012</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sheep</td>\n",
       "      <td>2012</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       animal_group  cal_year  animal_count\n",
       "0                              Bird      2010            99\n",
       "1                           Hamster      2011             3\n",
       "2  Unknown - Domestic Animal Or Pet      2012            18\n",
       "3  Unknown - Heavy Livestock Animal      2012             4\n",
       "4                             Sheep      2012             1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "import yaml\n",
    "\n",
    "# Create spark session\n",
    "spark = (SparkSession.builder.master(\"local[2]\")\n",
    "         .appName(\"Partitioning-Unioning\")\n",
    "         .config(\"spark.sql.shuffle.partitions\", 12)\n",
    "         .getOrCreate())\n",
    "\n",
    "# Set the data path\n",
    "with open(\"../../../config.yaml\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "rescue_path_csv = config[\"rescue_path_csv\"]\n",
    "\n",
    "# Read in and shuffle data\n",
    "rescue = (spark.read.csv(rescue_path_csv, header=True, inferSchema=True)  \n",
    "          .withColumnRenamed(\"IncidentNumber\", \"incident_number\")\n",
    "          .withColumnRenamed(\"AnimalGroupParent\", \"animal_group\")\n",
    "          .withColumnRenamed(\"CalYear\", \"cal_year\")\n",
    "          .groupBy(\"animal_group\", \"cal_year\")\n",
    "          .agg(F.count(\"incident_number\").alias(\"animal_count\")))\n",
    "\n",
    "rescue.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "library(sparklyr)\n",
    "library(dplyr)\n",
    "\n",
    "# Create spark session\n",
    "small_config <- sparklyr::spark_config()\n",
    "small_config$spark.sql.shuffle.partitions <- 12\n",
    "\n",
    "sc <- sparklyr::spark_connect(\n",
    "  master = \"local\",\n",
    "  app_name = \"Partitioning-Unioning\",\n",
    "  config = small_config)\n",
    "\n",
    "sparklyr::spark_connection_is_open(sc)\n",
    "\n",
    "# Set the data path\n",
    "config <- yaml::yaml.load_file(\"ons-spark/config.yaml\")\n",
    "# Read in and shuffle data\n",
    "rescue <- sparklyr::spark_read_csv(sc, config$rescue_path_csv, header=TRUE, infer_schema=TRUE)\n",
    "\n",
    "rescue <- rescue %>%\n",
    "    dplyr::rename(\n",
    "        incident_number = IncidentNumber,\n",
    "        animal_group = AnimalGroupParent,\n",
    "        cal_year = CalYear) %>%\n",
    "        dplyr::group_by(animal_group,cal_year) %>%\n",
    "        dplyr::count(animal_count = count())\n",
    "\n",
    "rescue %>% head(5)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can confirm the number of partitions of the `rescue` DataFrame using `.rdd.getNumPartitions()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rescue partitions: 12\n"
     ]
    }
   ],
   "source": [
    "print('Rescue partitions:',rescue.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "print(paste0(\"Rescue partitions: \", sparklyr::sdf_num_partitions(rescue)))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create will create some smaller DataFrames, containing different animals, by filtering the rescue data. We will preview just the `dogs` data, as all others will be of a similar format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>animal_group</th>\n",
       "      <th>cal_year</th>\n",
       "      <th>animal_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dog</td>\n",
       "      <td>2011</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dog</td>\n",
       "      <td>2017</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dog</td>\n",
       "      <td>2009</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dog</td>\n",
       "      <td>2012</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dog</td>\n",
       "      <td>2014</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  animal_group  cal_year  animal_count\n",
       "0          Dog      2011           103\n",
       "1          Dog      2017            81\n",
       "2          Dog      2009           132\n",
       "3          Dog      2012           100\n",
       "4          Dog      2014            90"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dogs = rescue.filter(F.col(\"animal_group\") == \"Dog\")\n",
    "cats = rescue.filter(F.col(\"animal_group\") == \"Cat\")\n",
    "hamsters = rescue.filter(F.col(\"animal_group\") == \"Hamster\")\n",
    "\n",
    "dogs.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "dogs <- rescue %>% sparklyr::filter(animal_group == 'Dog')\n",
    "cats <- rescue %>% sparklyr::filter(animal_group == 'Cat')\n",
    "hamsters <- rescue %>% sparklyr::filter(animal_group == 'Hamster')\n",
    "\n",
    "dogs %>% head(5)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check that each of these DataFrames has 12 partitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Dog partitions: 12 \n",
      " Cat partitions: 12 \n",
      " Hamster partitions: 12\n"
     ]
    }
   ],
   "source": [
    "print(' Dog partitions:', dogs.rdd.getNumPartitions(),\n",
    "      '\\n Cat partitions:', cats.rdd.getNumPartitions(),\n",
    "      '\\n Hamster partitions:', hamsters.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "print(paste0(\"Dogs partitions: \", sparklyr::sdf_num_partitions(dogs)))\n",
    "print(paste0(\"Cats partitions: \", sparklyr::sdf_num_partitions(cats)))\n",
    "print(paste0(\"Hamsters partitions: \", sparklyr::sdf_num_partitions(hamsters)))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we apply the union function, the number of partitions in the unioned DataFrame will be the sum of partitions in each DataFrame. In this case we will now have 12 + 12 = 24 partitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dogs and Cats union partitions: 24\n"
     ]
    }
   ],
   "source": [
    "dogs_and_cats = dogs.union(cats)\n",
    "print('Dogs and Cats union partitions:',dogs_and_cats.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "dogs_and_cats = sparklyr::sdf_bind_rows(dogs,cats)\n",
    "print(paste0(\"Dogs and Cats union partitions: \", sparklyr::sdf_num_partitions(dogs_and_cats)))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unioning another DataFrame adds another 12 partitions to make 36 (24 + 12):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dogs, Cats and Hamsters union partitions: 36\n"
     ]
    }
   ],
   "source": [
    "dogs_cats_and_hamsters = dogs_and_cats.union(hamsters)\n",
    "print('Dogs, Cats and Hamsters union partitions:',dogs_cats_and_hamsters.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "dogs_cats_and_hamsters = sparklyr::sdf_bind_rows(dogs_and_cats,hamsters)\n",
    "print(paste0(\"Dogs, Cats and Hamsters union partitions: \", sdf_num_partitions(dogs_cats_and_hamsters)))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we only have 36 partitions here it is easy to see how this might get excessive with too many `.union()` statements. This can also become a bigger problem if the number of partitons is left to its default value of 200, where we would end up with 600 partitions! \n",
    "\n",
    "A subsequent shuffle (e.g. sorting the DataFrame) will reset the number of partitions to that specified in `spark.sql.shuffle.partitions`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dogs_cats_and_hamsters.orderBy(\"animal_group\", \"cal_year\").rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "sparklyr::sdf_num_partitions(dogs_cats_and_hamsters %>% sparklyr::sdf_sort(c('animal_group','cal_year')))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "You can also use `.repartition()` or `.coalesce()`. `.repartition()` involves a shuffle of the DataFrame and puts the data into roughly equal partition sizes, whereas `.coalesce()` combines partitions without a full shuffle, and so is more efficient, although at the potential cost of less equal partition sizes and therefore potential skew in the data. See the see the Section on Coalcessing partitions for more information. **NATHAN**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dogs_cats_and_hamsters.repartition(20).rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "sparklyr::sdf_num_partitions(dogs_cats_and_hamsters %>% sparklyr::sdf_repartition(20))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Further Resources\n",
    "\n",
    "Spark at the ONS Articles:\n",
    "- [Checkpoints and Staging Tables](../spark-concepts/checkpoint-staging.md)\n",
    "\n",
    "PySpark Documentation:\n",
    "- [`.union()`](https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.union)\n",
    "- [`.coalesce()`](https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.coalesce)\n",
    "- [`.repartition()`](https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.repartition)\n",
    "\n",
    "sparklyr Documentation:\n",
    "- [`sdf_bind_rows()`](https://spark.rstudio.com/reference/sdf_bind.html)\n",
    "- [`sdf_coalesce()`](https://spark.rstudio.com/reference/sdf_coalesce.html)\n",
    "- [`sdf_repartition()`](https://spark.rstudio.com/reference/sdf_repartition.html)\n",
    "\n",
    "Spark SQL Documentation:\n",
    "- [`round`](https://spark.apache.org/docs/latest/api/sql/index.html#round)\n",
    "- [`bround`](https://spark.apache.org/docs/latest/api/sql/index.html#bround)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
