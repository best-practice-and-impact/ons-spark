{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitions when Unioning or Binding DataFrames (working title)\n",
    "\n",
    "You can append two DataFrames in PySpark that have the same schema with the [`.union()`](https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.union) operation. In sparklyr, the equivalent operation is [`sdf_bind_rows()`](https://spark.rstudio.com/reference/sdf_bind.html) and R users will often refer to appending two DataFrames as *binding*.\n",
    "\n",
    "The `.union()` function in PySpark and the `sdf_bind_rows()` function in sparklyr are both equivalent to `UNION ALL` in SQL. A regular `UNION` operation in SQL will remove duplicates, whereas `.union()` in PySpark and `sdf_bind_rows()` in sparklyr will not.\n",
    "\n",
    "This article uses the PySpark operation `.union()` as an example, but the principles are identical in sparklyr when using `sdf_bind_rows()`.\n",
    "\n",
    "Unlike joining two DataFrames, `.union()` does not involve a full shuffle, as the data does not move between partitions. Instead, the number of partitions in the unioned DataFrame is equal to the sum of the number of partitions in the two source DataFrames, e.g. if you union a DataFrame consisting of 100 partitions and one consisting of 50 partitions, your unioned DataFrame will have 150 partitions.\n",
    "\n",
    "### Section needed?\n",
    "This issue was highlighed by a user who had created a function which split a DataFrame into three smaller DataFrames, did different calculations to each, and then unioned them together. Each time the function was called the number of partitions in the DataFrame was multiplied by three, so a DataFrame consisting of 200 partitions then had 600, then 1800 etc, and these excessive partitions caused the Spark session to crash, as excessive partitioning is inefficient. [Checkpoints and Staging Tables](https://gitlab-app-l-01/DAP_CATS/troubleshooting/tip-of-the-week/-/blob/master/tip_21_checkpoint.ipynb) or using [staging tables](https://gitlab-app-l-01/DAP_CATS/troubleshooting/tip-of-the-week/-/blob/master/tip_15_staging_tables.ipynb) to cut the lineage of the DataFrame did not work, as the data was written out to disk and then read back in with the same number of partitions.\n",
    "### end\n",
    "\n",
    "To avoid this issue, you can use [`.coalesce()`](https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.coalesce) or [`.repartition()`](https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.repartition) to reduce the number of partitions (use [`sdf_coalesce()`](https://spark.rstudio.com/reference/sdf_coalesce.html) or [`sdf_repartition()`](https://spark.rstudio.com/reference/sdf_repartition.html)\n",
    " in sparklyr). The DataFrame will also get repartitioned when a *wide transformation* is applied to the DataFrame (also called a shuffle), e.g. with a `.groupBy()` or `.orderBy()`.\n",
    " \n",
    "It is also worth being aware that storing data on HDFS as many [small files](https://gitlab-app-l-01/DAP_CATS/troubleshooting/tip-of-the-week/-/blob/master/tip_54_coalesce_small_files.md) is inefficient, both in terms of how the data is stored and in reading it in. Unioning many DataFrames and then writing straight to HDFS can be a cause of this issue.\n",
    "## An example\n",
    "\n",
    "Start a Spark session, read the Animal Rescue data, group by animal and year, and then count. The grouping and aggregation will cause a shuffle, meaning that the DataFrame will have 12 partitions, as we set `spark.sql.shuffle.partitions` to `12` in the `SparkSession.builder`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"festive-dates\")\n",
    "    .config(\"spark.executor.memory\", \"1g\")\n",
    "    .config(\"spark.executor.cores\", 1)\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"true\")\n",
    "    .config(\"spark.dynamicAllocation.maxExecutors\", 3)\n",
    "    .config(\"spark.sql.shuffle.partitions\", 12)\n",
    "    .config(\"spark.shuffle.service.enabled\", \"true\")\n",
    "    .config(\"spark.ui.showConsoleProgress\", \"false\")\n",
    "    .enableHiveSupport()\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "rescue = (spark.read.csv(\"/training/animal_rescue.csv\", header=True, inferSchema=True)\n",
    "          .withColumnRenamed(\"IncidentNumber\", \"incident_number\")\n",
    "          .withColumnRenamed(\"AnimalGroupParent\", \"animal_group\")\n",
    "          .withColumnRenamed(\"CalYear\", \"cal_year\")\n",
    "          .groupBy(\"animal_group\", \"cal_year\")\n",
    "          .agg(F.count(\"incident_number\").alias(\"animal_count\")))\n",
    "\n",
    "rescue.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can confirm the number of partitions with `.rdd.getNumPartitions()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rescue.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create some smaller DataFrames, containing different animals, and preview one of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dogs = rescue.filter(F.col(\"animal_group\") == \"Dog\")\n",
    "cats = rescue.filter(F.col(\"animal_group\") == \"Cat\")\n",
    "hamsters = rescue.filter(F.col(\"animal_group\") == \"Hamster\")\n",
    "\n",
    "dogs.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these DataFrames has 12 partitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dogs.rdd.getNumPartitions(),\n",
    "      cats.rdd.getNumPartitions(),\n",
    "      hamsters.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we union two of them, we now get 12 + 12 = 24 partitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dogs_and_cats = dogs.union(cats)\n",
    "dogs_and_cats.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unioning another DataFrame adds another 12 partitions to make 36 (24 + 12):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dogs_cats_hamsters = dogs_and_cats.union(hamsters)\n",
    "dogs_cats_hamsters.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we only have 36 partitions here it is easy to see how this might get excessive with too many `.union()` statements.\n",
    "\n",
    "A subsequent shuffle (e.g. sorting the DataFrame) will reset the number of partitions to that specified in `spark.sql.shuffle.partitions`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dogs_cats_hamsters.orderBy(\"animal_group\", \"cal_year\").rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **This is a reference to the work Nathan is working on**\n",
    "You can also use `.repartition()` or `.coalesce()`. `.repartition()` involves a shuffle of the DataFrame and puts the data into roughly equal partition sizes, whereas `.coalesce()` combines partitions without a full shuffle, and so is more efficient, although at the potential cost of less equal partition sizes and therefore potential skew in the data. See the [Small files issue](https://gitlab-app-l-01/DAP_CATS/troubleshooting/tip-of-the-week/-/blob/master/tip_54_coalesce_small_files.md) for more information.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dogs_cats_hamsters.repartition(20).rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Further Resources\n",
    "\n",
    "Spark at the ONS Articles:\n",
    "- [Checkpoints and Staging Tables](../spark-concepts/checkpoint-staging.md)\n",
    "\n",
    "PySpark Documentation:\n",
    "- [`.union()`](https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.union)\n",
    "- [`.coalesce()`](https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.coalesce)\n",
    "- [`.repartition()`](https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.repartition)\n",
    "\n",
    "sparklyr Documentation:\n",
    "- [`sdf_bind_rows()`](https://spark.rstudio.com/reference/sdf_bind.html)\n",
    "- [`sdf_coalesce()`](https://spark.rstudio.com/reference/sdf_coalesce.html)\n",
    "- [`sdf_repartition()`](https://spark.rstudio.com/reference/sdf_repartition.html)\n",
    "\n",
    "Spark SQL Documentation:\n",
    "- [`round`](https://spark.apache.org/docs/latest/api/sql/index.html#round)\n",
    "- [`bround`](https://spark.apache.org/docs/latest/api/sql/index.html#bround)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
