{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Generating Synthetic/Dummy Data Using Faker**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Faker\n",
    "\n",
    "`Faker` is a Python package that generates fake data such as names, addresses, emails, dates, credit card numbers, and more.\n",
    "\n",
    "**Key Features:** Randomised generation, locale support, wide range of data types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Install Required Libraries\n",
    "To start, we will need the `Faker` library to generate the dummy data. Additionally, we will use PySpark for working with Spark DataFrames in Python.\n",
    "\n",
    "For Python, use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://njobud:****@onsart-01.ons.statistics.gov.uk/artifactory/api/pypi/yr-python/simple\n",
      "Requirement already satisfied: Faker in /home/cdsw/ons-spark/.venv/lib/python3.10/site-packages (33.3.1)\n",
      "Requirement already satisfied: typing-extensions in /runtime-addons/cmladdon-2.0.46-b210/opt/cmladdons/python/site-packages (from Faker) (4.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.4 in /runtime-addons/cmladdon-2.0.46-b210/opt/cmladdons/python/site-packages (from Faker) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /runtime-addons/cmladdon-2.0.46-b210/opt/cmladdons/python/site-packages (from python-dateutil>=2.4->Faker) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install Faker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For further details to assist on the setup, please visit the [official documentation](https://faker.readthedocs.io/en/master/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check the Version of the `Faker` package installed in your environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faker_version: 33.3.1\n"
     ]
    }
   ],
   "source": [
    "import pkg_resources\n",
    "faker_version = pkg_resources.get_distribution(\"Faker\").version\n",
    "print(f\"faker_version: {faker_version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set Up a Global Random Seed for Reproducibility**\n",
    "\n",
    "You can set a global seed for all data providers and use it without explicitly passing it to each provider:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "fake = Faker()\n",
    "fake.seed_instance(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate a First Fake Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Allison Hill\n",
      "Email: donaldgarcia@example.net\n",
      "Address: 600 Jeffery Parkways\n",
      "New Jamesside, MT 29394\n",
      "Phone: 394.802.6542x351\n",
      "Birthday: 1941-02-14\n"
     ]
    }
   ],
   "source": [
    "# Generate sample data\n",
    "print(\"Name:\", fake.name())\n",
    "print(\"Email:\", fake.email())\n",
    "print(\"Address:\", fake.address())\n",
    "print(\"Phone:\", fake.phone_number())\n",
    "print(\"Birthday:\", fake.date_of_birth(minimum_age=18, maximum_age=90))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. International Data Generation\n",
    "\n",
    "\n",
    "Faker has a great feature that allows it to generate data tailored to specific locales. Here's how you can create data that reflects various locations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "US: PSC 1156, Box 1960\n",
      "APO AP 27868\n",
      "UK: 00 King turnpike\n",
      "Brownton\n",
      "M6E 9NX\n",
      "France: 58, avenue Marie Wagner\n",
      "37523 Bailly-sur-Rodriguez\n",
      "Japan: 千葉県台東区東三島4丁目4番9号\n"
     ]
    }
   ],
   "source": [
    "# Create localised Faker instances\n",
    "fake_us = Faker('en_US')\n",
    "fake_uk = Faker('en_GB')\n",
    "fake_fr = Faker('fr_FR')\n",
    "fake_jp = Faker('ja_JP')\n",
    "\n",
    "print(\"US:\", fake_us.address())\n",
    "print(\"UK:\", fake_uk.address())\n",
    "print(\"France:\", fake_fr.address())\n",
    "print(\"Japan:\", fake_jp.address())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the differences in address formats:\n",
    "\n",
    "* US addresses include state abbreviations and ZIP codes.  \n",
    "* UK addresses use British postal codes.  \n",
    "* France addresses follow European conventions.  \n",
    "* Japan addresses are formatted with the correct characters and local conventions.\n",
    "\n",
    "\n",
    "For an up-to-date list of supported locales, you can check the [official documentation](https://fakerjs.dev/guide/localization.html#available-locales)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Faker’s Provider Architecture\n",
    "\n",
    "\n",
    "Faker utilises a modular system of \"providers,\" where each provider is responsible for generating a specific type of data. Let's take a closer look at how providers work and the types available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Faker Providers:\n",
      "- <faker.providers.user_agent.Provider object at 0x7f6b864cb130>\n",
      "- <faker.providers.ssn.en_GB.Provider object at 0x7f6b864cb220>\n",
      "- <faker.providers.sbn.Provider object at 0x7f6b864cb1c0>\n",
      "- <faker.providers.python.Provider object at 0x7f6b864c96c0>\n",
      "- <faker.providers.profile.Provider object at 0x7f6b864cb100>\n",
      "- <faker.providers.phone_number.en_GB.Provider object at 0x7f6b864ca830>\n",
      "- <faker.providers.person.en_GB.Provider object at 0x7f6b864caaa0>\n",
      "- <faker.providers.passport.en_US.Provider object at 0x7f6b864cacb0>\n",
      "- <faker.providers.misc.en_US.Provider object at 0x7f6b864ca9e0>\n",
      "- <faker.providers.lorem.la.Provider object at 0x7f6b864cab00>\n",
      "- <faker.providers.job.en_US.Provider object at 0x7f6b864cab60>\n",
      "- <faker.providers.isbn.en_US.Provider object at 0x7f6b864cabc0>\n",
      "- <faker.providers.internet.en_GB.Provider object at 0x7f6b864cac20>\n",
      "- <faker.providers.geo.en_US.Provider object at 0x7f6b864caf80>\n",
      "- <faker.providers.file.Provider object at 0x7f6b864cace0>\n",
      "- <faker.providers.emoji.Provider object at 0x7f6b864cad40>\n",
      "- <faker.providers.date_time.en_US.Provider object at 0x7f6b864cada0>\n",
      "- <faker.providers.currency.en_US.Provider object at 0x7f6b864caec0>\n",
      "- <faker.providers.credit_card.en_US.Provider object at 0x7f6b864cae30>\n",
      "- <faker.providers.company.en_US.Provider object at 0x7f6b864cb010>\n",
      "- <faker.providers.color.en_US.Provider object at 0x7f6b864cb070>\n",
      "- <faker.providers.barcode.en_US.Provider object at 0x7f6b864caad0>\n",
      "- <faker.providers.bank.en_GB.Provider object at 0x7f6b864cafb0>\n",
      "- <faker.providers.automotive.en_GB.Provider object at 0x7f6b864cae60>\n",
      "- <faker.providers.address.en_GB.Provider object at 0x7f6b864c9720>\n"
     ]
    }
   ],
   "source": [
    "from faker import Faker\n",
    "\n",
    "fake = Faker('en_GB')\n",
    "\n",
    "print(\"Available Faker Providers:\")\n",
    "for provider in fake.providers:\n",
    "    print(f\"- {provider}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each item in the list corresponds to a specific provider, such as:\n",
    "\n",
    "- **User Agent Provider:** Generates browser and device identification strings.  \n",
    "- **SSN Provider:** Creates valid-format social security numbers (US-specific).  \n",
    "- **SBN Provider:** Generates 9-digit Standard Book Numbers (used in older systems prior to 1974).  \n",
    "\n",
    "In addition to these, Faker includes other commonly used providers for generating various types of data:\n",
    "\n",
    "- **Person Provider:** Generates names, birthdates, and personal details.  \n",
    "- **Address Provider:** Produces realistic street addresses and postal codes.\n",
    "- **Internet Provider:** Generates email addresses, domain names, and URLs.\n",
    "\n",
    "You can explore the full list of available providers and their methods in the official documentation, which offers detailed information and usage examples for each one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Best Practices for Using Faker\n",
    "\n",
    "When working with Faker, keep the following best practices in mind:\n",
    "\n",
    "- Select locale-specific providers when generating data tailored to a particular region.\n",
    "- Combine different providers to generate more realistic and interconnected data.\n",
    "- Organise your fake data generation to align with the requirements of your application.\n",
    "- Explore the official documentation to learn about additional provider features and options."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Building a Typical Dataset\n",
    "\n",
    "\n",
    "Now, let's explore how to combine multiple providers to generate rich and interconnected data for more complex structures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>age</th>\n",
       "      <th>email</th>\n",
       "      <th>street</th>\n",
       "      <th>city</th>\n",
       "      <th>postcode</th>\n",
       "      <th>job</th>\n",
       "      <th>company</th>\n",
       "      <th>username</th>\n",
       "      <th>website</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>William Jennings</td>\n",
       "      <td>35</td>\n",
       "      <td>francisdavidson@example.org</td>\n",
       "      <td>600 Charlie fort</td>\n",
       "      <td>New Joeside</td>\n",
       "      <td>DT79 0GS</td>\n",
       "      <td>Librarian, public</td>\n",
       "      <td>Bryan-Andrews</td>\n",
       "      <td>timothy16</td>\n",
       "      <td>http://butler-gough.info/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Benjamin Simpson</td>\n",
       "      <td>42</td>\n",
       "      <td>fisherteresa@example.net</td>\n",
       "      <td>Studio 31\\nIrene forks</td>\n",
       "      <td>Jasonbury</td>\n",
       "      <td>S4 5GQ</td>\n",
       "      <td>Marine scientist</td>\n",
       "      <td>Davies Ltd</td>\n",
       "      <td>johnronald</td>\n",
       "      <td>http://www.lamb-scott.co.uk/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ricky Lloyd-Duncan</td>\n",
       "      <td>67</td>\n",
       "      <td>wilsonryan@example.com</td>\n",
       "      <td>Flat 13K\\nKelly parks</td>\n",
       "      <td>Youngmouth</td>\n",
       "      <td>M23 9SY</td>\n",
       "      <td>Oceanographer</td>\n",
       "      <td>Lloyd-Turner</td>\n",
       "      <td>dgreen</td>\n",
       "      <td>http://www.walsh.biz/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dr Jasmine Smith</td>\n",
       "      <td>53</td>\n",
       "      <td>daviesgeoffrey@example.net</td>\n",
       "      <td>Studio 51s\\nSteele alley</td>\n",
       "      <td>Donnaburgh</td>\n",
       "      <td>E4W 2QG</td>\n",
       "      <td>Solicitor, Scotland</td>\n",
       "      <td>Bell, Anderson and Jones</td>\n",
       "      <td>murraymohammad</td>\n",
       "      <td>http://www.shaw.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Norman Sharp-Stewart</td>\n",
       "      <td>52</td>\n",
       "      <td>scottknowles@example.net</td>\n",
       "      <td>89 Arnold plains</td>\n",
       "      <td>Lake Graeme</td>\n",
       "      <td>SG57 1JJ</td>\n",
       "      <td>Bonds trader</td>\n",
       "      <td>Brown-Naylor</td>\n",
       "      <td>thomashammond</td>\n",
       "      <td>http://www.howe.com/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   name  age                        email  \\\n",
       "0      William Jennings   35  francisdavidson@example.org   \n",
       "1      Benjamin Simpson   42     fisherteresa@example.net   \n",
       "2    Ricky Lloyd-Duncan   67       wilsonryan@example.com   \n",
       "3      Dr Jasmine Smith   53   daviesgeoffrey@example.net   \n",
       "4  Norman Sharp-Stewart   52     scottknowles@example.net   \n",
       "\n",
       "                     street         city  postcode                  job  \\\n",
       "0          600 Charlie fort  New Joeside  DT79 0GS    Librarian, public   \n",
       "1    Studio 31\\nIrene forks    Jasonbury    S4 5GQ     Marine scientist   \n",
       "2     Flat 13K\\nKelly parks   Youngmouth   M23 9SY        Oceanographer   \n",
       "3  Studio 51s\\nSteele alley   Donnaburgh   E4W 2QG  Solicitor, Scotland   \n",
       "4          89 Arnold plains  Lake Graeme  SG57 1JJ         Bonds trader   \n",
       "\n",
       "                    company        username                       website  \n",
       "0             Bryan-Andrews       timothy16     http://butler-gough.info/  \n",
       "1                Davies Ltd      johnronald  http://www.lamb-scott.co.uk/  \n",
       "2              Lloyd-Turner          dgreen         http://www.walsh.biz/  \n",
       "3  Bell, Anderson and Jones  murraymohammad          http://www.shaw.com/  \n",
       "4              Brown-Naylor   thomashammond          http://www.howe.com/  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from faker import Faker\n",
    "import pandas as pd\n",
    "\n",
    "fake = Faker('en_GB')\n",
    "fake.seed_instance(42)\n",
    "\n",
    "n_rows = 100\n",
    "\n",
    "# Function to generate a user profile\n",
    "def generate_user_profile():\n",
    "    return {\n",
    "        # Personal details\n",
    "        'name': fake.name(),\n",
    "        'age': fake.random_int(min=18, max=80),\n",
    "        'email': fake.email(),\n",
    "        \n",
    "        # Location details\n",
    "        'street': fake.street_address(),\n",
    "        'city': fake.city(),\n",
    "        #'region': fake.region(),\n",
    "        'postcode': fake.postcode(),\n",
    "        \n",
    "        # Professional information\n",
    "        'job': fake.job(),\n",
    "        'company': fake.company(),\n",
    "        \n",
    "        # Other details\n",
    "        'username': fake.user_name(),\n",
    "        'website': fake.url()\n",
    "    }\n",
    "\n",
    "# Generate a list of user profiles\n",
    "profiles = [generate_user_profile() for _ in range(n_rows)]\n",
    "\n",
    "# Convert the list of profiles to a DataFrame\n",
    "df = pd.DataFrame(profiles)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Using Synthetic Data with Big Data Frameworks\n",
    "\n",
    "### 8.1. Using Synthetic Data with PySpark\n",
    "\n",
    "Demonstrate how to create synthetic data in PySpark and use it within Spark DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|                Name|             Address|               Email|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|    William Jennings|2 Sian streets, N...|francescaharrison...|\n",
      "|     Rosemary Wright|654 Robin track, ...|simpsongemma@exam...|\n",
      "|         Sean Norton|103 Robinson walk...|  rita19@example.net|\n",
      "|       Brenda Briggs|Studio 4, Lydia i...|iwilkins@example.org|\n",
      "|Leonard Powell-Mo...|Flat 32G, Green c...|andrea01@example.net|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "from faker import Faker\n",
    "import io\n",
    "import pandas as pd\n",
    "pd.DataFrame.iteritems = pd.DataFrame.items\n",
    "\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"Synthetic Data Example\").getOrCreate()\n",
    "\n",
    "\n",
    "# Create synthetic data with Faker\n",
    "fake = Faker('en_GB')\n",
    "fake.seed_instance(42)\n",
    "\n",
    "n_rows = 100\n",
    "data = [(fake.name(), fake.address(), fake.email()) for _ in range(n_rows)]\n",
    "\n",
    "df_pandas = pd.DataFrame(data, columns=[\"Name\", \"Address\", \"Email\"])\n",
    "\n",
    "# Replace newline characters with commas in the 'Address' column using chaining\n",
    "df_pandas['Address'] = df_pandas['Address'].str.replace(\"\\n\", \", \")\n",
    "\n",
    "#I'm having problems with using spark.createDataFrame, hence, I have to create a csv file and read it\n",
    "df_pandas.to_csv('temp.csv', index=False)\n",
    "\n",
    "df_spark = spark.read.csv('temp.csv', header=True, inferSchema=True)\n",
    "\n",
    "df_spark.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Application to Big Data:**\n",
    "\n",
    "This example can be extended to generate large datasets (millions of records) that can be processed in parallel using PySpark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2. Generate Synthetic Data with More Features\n",
    "\n",
    "We will generate synthetic data with the following additional features:\n",
    "1. Mode of Transportation (Car, Public Transport, Walking, etc.)  \n",
    "2. Highest Education (High School, Bachelor's, Master's, PhD, etc.)  \n",
    "3. Marital Status (Single, Married, Divorced, Widowed)  \n",
    "4. Favorite High Street Supermarket (Tesco, Sainsbury's, Asda, etc.)  \n",
    "5. Pet Ownership (Yes, No - with type of pet if Yes)  \n",
    "\n",
    "We will also simulate some missing data in the dataset, which is commonly encountered in real-world scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Address</th>\n",
       "      <th>Postcode</th>\n",
       "      <th>City</th>\n",
       "      <th>Email</th>\n",
       "      <th>Mode_of_Transport</th>\n",
       "      <th>Education</th>\n",
       "      <th>Marital_Status</th>\n",
       "      <th>Supermarket</th>\n",
       "      <th>Pet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>William Jennings</td>\n",
       "      <td>2 Sian streets, New Maryton, E3 8ZA</td>\n",
       "      <td>L8G 7YL</td>\n",
       "      <td>Port Samchester</td>\n",
       "      <td>ricky23@example.com</td>\n",
       "      <td>Car</td>\n",
       "      <td>Primary</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>Tesco</td>\n",
       "      <td>Cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dr Josh Pritchard</td>\n",
       "      <td>Studio 16, Lynn hill, Melissaborough, BR0X 4DJ</td>\n",
       "      <td>S4 5GQ</td>\n",
       "      <td>Rhysview</td>\n",
       "      <td>johnronald@example.net</td>\n",
       "      <td>None</td>\n",
       "      <td>High School</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>Morrisons</td>\n",
       "      <td>Cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Leigh Randall</td>\n",
       "      <td>0 Alexander circles, New Guy, W67 4FJ</td>\n",
       "      <td>NW8M 9RQ</td>\n",
       "      <td>East Natasha</td>\n",
       "      <td>dgreen@example.org</td>\n",
       "      <td>Public Transport</td>\n",
       "      <td>PhD</td>\n",
       "      <td>Married</td>\n",
       "      <td>Morrisons</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lorraine Palmer</td>\n",
       "      <td>Studio 01, Read junctions, West Tracyburgh, AB...</td>\n",
       "      <td>MK9H 5GX</td>\n",
       "      <td>New Brandonfort</td>\n",
       "      <td>griffithslinda@example.com</td>\n",
       "      <td>Public Transport</td>\n",
       "      <td>Primary</td>\n",
       "      <td>Married</td>\n",
       "      <td>Waitrose</td>\n",
       "      <td>Dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>June Sharp</td>\n",
       "      <td>782 Hill rest, Arnoldside, SM3Y 6QT</td>\n",
       "      <td>W1D 1PA</td>\n",
       "      <td>New Gerald</td>\n",
       "      <td>hammondjulia@example.org</td>\n",
       "      <td>Public Transport</td>\n",
       "      <td>Primary</td>\n",
       "      <td>None</td>\n",
       "      <td>Asda</td>\n",
       "      <td>Dog</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Name                                            Address  \\\n",
       "0   William Jennings                2 Sian streets, New Maryton, E3 8ZA   \n",
       "1  Dr Josh Pritchard     Studio 16, Lynn hill, Melissaborough, BR0X 4DJ   \n",
       "2      Leigh Randall              0 Alexander circles, New Guy, W67 4FJ   \n",
       "3    Lorraine Palmer  Studio 01, Read junctions, West Tracyburgh, AB...   \n",
       "4         June Sharp                782 Hill rest, Arnoldside, SM3Y 6QT   \n",
       "\n",
       "   Postcode             City                       Email Mode_of_Transport  \\\n",
       "0   L8G 7YL  Port Samchester         ricky23@example.com               Car   \n",
       "1    S4 5GQ         Rhysview      johnronald@example.net              None   \n",
       "2  NW8M 9RQ     East Natasha          dgreen@example.org  Public Transport   \n",
       "3  MK9H 5GX  New Brandonfort  griffithslinda@example.com  Public Transport   \n",
       "4   W1D 1PA       New Gerald    hammondjulia@example.org  Public Transport   \n",
       "\n",
       "     Education Marital_Status Supermarket   Pet  \n",
       "0      Primary        Widowed       Tesco   Cat  \n",
       "1  High School        Widowed   Morrisons   Cat  \n",
       "2          PhD        Married   Morrisons  None  \n",
       "3      Primary        Married    Waitrose   Dog  \n",
       "4      Primary           None        Asda   Dog  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from faker import Faker\n",
    "import random\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "fake = Faker('en_GB')\n",
    "fake.seed_instance(42)\n",
    "\n",
    " \n",
    "n_samples = 1000   \n",
    "\n",
    "# Create lists of categories for features not captured in the Faker library\n",
    "mode_of_transport = ['Car', 'Public Transport', 'Walking', 'Cycling', 'Taxi']\n",
    "education_levels = ['Primary', 'High School', 'Vocational', 'Bachelor\\'s', 'Master\\'s', 'PhD']\n",
    "marital_status = ['Single', 'Married', 'Divorced', 'Widowed']\n",
    "supermarkets = ['Tesco', 'Sainsbury\\'s', 'Asda', 'Morrisons', 'Waitrose']\n",
    "pets = ['Dog', 'Cat', 'None']\n",
    "\n",
    "# Generate synthetic data with the new features\n",
    "data = []\n",
    "for _ in range(n_samples):\n",
    "    name = fake.name()\n",
    "    address = fake.address()\n",
    "    postcode = fake.postcode()\n",
    "    city = fake.city()\n",
    "    email = fake.email()\n",
    "    transport = fake.random_element(mode_of_transport)\n",
    "    education = fake.random_element(education_levels)\n",
    "    marital = fake.random_element(marital_status)\n",
    "    supermarket = fake.random_element(supermarkets)\n",
    "    pet = fake.random_element(pets)\n",
    "    \n",
    "    # Simulating missing data by randomly omitting some features\n",
    "    if random.random() < 0.1:  # 10% chance to have missing data for 'Mode of Transport'\n",
    "        transport = None\n",
    "    if random.random() < 0.1:  # 10% chance to have missing data for 'Education'\n",
    "        education = None\n",
    "    if random.random() < 0.1:  # 10% chance to have missing data for 'Marital Status'\n",
    "        marital = None\n",
    "    if random.random() < 0.1:  # 10% chance to have missing data for 'Supermarket'\n",
    "        supermarket = None\n",
    "    if random.random() < 0.1:  # 10% chance to have missing data for 'Pet'\n",
    "        pet = None\n",
    "\n",
    "    data.append([name, address, postcode, city, email, transport, education, marital, supermarket, pet])\n",
    "\n",
    "\n",
    "columns = ['Name', 'Address', 'Postcode', 'City', 'Email', 'Mode_of_Transport', 'Education', 'Marital_Status', 'Supermarket', 'Pet']\n",
    "synthetic_df = pd.DataFrame(data, columns=columns)\n",
    "synthetic_df['Address'] = synthetic_df['Address'].str.replace(\"\\n\", \", \")\n",
    "\n",
    "\n",
    "synthetic_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Integrate with PySpark for Big Data Workflow**\n",
    "\n",
    "Now that we have a robust dataset with additional features and some missing data, let's see how to integrate this with PySpark, which is commonly used in big data workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o76.legacyInferArrayTypeFromFirstElement. Trace:\npy4j.Py4JException: Method legacyInferArrayTypeFromFirstElement([]) does not exist\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326)\n\tat py4j.Gateway.invoke(Gateway.java:274)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mmaster(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocale[2]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSyntheticDataForBigData\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Convert the synthetic DataFrame to a Spark DataFrame\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m spark_df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43msynthetic_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Show the Spark DataFrame\u001b[39;00m\n\u001b[1;32m      7\u001b[0m spark_df\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m5\u001b[39m)\n",
      "File \u001b[0;32m~/ons-spark/.venv/lib/python3.10/site-packages/pyspark/sql/session.py:1440\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1436\u001b[0m     data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data, columns\u001b[38;5;241m=\u001b[39mcolumn_names)\n\u001b[1;32m   1438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pd\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[0;32m-> 1440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mSparkSession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[1;32m   1441\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\n\u001b[1;32m   1442\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1443\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_dataframe(\n\u001b[1;32m   1444\u001b[0m     data, schema, samplingRatio, verifySchema  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m   1445\u001b[0m )\n",
      "File \u001b[0;32m~/ons-spark/.venv/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:363\u001b[0m, in \u001b[0;36mSparkConversionMixin.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    361\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    362\u001b[0m converted_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_from_pandas(data, schema, timezone)\n\u001b[0;32m--> 363\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconverted_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ons-spark/.venv/lib/python3.10/site-packages/pyspark/sql/session.py:1485\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1483\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromRDD(data\u001b[38;5;241m.\u001b[39mmap(prepare), schema, samplingRatio)\n\u001b[1;32m   1484\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1485\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_createFromLocal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1486\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1487\u001b[0m jrdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSerDeUtil\u001b[38;5;241m.\u001b[39mtoJavaArray(rdd\u001b[38;5;241m.\u001b[39m_to_java_object_rdd())\n",
      "File \u001b[0;32m~/ons-spark/.venv/lib/python3.10/site-packages/pyspark/sql/session.py:1093\u001b[0m, in \u001b[0;36mSparkSession._createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m   1090\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(data)\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m-> 1093\u001b[0m     struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inferSchemaFromList\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1094\u001b[0m     converter \u001b[38;5;241m=\u001b[39m _create_converter(struct)\n\u001b[1;32m   1095\u001b[0m     tupled_data: Iterable[Tuple] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(converter, data)\n",
      "File \u001b[0;32m~/ons-spark/.venv/lib/python3.10/site-packages/pyspark/sql/session.py:953\u001b[0m, in \u001b[0;36mSparkSession._inferSchemaFromList\u001b[0;34m(self, data, names)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkValueError(\n\u001b[1;32m    949\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCANNOT_INFER_EMPTY_SCHEMA\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    950\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{},\n\u001b[1;32m    951\u001b[0m     )\n\u001b[1;32m    952\u001b[0m infer_dict_as_struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jconf\u001b[38;5;241m.\u001b[39minferDictAsStruct()\n\u001b[0;32m--> 953\u001b[0m infer_array_from_first_element \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jconf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlegacyInferArrayTypeFromFirstElement\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    954\u001b[0m prefer_timestamp_ntz \u001b[38;5;241m=\u001b[39m is_timestamp_ntz_preferred()\n\u001b[1;32m    955\u001b[0m schema \u001b[38;5;241m=\u001b[39m reduce(\n\u001b[1;32m    956\u001b[0m     _merge_type,\n\u001b[1;32m    957\u001b[0m     (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    966\u001b[0m     ),\n\u001b[1;32m    967\u001b[0m )\n",
      "File \u001b[0;32m~/ons-spark/.venv/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/ons-spark/.venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/ons-spark/.venv/lib/python3.10/site-packages/py4j/protocol.py:330\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 330\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    334\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    335\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    336\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name))\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o76.legacyInferArrayTypeFromFirstElement. Trace:\npy4j.Py4JException: Method legacyInferArrayTypeFromFirstElement([]) does not exist\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326)\n\tat py4j.Gateway.invoke(Gateway.java:274)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark = SparkSession.builder.master(\"locale[2]\").appName(\"SyntheticDataForBigData\").getOrCreate()\n",
    "\n",
    "# Convert the synthetic DataFrame to a Spark DataFrame\n",
    "spark_df = spark.createDataFrame(synthetic_df)\n",
    "\n",
    "# Show the Spark DataFrame\n",
    "spark_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "**Summary:** Generating synthetic data is a crucial tool for testing algorithms, saving resources, and maintaining privacy. `Python` offers various packages to generate synthetic data, and big data frameworks like `PySpark`  can help handle large datasets efficiently.\n",
    "\n",
    "**Recommendation:** Use synthetic data in development and testing environments to improve efficiency, save costs, and ensure compliance with privacy regulations.\n",
    "\n",
    "# References and Further Reading\n",
    "* Faker Documentation (Python)\n",
    "* PySpark Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
