{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference columns by name: `F.col()`\n",
    "\n",
    "There are several different ways to reference columns in a PySpark DataFrame, e.g. in a `.filter()` operation:\n",
    "- `df.filter(F.col(\"column_name\" == value))`: references column by name; the recommended method, used throughout this book\n",
    "- `df.filter(df.column_name == value)`: references column directly from the DF\n",
    "- `df.flter(df[\"column_name\"] == value)`: pandas style, less commonly used in PySpark\n",
    "\n",
    "The preferred method is using `F.col()` from the `pyspark.sql.functions` module and is used throughout this book. Although all three methods above will work in some circumstances, only `F.col()` will always have the desired outcome. This is because it references the column by *name* rather than directly from the DF, which means columns not yet assigned to the DF can be used, e.g. when chaining several operations on the same DF together.\n",
    "\n",
    "There are several cases where `F.col()` will work but one of the other methods will not:\n",
    "- [Filter the DataFrame when reading in](#filter-the-DataFrame-when-reading-in)\n",
    "- [Filter on a new column](#filter-on-a-new-column)\n",
    "- [Ensuring you are using the latest values](#ensuring-you-are-using-the-latest-values)\n",
    "- [Columns with special characters or spaces](#columns-with-special-characters-or-spaces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter the DataFrame when reading in\n",
    "\n",
    "First, import the modules and create a Spark session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, pyspark.sql.functions as F\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[2]\").appName(\"sampling\").getOrCreate()\n",
    "data_path = f\"file:///{os.getcwd()}/../data/animal_rescue.parquet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can filter on columns when reading in the DataFrame. For instance to only read `\"Cat\"` from the animal rescue data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------------+\n",
      "|IncidentNumber|AnimalGroupParent|\n",
      "+--------------+-----------------+\n",
      "|       5186091|              Cat|\n",
      "|       5724091|              Cat|\n",
      "|       5770091|              Cat|\n",
      "|       5789091|              Cat|\n",
      "|       6317091|              Cat|\n",
      "|       6353091|              Cat|\n",
      "|       6378091|              Cat|\n",
      "|       6530091|              Cat|\n",
      "|       7613091|              Cat|\n",
      "|      10829091|              Cat|\n",
      "+--------------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cats = spark.read.csv(\"/training/animal_rescue.csv\", header=True).filter(F.col(\"AnimalGroupParent\") == \"Cat\")\n",
    "cats.select(\"IncidentNumber\", \"AnimalGroupParent\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can't be done using the `cats.AnimalGroupParent` as we haven't defined `cats` when referencing the DataFrame. To use the other notation we need to define `rescue` then filter on `cats.AnimalGroupParent`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------------+\n",
      "|IncidentNumber|AnimalGroupParent|\n",
      "+--------------+-----------------+\n",
      "|       5186091|              Cat|\n",
      "|       5724091|              Cat|\n",
      "|       5770091|              Cat|\n",
      "|       5789091|              Cat|\n",
      "|       6317091|              Cat|\n",
      "|       6353091|              Cat|\n",
      "|       6378091|              Cat|\n",
      "|       6530091|              Cat|\n",
      "|       7613091|              Cat|\n",
      "|      10829091|              Cat|\n",
      "+--------------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rescue = spark.read.csv(\"/training/animal_rescue.csv\", header=True)\n",
    "cats.filter(rescue.AnimalGroupParent == \"Cat\").select(\"IncidentNumber\", \"AnimalGroupParent\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter on a new column\n",
    "\n",
    "Read in the animal rescue data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rescue = spark.read.csv(\"/training/animal_rescue.csv\", header=True).select(\"IncidentNumber\", \"AnimalGroupParent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a new column, `AnimalGroup`, which consists of the `AnimalGroupParent` in uppercase.\n",
    "\n",
    "If we try and immediately filter on this column using `df.AnimalGroup`, it won't work. This is because we have yet to define the column in `rescue`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'DataFrame' object has no attribute 'AnimalGroup'\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    rescue.withColumn(\"AnimalGroup\", F.upper(df.AnimalGroupParent)).filter(df.AnimalGroup == \"CAT\").show(10)\n",
    "except AttributeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could split this statement up over two different lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------------+-----------+\n",
      "|IncidentNumber|AnimalGroupParent|AnimalGroup|\n",
      "+--------------+-----------------+-----------+\n",
      "|       5186091|              Cat|        CAT|\n",
      "|       5724091|              Cat|        CAT|\n",
      "|       5770091|              Cat|        CAT|\n",
      "|       5789091|              Cat|        CAT|\n",
      "|       6317091|              Cat|        CAT|\n",
      "|       6353091|              Cat|        CAT|\n",
      "|       6378091|              Cat|        CAT|\n",
      "|       6530091|              Cat|        CAT|\n",
      "|       7613091|              Cat|        CAT|\n",
      "|      10829091|              Cat|        CAT|\n",
      "+--------------+-----------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rescue_upper = df.withColumn(\"AnimalGroup\", F.upper(df.AnimalGroupParent))\n",
    "rescue_upper.filter(rescue_upper.AnimalGroup == \"CAT\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a case where we could use `F.col()` instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------------+-----------+\n",
      "|IncidentNumber|AnimalGroupParent|AnimalGroup|\n",
      "+--------------+-----------------+-----------+\n",
      "|       5186091|              Cat|        CAT|\n",
      "|       5724091|              Cat|        CAT|\n",
      "|       5770091|              Cat|        CAT|\n",
      "|       5789091|              Cat|        CAT|\n",
      "|       6317091|              Cat|        CAT|\n",
      "|       6353091|              Cat|        CAT|\n",
      "|       6378091|              Cat|        CAT|\n",
      "|       6530091|              Cat|        CAT|\n",
      "|       7613091|              Cat|        CAT|\n",
      "|      10829091|              Cat|        CAT|\n",
      "+--------------+-----------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rescue.withColumn(\"AnimalGroup\", F.upper(\"AnimalGroupParent\")).filter(F.col(\"AnimalGroup\") == \"CAT\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Ensuring you are using the latest values\n",
    "\n",
    "Using `df.column_name` can also result in bugs when you think you are referencing the latest values, but aren't. Here, the values in `AnimalGroupParent` are changed, but `df` is yet to be redefined, and so the old values are used. As such no data is returned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------------+\n",
      "|IncidentNumber|AnimalGroupParent|\n",
      "+--------------+-----------------+\n",
      "+--------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rescue = spark.read.csv(\"/training/animal_rescue.csv\", header=True).select(\"IncidentNumber\", \"AnimalGroupParent\")\n",
    "rescue.withColumn(\"AnimalGroupParent\", F.upper(rescue.AnimalGroupParent)).filter(rescue.AnimalGroupParent == \"CAT\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing to `F.col(\"AnimalGroupParent\")` gives the correct result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------------+\n",
      "|IncidentNumber|AnimalGroupParent|\n",
      "+--------------+-----------------+\n",
      "|       5186091|              CAT|\n",
      "|       5724091|              CAT|\n",
      "|       5770091|              CAT|\n",
      "|       5789091|              CAT|\n",
      "|       6317091|              CAT|\n",
      "|       6353091|              CAT|\n",
      "|       6378091|              CAT|\n",
      "|       6530091|              CAT|\n",
      "|       7613091|              CAT|\n",
      "|      10829091|              CAT|\n",
      "+--------------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"AnimalGroupParent\", F.upper(\"AnimalGroupParent\")).filter(F.col(\"AnimalGroupParent\") == \"CAT\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4: Columns with special characters or spaces\n",
    "\n",
    "One final use case for this method is when your source data has column names with spaces or special characters in them. The animal rescue data has a column called `IncidentNotionalCost(£)`. You can't refer to the column using `df.IncidentNotionalCost(£)`, instead, use `F.col(\"df.IncidentNotionalCost(£)\")`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------------------+\n",
      "| IncidentNumber|IncidentNotionalCost(£)|\n",
      "+---------------+-----------------------+\n",
      "|       48360131|                 3480.0|\n",
      "|       49076141|                 2655.0|\n",
      "|       62700151|                 2980.0|\n",
      "|098141-28072016|                 3912.0|\n",
      "|092389-09072018|                 2664.0|\n",
      "+---------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rescue = spark.read.csv(\"/training/animal_rescue.csv\", header=True).select(\"IncidentNumber\", \"IncidentNotionalCost(£)\")\n",
    "rescue.filter(F.col(\"IncidentNotionalCost(£)\") > 2500).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the pandas style `df[\"IncidentNotionalCost(£)\"]` but this notation isn't encouraged in PySpark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------------------+\n",
      "| IncidentNumber|IncidentNotionalCost(£)|\n",
      "+---------------+-----------------------+\n",
      "|       48360131|                 3480.0|\n",
      "|       49076141|                 2655.0|\n",
      "|       62700151|                 2980.0|\n",
      "|098141-28072016|                 3912.0|\n",
      "|092389-09072018|                 2664.0|\n",
      "+---------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rescue.filter(rescue[\"IncidentNotionalCost(£)\"] > 2500).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, the best idea is to rename the column something sensible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------+\n",
      "| IncidentNumber|notional_cost|\n",
      "+---------------+-------------+\n",
      "|       48360131|       3480.0|\n",
      "|       49076141|       2655.0|\n",
      "|       62700151|       2980.0|\n",
      "|098141-28072016|       3912.0|\n",
      "|092389-09072018|       2664.0|\n",
      "+---------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rescue = rescue.withColumnRenamed(\"IncidentNotionalCost(£)\", \"notional_cost\")\n",
    "rescue.filter(F.col(\"notional_cost\") > 2500).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Further Resources\n",
    "\n",
    "PySpark Documentation:\n",
    "- [`.sample()`](https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.sample)\n",
    "- [`.coalesce()`](https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#pyspark.sql.functions.coalesce)\n",
    "- [`.randomSplit()`](https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.randomSplit)\n",
    "- [`.sampleBy()`](https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.sampleBy)\n",
    "\n",
    "sparklyr Documentation:\n",
    "- [`sdf_sample()`](https://spark.rstudio.com/reference/sdf_sample.html)\n",
    "- [`sdf_coalesce()`](https://spark.rstudio.com/reference/sdf_coalesce.html)\n",
    "- [`sdf_random_split()`](https://spark.rstudio.com/reference/sdf_random_split.html)\n",
    "- [`sdf_weighted_sample()`](https://spark.rstudio.com/reference/sdf_weighted_sample.html)\n",
    "\n",
    "Spark in ONS material:\n",
    "- Style guide\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
