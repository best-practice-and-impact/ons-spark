{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Returning Data from Cluster to Driver\n",
    "\n",
    "This article explores different ways of moving small amounts of data from a PySpark DataFrame, which is lazily evaluated on the Spark *cluster*, into the *driver*. Remember that the Spark cluster will have more memory than the driver, so be careful about the amount of data that you are returning.\n",
    "\n",
    "There are lots of ways to do this; most users will use [`.show()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.show.html), the [`.limit()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.limit.html)\n",
    "[`.toPandas()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.toPandas.html) combination or use eager evaluation. Several other methods are included for completeness, including the original method, [`.collect()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.collect.html):\n",
    "\n",
    "- [`.show()`](#show)\n",
    "- [`.toPandas()`](#topandas)\n",
    "- [Eager Evaluation](#eager-evaluation)\n",
    "- [`.collect()` and Variations](#collect-and-variations)\n",
    "    - [`.take()`](#take-combine-limit-and-collect)\n",
    "    - [`.first()`](#first-return-one-row)\n",
    "    - [`.head()`](#head-a-confusing-function)\n",
    "\n",
    "### Lazy Evaluation and `.printSchema()`\n",
    "\n",
    "Before looking at each of the methods in turn, it is worth revisiting the concept of lazy evaluation. Spark DataFrames are not evaluated by default. An *action* has to be called in order for the cluster to process the Spark plan. `.show()`, `.toPandas()` and `.collect()` are all examples of *actions*.\n",
    "\n",
    "Note that if you implicitly try and print the DataFrame it will just return the schema, rather than evaluating the DataFrame. First, start a Spark session and load the Animal Rescue data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "\n",
    "import pandas as pd\n",
    "import yaml\n",
    "\n",
    "with open(\"../../config.yaml\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "\n",
    "spark = (SparkSession.builder.master(\"local[2]\")\n",
    "         .appName(\"returning-data\")\n",
    "         .getOrCreate())\n",
    "\n",
    "rescue_path = config[\"rescue_path\"]\n",
    "\n",
    "rescue = spark.read.parquet(rescue_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then type the name of the DataFrame; this will print the column names and types but will not evaluate the DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[incident_number: string, date_time_of_call: string, cal_year: int, fin_year: string, type_of_incident: string, engine_count: double, job_hours: double, hourly_cost: int, total_cost: double, description: string, animal_group: string, origin_of_call: string, property_type: string, property_category: string, special_service_type_category: string, special_service_type: string, ward_code: string, ward: string, borough_code: string, borough: string, stn_ground_name: string, postcode_district: string, easting_m: double, northing_m: double, easting_rounded: int, northing_rounded: int]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rescue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A better way of getting the schema is with [`.printSchema()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.printSchema.html), which prints out in a much more readable manner. This is not an action as the DataFrame does not need to be evaluated to get this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- incident_number: string (nullable = true)\n",
      " |-- date_time_of_call: string (nullable = true)\n",
      " |-- cal_year: integer (nullable = true)\n",
      " |-- fin_year: string (nullable = true)\n",
      " |-- type_of_incident: string (nullable = true)\n",
      " |-- engine_count: double (nullable = true)\n",
      " |-- job_hours: double (nullable = true)\n",
      " |-- hourly_cost: integer (nullable = true)\n",
      " |-- total_cost: double (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- animal_group: string (nullable = true)\n",
      " |-- origin_of_call: string (nullable = true)\n",
      " |-- property_type: string (nullable = true)\n",
      " |-- property_category: string (nullable = true)\n",
      " |-- special_service_type_category: string (nullable = true)\n",
      " |-- special_service_type: string (nullable = true)\n",
      " |-- ward_code: string (nullable = true)\n",
      " |-- ward: string (nullable = true)\n",
      " |-- borough_code: string (nullable = true)\n",
      " |-- borough: string (nullable = true)\n",
      " |-- stn_ground_name: string (nullable = true)\n",
      " |-- postcode_district: string (nullable = true)\n",
      " |-- easting_m: double (nullable = true)\n",
      " |-- northing_m: double (nullable = true)\n",
      " |-- easting_rounded: integer (nullable = true)\n",
      " |-- northing_rounded: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rescue.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `.show()`\n",
    "\n",
    "Using [`.show()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.show.html) is the easiest way to preview a PySpark DataFrame. By default it will print out $20$ rows of the DataFrame.\n",
    "\n",
    "You cannot assign the results of `.show()` to a variable, so it is purely for information.\n",
    "\n",
    "By default Spark DataFrames are not ordered, due to the fact they are distributed on the cluster. This means that calling `.show()` on the same DataFrame several times can return different results.\n",
    "\n",
    "This can look ugly if there are many columns in the DataFrame, so often you will want to use [`.select()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.select.html) to only return the columns you are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+----------+--------------------+\n",
      "|incident_number|        animal_group|total_cost|         description|\n",
      "+---------------+--------------------+----------+--------------------+\n",
      "|       80771131|                 Cat|     290.0|CAT TRAPPED IN BA...|\n",
      "|      141817141|               Horse|     590.0|HORSE TRAPPED IN ...|\n",
      "|143166-22102016|                Bird|     326.0|PIGEON WITH WING ...|\n",
      "|       43051141|                 Cat|     295.0|ASSIST RSPCA WITH...|\n",
      "|        9393131|                 Dog|     260.0|DOG FALLEN INTO T...|\n",
      "|       44345121|                Deer|     520.0|DEER STUCK IN RAI...|\n",
      "|       58835101|                Deer|     260.0|DEER TRAPPED IN F...|\n",
      "|126246-03092018|                 Cat|     333.0|KITTEN TRAPPED BE...|\n",
      "|       98474151|                 Fox|     298.0|ASSIST RSPCA WAS ...|\n",
      "|       17398141|                 Cat|    1160.0|CAT STUCK IN CAR ...|\n",
      "|       26486141|                Bird|     290.0|PEREGRINE FALCON ...|\n",
      "|      144750111|                 Dog|     260.0|DOG IN PRECARIOUS...|\n",
      "|129971-26092017|                 Cat|     328.0|KITTEN TRAPPED UN...|\n",
      "|      113396111|                 Cat|     260.0|ASSIST RSPCA WITH...|\n",
      "|      105429101|               Horse|     260.0|HORSE STUCK IN DITCH|\n",
      "|165278-09122017|Unknown - Domesti...|     328.0|CALLERS SMALL PET...|\n",
      "|      143598091|                 Cat|     260.0|         CAT UP TREE|\n",
      "|       38468151|                Bird|     298.0|BIRD TRAPPED IN C...|\n",
      "|052371-03052016|                 Cat|     326.0|ASSIST RSPCA INSP...|\n",
      "|      156017101|                Deer|     520.0|DEER TRAPPED IN R...|\n",
      "+---------------+--------------------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(rescue\n",
    "    .select(\"incident_number\", \"animal_group\", \"total_cost\", \"description\")\n",
    "    .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.show()` has three arguments. `n` is the number of rows to return (default `20`), `truncate` will truncate long string entries (default `True`) and `vertical` will return one row per line (default `False`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------------------------------------------------------------------------\n",
      " incident_number | 80771131                                                         \n",
      " animal_group    | Cat                                                              \n",
      " total_cost      | 290.0                                                            \n",
      " description     | CAT TRAPPED IN BASEMENT                                          \n",
      "-RECORD 1---------------------------------------------------------------------------\n",
      " incident_number | 141817141                                                        \n",
      " animal_group    | Horse                                                            \n",
      " total_cost      | 590.0                                                            \n",
      " description     | HORSE TRAPPED IN GATE                                            \n",
      "-RECORD 2---------------------------------------------------------------------------\n",
      " incident_number | 143166-22102016                                                  \n",
      " animal_group    | Bird                                                             \n",
      " total_cost      | 326.0                                                            \n",
      " description     | PIGEON WITH WING IMAPLED ON SHARP IMPLEMENT  UNDER A BRIDGE NEAR \n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(rescue\n",
    "    .select(\"incident_number\", \"animal_group\", \"total_cost\", \"description\")\n",
    "    .show(n=3, truncate=False, vertical=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `.toPandas()`\n",
    "\n",
    "[`.toPandas()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.toPandas.html) converts a PySpark DataFrame into a pandas DataFrame. You can then use all the usual methods on a pandas DataFrame and it will have the standard pandas properties of being mutable, with a fixed row order.\n",
    "\n",
    "Be careful with `toPandas()` as it will convert the whole DF, which is an issue when you are dealing with large data on the Spark cluster. Calling [`limit()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.limit.html) before `.toPandas()` is a good way to ensure that you do not overload the driver.\n",
    "\n",
    "You can either print out the results of `toPandas()` immediately, or assign it to a variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[incident_number: string, date_time_of_call: string, cal_year: int, fin_year: string, type_of_incident: string, engine_count: double, job_hours: double, hourly_cost: int, total_cost: double, description: string, animal_group: string, origin_of_call: string, property_type: string, property_category: string, special_service_type_category: string, special_service_type: string, ward_code: string, ward: string, borough_code: string, borough: string, stn_ground_name: string, postcode_district: string, easting_m: double, northing_m: double, easting_rounded: int, northing_rounded: int]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rescue_pandas = (rescue\n",
    "                 .select(\"incident_number\", \"animal_group\", \"total_cost\", \"description\")\n",
    "                 .limit(5)\n",
    "                 .toPandas())\n",
    "rescue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>incident_number</th>\n",
       "      <th>animal_group</th>\n",
       "      <th>total_cost</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>80771131</td>\n",
       "      <td>Cat</td>\n",
       "      <td>290.0</td>\n",
       "      <td>CAT TRAPPED IN BASEMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>141817141</td>\n",
       "      <td>Horse</td>\n",
       "      <td>590.0</td>\n",
       "      <td>HORSE TRAPPED IN GATE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>143166-22102016</td>\n",
       "      <td>Bird</td>\n",
       "      <td>326.0</td>\n",
       "      <td>PIGEON WITH WING IMAPLED ON SHARP IMPLEMENT  U...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>43051141</td>\n",
       "      <td>Cat</td>\n",
       "      <td>295.0</td>\n",
       "      <td>ASSIST RSPCA WITH CAT STUCK ON CHIMNEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9393131</td>\n",
       "      <td>Dog</td>\n",
       "      <td>260.0</td>\n",
       "      <td>DOG FALLEN INTO THE CANAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   incident_number animal_group  total_cost  \\\n",
       "0         80771131          Cat       290.0   \n",
       "1        141817141        Horse       590.0   \n",
       "2  143166-22102016         Bird       326.0   \n",
       "3         43051141          Cat       295.0   \n",
       "4          9393131          Dog       260.0   \n",
       "\n",
       "                                         description  \n",
       "0                            CAT TRAPPED IN BASEMENT  \n",
       "1                              HORSE TRAPPED IN GATE  \n",
       "2  PIGEON WITH WING IMAPLED ON SHARP IMPLEMENT  U...  \n",
       "3             ASSIST RSPCA WITH CAT STUCK ON CHIMNEY  \n",
       "4                          DOG FALLEN INTO THE CANAL  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(rescue\n",
    "    .select(\"incident_number\", \"animal_group\", \"total_cost\", \"description\")\n",
    "    .limit(5)\n",
    "    .toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative to using `.limit()` is to first check the size of the DataFrame. If it is small there is no need to call `.limit()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lizards = (rescue\n",
    "           .select(\"incident_number\", \"animal_group\", \"total_cost\", \"description\")\n",
    "           .filter(F.col(\"animal_group\") == \"Lizard\"))\n",
    "lizards.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>incident_number</th>\n",
       "      <th>animal_group</th>\n",
       "      <th>total_cost</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>117580091</td>\n",
       "      <td>Lizard</td>\n",
       "      <td>260.0</td>\n",
       "      <td>PET LIZARD TRAPPED BEHIND RADIATOR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>74480101</td>\n",
       "      <td>Lizard</td>\n",
       "      <td>260.0</td>\n",
       "      <td>IGUANA TRAPPED BEHIND HEATING PIPES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>070849-04062018</td>\n",
       "      <td>Lizard</td>\n",
       "      <td>333.0</td>\n",
       "      <td>ASSIST RSPCA WITH IGUANA IN TREE (30 FEET HIGH)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   incident_number animal_group  total_cost  \\\n",
       "0        117580091       Lizard       260.0   \n",
       "1         74480101       Lizard       260.0   \n",
       "2  070849-04062018       Lizard       333.0   \n",
       "\n",
       "                                       description  \n",
       "0               PET LIZARD TRAPPED BEHIND RADIATOR  \n",
       "1              IGUANA TRAPPED BEHIND HEATING PIPES  \n",
       "2  ASSIST RSPCA WITH IGUANA IN TREE (30 FEET HIGH)  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lizards.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eager Evaluation\n",
    "\n",
    "By default, Spark does not display the contents of your DataFrame when you type the name of your DataFrame to implicitly print, instead returning the column name and type; we saw the output from printing `rescue` earlier.\n",
    "\n",
    "There is a way to change this behaviour so that typing the name of your DataFrame returns a nicely formatted preview of the results: add `.config(\"spark.sql.repl.eagerEval.enabled\", 'true')` to the Spark session builder. This will make the behaviour of typing the DF similar to `.show()`. The number of rows returned will be limited automatically so there is no need to apply the `.limit()` command to your DataFrame. You can set the number of rows to be returned with `.config(\"spark.sql.repl.eagerEval.maxNumRows\", number_of_rows)`; by default this is `20`. Columns are truncated to `20` characters by default; this can be changed with `spark.sql.repl.eagerEval.truncate`. More information is in the [Spark Configuration](https://spark.apache.org/docs/latest/configuration.html#runtime-sql-configuration) documentation.\n",
    "\n",
    "Before setting this property we need to stop the existing Spark session with [`spark.stop()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.SparkSession.stop.html).\n",
    "\n",
    "If this is something that you find desirable you may want to add these settings to a [`spark-defaults.conf`](../spark-overview/spark-defaults) file, so they are applied automatically when starting a Spark session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>incident_number</th><th>animal_group</th><th>total_cost</th><th>description</th></tr>\n",
       "<tr><td>80771131</td><td>Cat</td><td>290.0</td><td>CAT TRAPPED IN BASEMENT</td></tr>\n",
       "<tr><td>141817141</td><td>Horse</td><td>590.0</td><td>HORSE TRAPPED IN GATE</td></tr>\n",
       "<tr><td>143166-22102016</td><td>Bird</td><td>326.0</td><td>PIGEON WITH WING IMAPLED ON SHARP IMPLEMENT  UNDER A BRIDGE NEAR</td></tr>\n",
       "<tr><td>43051141</td><td>Cat</td><td>295.0</td><td>ASSIST RSPCA WITH CAT STUCK ON CHIMNEY</td></tr>\n",
       "<tr><td>9393131</td><td>Dog</td><td>260.0</td><td>DOG FALLEN INTO THE CANAL</td></tr>\n",
       "<tr><td>44345121</td><td>Deer</td><td>520.0</td><td>DEER STUCK IN RAILINGS</td></tr>\n",
       "<tr><td>58835101</td><td>Deer</td><td>260.0</td><td>DEER TRAPPED IN FENCE</td></tr>\n",
       "<tr><td>126246-03092018</td><td>Cat</td><td>333.0</td><td>KITTEN TRAPPED BEHIND BATH</td></tr>\n",
       "<tr><td>98474151</td><td>Fox</td><td>298.0</td><td>ASSIST RSPCA WAS FOX TRAPPED IN BARBED WIRE</td></tr>\n",
       "<tr><td>17398141</td><td>Cat</td><td>1160.0</td><td>CAT STUCK IN CAR ENGINE</td></tr>\n",
       "</table>\n",
       "only showing top 10 rows\n"
      ],
      "text/plain": [
       "+---------------+------------+----------+----------------------------------------------------------------+\n",
       "|incident_number|animal_group|total_cost|                                                     description|\n",
       "+---------------+------------+----------+----------------------------------------------------------------+\n",
       "|       80771131|         Cat|     290.0|                                         CAT TRAPPED IN BASEMENT|\n",
       "|      141817141|       Horse|     590.0|                                           HORSE TRAPPED IN GATE|\n",
       "|143166-22102016|        Bird|     326.0|PIGEON WITH WING IMAPLED ON SHARP IMPLEMENT  UNDER A BRIDGE NEAR|\n",
       "|       43051141|         Cat|     295.0|                          ASSIST RSPCA WITH CAT STUCK ON CHIMNEY|\n",
       "|        9393131|         Dog|     260.0|                                       DOG FALLEN INTO THE CANAL|\n",
       "|       44345121|        Deer|     520.0|                                          DEER STUCK IN RAILINGS|\n",
       "|       58835101|        Deer|     260.0|                                           DEER TRAPPED IN FENCE|\n",
       "|126246-03092018|         Cat|     333.0|                                      KITTEN TRAPPED BEHIND BATH|\n",
       "|       98474151|         Fox|     298.0|                     ASSIST RSPCA WAS FOX TRAPPED IN BARBED WIRE|\n",
       "|       17398141|         Cat|    1160.0|                                         CAT STUCK IN CAR ENGINE|\n",
       "+---------------+------------+----------+----------------------------------------------------------------+\n",
       "only showing top 10 rows"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.stop()\n",
    "\n",
    "spark = (SparkSession.builder.master(\"local[2]\")\n",
    "         .appName(\"returning-data\")\n",
    "         # Enable eager evaluation of PySpark DFs\n",
    "         .config(\"spark.sql.repl.eagerEval.enabled\", 'true')\n",
    "         # Maximum rows to return from the DF preview\n",
    "         .config(\"spark.sql.repl.eagerEval.maxNumRows\", 10)\n",
    "         # Set number of characters to return per column\n",
    "         .config(\"spark.sql.repl.eagerEval.truncate\", 100)\n",
    "         .getOrCreate())\n",
    "\n",
    "rescue = spark.read.parquet(rescue_path)\n",
    "\n",
    "rescue.select(\"incident_number\", \"animal_group\", \"total_cost\", \"description\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `.collect()` and variations\n",
    "\n",
    "[`.collect()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.collect.html) is the original way of converting Spark DataFrames from the cluster to the driver and can also be used directly on [RDDs (Resilient Distributed Datasets)](../spark-concepts/shuffling.html#a-quick-note-on-rdds) as well as DataFrames. Like with `.toPandas()` this will bring back the whole DataFrame to the driver, so either combine with `.limit()` or verify that the DF is not too large with [`.count()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.count.html) first.\n",
    "\n",
    "Manipulating the output of `.collect()` is quite awkward, as it returns a list of `Row` objects. It is generally better to just use `.toPandas()`, as pandas DataFrames are easy to manipulate and have built in methods to convert to other data structures if desired.\n",
    "\n",
    "The output of `.collect()` can be assigned to variable or printed out directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(incident_number='80771131', animal_group='Cat', total_cost=290.0, description='CAT TRAPPED IN BASEMENT'),\n",
       " Row(incident_number='141817141', animal_group='Horse', total_cost=590.0, description='HORSE TRAPPED IN GATE'),\n",
       " Row(incident_number='143166-22102016', animal_group='Bird', total_cost=326.0, description='PIGEON WITH WING IMAPLED ON SHARP IMPLEMENT  UNDER A BRIDGE NEAR'),\n",
       " Row(incident_number='43051141', animal_group='Cat', total_cost=295.0, description='ASSIST RSPCA WITH CAT STUCK ON CHIMNEY'),\n",
       " Row(incident_number='9393131', animal_group='Dog', total_cost=260.0, description='DOG FALLEN INTO THE CANAL')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rescue_collected = (rescue\n",
    "                    .select(\"incident_number\", \"animal_group\", \"total_cost\", \"description\")\n",
    "                    .limit(5)\n",
    "                    .collect())\n",
    "rescue_collected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(incident_number='80771131', animal_group='Cat', total_cost=290.0, description='CAT TRAPPED IN BASEMENT'),\n",
       " Row(incident_number='141817141', animal_group='Horse', total_cost=590.0, description='HORSE TRAPPED IN GATE'),\n",
       " Row(incident_number='143166-22102016', animal_group='Bird', total_cost=326.0, description='PIGEON WITH WING IMAPLED ON SHARP IMPLEMENT  UNDER A BRIDGE NEAR'),\n",
       " Row(incident_number='43051141', animal_group='Cat', total_cost=295.0, description='ASSIST RSPCA WITH CAT STUCK ON CHIMNEY'),\n",
       " Row(incident_number='9393131', animal_group='Dog', total_cost=260.0, description='DOG FALLEN INTO THE CANAL')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(rescue\n",
    "    .select(\"incident_number\", \"animal_group\", \"total_cost\", \"description\")\n",
    "    .limit(5)\n",
    "    .collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even just collecting one column still returns a list of `Row` objects. You can use an identity [`.flatMap()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.RDD.flatMap.html) to return a normal Python list. Note that `.flatMap()` is an RDD method and so you need to convert the DF to an RDD first with [`.rdd`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['80771131', '141817141', '143166-22102016', '43051141', '9393131']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rescue.select(\"incident_number\").limit(5).rdd.flatMap(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although it is much easier in pandas by converting the one column DataFrame into a series with [`.squeeze()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.squeeze.html) (or by selecting the column name for implicit conversion) then using [`.tolist()`](https://pandas.pydata.org/docs/reference/api/pandas.Series.tolist.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['80771131', '141817141', '143166-22102016', '43051141', '9393131']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rescue.select(\"incident_number\").limit(5).toPandas().squeeze().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['80771131', '141817141', '143166-22102016', '43051141', '9393131']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rescue.select(\"incident_number\").limit(5).toPandas()[\"incident_number\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main use case for `.collect()` is returning a value from a one row DataFrame as a scalar value and then assigning to a variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2012231.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_cost = (rescue\n",
    "            .agg(F.sum(\"total_cost\"))\n",
    "            .collect()[0][0])\n",
    "sum_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.take()`: combine `.limit()` and `.collect()`\n",
    "\n",
    "An alternative to `.collect()` is to use [`.take(num)`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.take.html) which will only return a maximum of `num` rows (or the whole DataFrame if the number of rows is less than `num`). This is equivalent to `.limit(num).collect()`. Remember that using `.collect()` will bring the entire DataFrame into the driver, so if it is large you need to reduce the size first. Using `.take()` removes this problem.\n",
    "\n",
    "`.take()` has no default value, so you must supply `num` (unlike `.show()`, which has a default of `20`).\n",
    "\n",
    "Just like `.collect()` you can assign the output of `.take()` to a variable or print out directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(incident_number='80771131', animal_group='Cat', total_cost=290.0, description='CAT TRAPPED IN BASEMENT'),\n",
       " Row(incident_number='141817141', animal_group='Horse', total_cost=590.0, description='HORSE TRAPPED IN GATE'),\n",
       " Row(incident_number='143166-22102016', animal_group='Bird', total_cost=326.0, description='PIGEON WITH WING IMAPLED ON SHARP IMPLEMENT  UNDER A BRIDGE NEAR'),\n",
       " Row(incident_number='43051141', animal_group='Cat', total_cost=295.0, description='ASSIST RSPCA WITH CAT STUCK ON CHIMNEY'),\n",
       " Row(incident_number='9393131', animal_group='Dog', total_cost=260.0, description='DOG FALLEN INTO THE CANAL')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rescue_take = rescue.select(\"incident_number\", \"animal_group\", \"total_cost\", \"description\").take(5)\n",
    "rescue_take"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.first()`: return one row\n",
    "\n",
    "If you only want one row, use [`.first()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.first.html). The advantage of `.first()` over `.take(1)` or `.limit(1).collect()` is that a single `Row` object is returned, rather than a `Row` object within a list of length 1. This means the code looks neater if you only want to extract one value.\n",
    "\n",
    "As PySpark DataFrames are not ordered by default `.first()` is non-deterministic, and so it is generally combined with `.orderBy()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(incident_number='098141-28072016', animal_group='Cat', total_cost=3912.0, description='CAT STUCK WITHIN WALL SPACE  RSPCA IN ATTENDANCE')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rescue_first = (rescue\n",
    "                .select(\"incident_number\", \"animal_group\", \"total_cost\", \"description\")\n",
    "                .orderBy(F.desc(\"total_cost\"))\n",
    "                .first())\n",
    "rescue_first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `Row` object is not in a list, and so values can be referenced directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3912.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_cost = rescue_first[\"total_cost\"]\n",
    "top_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if the DataFrame is empty this will return `None`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "first_none = (rescue\n",
    "              .filter(F.col(\"animal_group\") == \"Dragon\")\n",
    "              .first())\n",
    "print(first_none)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whereas `.collect()` and `.take()` will return an empty list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collect_empty = (rescue\n",
    "                 .filter(F.col(\"animal_group\") == \"Dragon\")\n",
    "                 .collect())\n",
    "collect_empty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.head()`: a confusing function\n",
    "\n",
    "Another method included for completeness is [`.head()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.head.html). This works in the same way as `.take()` if you specify the number of rows (returns a list of `Row` objects, even if it only has one row), or `.first()` if not (returns a single `Row` object). This could potentially get confusing so it is better to just use `.take()` or `.first()` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(incident_number='80771131', animal_group='Cat', total_cost=290.0, description='CAT TRAPPED IN BASEMENT')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Same as .first()\n",
    "rescue_blank = rescue.select(\"incident_number\", \"animal_group\", \"total_cost\", \"description\").head()\n",
    "\n",
    "# Returns a Row object\n",
    "rescue_blank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(incident_number='80771131', animal_group='Cat', total_cost=290.0, description='CAT TRAPPED IN BASEMENT')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Same as .take(1)\n",
    "rescue_blank = rescue.select(\"incident_number\", \"animal_group\", \"total_cost\", \"description\").head(1)\n",
    "\n",
    "# Returns a list of Row objects (only one item in this case)\n",
    "rescue_blank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Source Code\n",
    "\n",
    "For these functions it is interesting to take a look at the PySpark source code; normally all you need is in the documentation, but here you can see how the methods actually work. For instance, [`df.take(10)` is the same as `df.limit(10).collect()`](https://spark.apache.org/docs/latest/api/python/_modules/pyspark/sql/dataframe.html#DataFrame.take).\n",
    "\n",
    "### Further Resources\n",
    "\n",
    "Spark at the ONS Articles:\n",
    "- [Configuration Hierarchy and `spark-defaults.conf`](../spark-overview/spark-defaults)\n",
    "- [RDDs (Resilient Distributed Datasets)](../spark-concepts/shuffling.html#a-quick-note-on-rdds)\n",
    "\n",
    "PySpark Documentation:\n",
    "- [`.show()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.show.html)\n",
    "- [`.limit()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.limit.html)\n",
    "- [`.toPandas()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.toPandas.html)\n",
    "- [`.collect()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.collect.html)\n",
    "- [`.printSchema()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.printSchema.html)\n",
    "- [`.select()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.select.html)\n",
    "- [Spark Configuration](https://spark.apache.org/docs/latest/configuration.html#runtime-sql-configuration): details of `spark.sql.repl.eagerEval.enabled`, `spark.sql.repl.eagerEval.maxNumRows` and `spark.sql.repl.eagerEval.truncate`\n",
    "- [`spark.stop()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.SparkSession.stop.html)\n",
    "- [`.count()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.count.html) \n",
    "- [`.flatMap()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.RDD.flatMap.html)\n",
    "- [`.rdd`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.html)\n",
    "- [`.take()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.take.html)\n",
    "- [`.first()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.first.html)\n",
    "- [`.head()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.head.html)\n",
    "\n",
    "PySpark Source Code:\n",
    "- [`.take()`](https://spark.apache.org/docs/latest/api/python/_modules/pyspark/sql/dataframe.html#DataFrame.take)\n",
    "- [`.first()`](https://spark.apache.org/docs/latest/api/python/_modules/pyspark/sql/dataframe.html#DataFrame.first)\n",
    "- [`.head()`](https://spark.apache.org/docs/latest/api/python/_modules/pyspark/sql/dataframe.html#DataFrame.head)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ons-spark-env-3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15 (default, Nov 24 2022, 14:38:14) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "9c57779df1dea1b9eb188cf873833e898918bc3cfb3a87a3118d4020e0c6a26f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
