
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Introduction to PySpark &#8212; Spark at the ONS</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Introduction to PySpark: Exercises" href="pyspark-intro-exercises.html" />
    <link rel="prev" title="Example Spark Sessions" href="../spark-overview/example-spark-sessions.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Spark at the ONS</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Introduction
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Spark overview
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../spark-overview/spark-session-guidance.html">
   Spark Sessions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../spark-overview/example-spark-sessions.html">
   Example Spark Sessions
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Introduction to PySpark
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Introduction to PySpark
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="pyspark-intro-exercises.html">
   Introduction to PySpark: Exercises
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="f-col.html">
   Reference columns by name:
   <code class="docutils literal notranslate">
    <span class="pre">
     F.col()
    </span>
   </code>
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Introduction to sparklyr
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../sparklyr-intro/sparklyr_functions.html">
   Using Spark functions in sparklyr
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Spark functions
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../spark-functions/union_dataframes_with_different_columns.html">
   Union two DataFrames with different columns
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../spark-functions/padding.html">
   Add leading zeros with
   <code class="docutils literal notranslate">
    <span class="pre">
     lpad()
    </span>
   </code>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../spark-functions/sampling.html">
   Sampling:
   <code class="docutils literal notranslate">
    <span class="pre">
     .sample()
    </span>
   </code>
   and
   <code class="docutils literal notranslate">
    <span class="pre">
     sdf_sample()
    </span>
   </code>
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Understanding and Optimising Spark
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../spark-concepts/spark_application_and_ui.html">
   Spark Application and UI
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../spark-concepts/shuffling.html">
   Shuffling
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../spark-concepts/join_concepts.html">
   Optimising Joins
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../spark-functions/cross_joins.html">
   Cross Joins
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../spark-concepts/salted_joins.html">
   Salted Joins
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../spark-concepts/partitions.html">
   Managing partitions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../spark-concepts/exercises.html">
   Exercises
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Ancillary Topics
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../ancillary-topics/visualisation.html">
   Spark and Visualisation
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/pyspark-intro/pyspark-intro.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/robertswh/Spark at the ONS"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/robertswh/Spark at the ONS/issues/new?title=Issue%20on%20page%20%2Fpyspark-intro/pyspark-intro.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/robertswh/Spark at the ONS/main?urlpath=tree/docs/pyspark-intro/pyspark-intro.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#pyspark-a-quick-introduction">
   PySpark: a quick introduction
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-pyspark-package">
   The
   <code class="docutils literal notranslate">
    <span class="pre">
     pyspark
    </span>
   </code>
   Package
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#create-a-spark-session-sparksession-builder">
   Create a Spark session:
   <code class="docutils literal notranslate">
    <span class="pre">
     SparkSession.builder
    </span>
   </code>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#reading-data-spark-read-csv">
   Reading data:
   <code class="docutils literal notranslate">
    <span class="pre">
     spark.read.csv()
    </span>
   </code>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#preview-data-printschema">
   Preview data:
   <code class="docutils literal notranslate">
    <span class="pre">
     .printSchema()
    </span>
   </code>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#show-data-show">
   Show data:
   <code class="docutils literal notranslate">
    <span class="pre">
     .show()
    </span>
   </code>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#convert-to-pandas-topandas">
   Convert to pandas:
   <code class="docutils literal notranslate">
    <span class="pre">
     .toPandas()
    </span>
   </code>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#select-columns-select">
   Select columns
   <code class="docutils literal notranslate">
    <span class="pre">
     .select()
    </span>
   </code>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#get-the-row-count-count">
   Get the row count:
   <code class="docutils literal notranslate">
    <span class="pre">
     .count()
    </span>
   </code>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#drop-columns-drop">
   Drop columns:
   <code class="docutils literal notranslate">
    <span class="pre">
     .drop()
    </span>
   </code>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#rename-columns-withcolumnrenamed">
   Rename columns:
   <code class="docutils literal notranslate">
    <span class="pre">
     .withColumnRenamed()
    </span>
   </code>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#filter-rows-filter-and-f-col">
   Filter rows:
   <code class="docutils literal notranslate">
    <span class="pre">
     .filter()
    </span>
   </code>
   and
   <code class="docutils literal notranslate">
    <span class="pre">
     F.col()
    </span>
   </code>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#adding-columns-withcolumn">
   Adding Columns:
   <code class="docutils literal notranslate">
    <span class="pre">
     .withColumn()
    </span>
   </code>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#sorting-orderby">
   Sorting:
   <code class="docutils literal notranslate">
    <span class="pre">
     .orderBy()
    </span>
   </code>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#grouping-and-aggregating-groupby-agg-and-alias">
   Grouping and Aggregating:
   <code class="docutils literal notranslate">
    <span class="pre">
     .groupBy()
    </span>
   </code>
   ,
   <code class="docutils literal notranslate">
    <span class="pre">
     .agg()
    </span>
   </code>
   and
   <code class="docutils literal notranslate">
    <span class="pre">
     .alias()
    </span>
   </code>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#reading-data-from-a-parquet-file-spark-read-parquet">
   Reading data from a Parquet file:
   <code class="docutils literal notranslate">
    <span class="pre">
     spark.read.parquet()
    </span>
   </code>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#joining-data-join">
   Joining Data
   <code class="docutils literal notranslate">
    <span class="pre">
     .join()
    </span>
   </code>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#writing-data-file-choice">
   Writing data: file choice
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#write-to-a-parquet-write-parquet">
   Write to a parquet:
   <code class="docutils literal notranslate">
    <span class="pre">
     .write.parquet()
    </span>
   </code>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#write-to-a-csv-write-csv-and-coalesce">
   Write to a CSV:
   <code class="docutils literal notranslate">
    <span class="pre">
     .write.csv()
    </span>
   </code>
   and
   <code class="docutils literal notranslate">
    <span class="pre">
     .coalesce()
    </span>
   </code>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#removing-files">
   Removing files
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#further-resources">
   Further Resources
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Introduction to PySpark</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#pyspark-a-quick-introduction">
   PySpark: a quick introduction
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-pyspark-package">
   The
   <code class="docutils literal notranslate">
    <span class="pre">
     pyspark
    </span>
   </code>
   Package
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#create-a-spark-session-sparksession-builder">
   Create a Spark session:
   <code class="docutils literal notranslate">
    <span class="pre">
     SparkSession.builder
    </span>
   </code>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#reading-data-spark-read-csv">
   Reading data:
   <code class="docutils literal notranslate">
    <span class="pre">
     spark.read.csv()
    </span>
   </code>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#preview-data-printschema">
   Preview data:
   <code class="docutils literal notranslate">
    <span class="pre">
     .printSchema()
    </span>
   </code>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#show-data-show">
   Show data:
   <code class="docutils literal notranslate">
    <span class="pre">
     .show()
    </span>
   </code>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#convert-to-pandas-topandas">
   Convert to pandas:
   <code class="docutils literal notranslate">
    <span class="pre">
     .toPandas()
    </span>
   </code>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#select-columns-select">
   Select columns
   <code class="docutils literal notranslate">
    <span class="pre">
     .select()
    </span>
   </code>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#get-the-row-count-count">
   Get the row count:
   <code class="docutils literal notranslate">
    <span class="pre">
     .count()
    </span>
   </code>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#drop-columns-drop">
   Drop columns:
   <code class="docutils literal notranslate">
    <span class="pre">
     .drop()
    </span>
   </code>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#rename-columns-withcolumnrenamed">
   Rename columns:
   <code class="docutils literal notranslate">
    <span class="pre">
     .withColumnRenamed()
    </span>
   </code>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#filter-rows-filter-and-f-col">
   Filter rows:
   <code class="docutils literal notranslate">
    <span class="pre">
     .filter()
    </span>
   </code>
   and
   <code class="docutils literal notranslate">
    <span class="pre">
     F.col()
    </span>
   </code>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#adding-columns-withcolumn">
   Adding Columns:
   <code class="docutils literal notranslate">
    <span class="pre">
     .withColumn()
    </span>
   </code>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#sorting-orderby">
   Sorting:
   <code class="docutils literal notranslate">
    <span class="pre">
     .orderBy()
    </span>
   </code>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#grouping-and-aggregating-groupby-agg-and-alias">
   Grouping and Aggregating:
   <code class="docutils literal notranslate">
    <span class="pre">
     .groupBy()
    </span>
   </code>
   ,
   <code class="docutils literal notranslate">
    <span class="pre">
     .agg()
    </span>
   </code>
   and
   <code class="docutils literal notranslate">
    <span class="pre">
     .alias()
    </span>
   </code>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#reading-data-from-a-parquet-file-spark-read-parquet">
   Reading data from a Parquet file:
   <code class="docutils literal notranslate">
    <span class="pre">
     spark.read.parquet()
    </span>
   </code>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#joining-data-join">
   Joining Data
   <code class="docutils literal notranslate">
    <span class="pre">
     .join()
    </span>
   </code>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#writing-data-file-choice">
   Writing data: file choice
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#write-to-a-parquet-write-parquet">
   Write to a parquet:
   <code class="docutils literal notranslate">
    <span class="pre">
     .write.parquet()
    </span>
   </code>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#write-to-a-csv-write-csv-and-coalesce">
   Write to a CSV:
   <code class="docutils literal notranslate">
    <span class="pre">
     .write.csv()
    </span>
   </code>
   and
   <code class="docutils literal notranslate">
    <span class="pre">
     .coalesce()
    </span>
   </code>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#removing-files">
   Removing files
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#further-resources">
   Further Resources
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="section" id="introduction-to-pyspark">
<h1>Introduction to PySpark<a class="headerlink" href="#introduction-to-pyspark" title="Permalink to this headline">¶</a></h1>
<p>This article aims to give hands on experience in working with the DataFrame API in PySpark. You can download this article as a notebook and run the code yourself by clicking on the download button above and selecting <code class="docutils literal notranslate"><span class="pre">.ipynb</span></code>.</p>
<p>We will not aim to cover all the PySpark DataFrame functionality or go into detail of how Spark works, but instead focus on practicality by performing some common operations on an example dataset. There are a handful of exercises that you can complete while reading this article.</p>
<p>Prerequisites for this article are some basic knowledge of Python and pandas. If you are completely new to Python then it is recommended to complete an introductory course first; your organisation may have specific Python training. Other resources include the <a class="reference external" href="https://docs.python.org/3/tutorial/">Python Tutorial</a> and <a class="reference external" href="https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html">10 Minutes to pandas</a>.</p>
<div class="section" id="pyspark-a-quick-introduction">
<h2>PySpark: a quick introduction<a class="headerlink" href="#pyspark-a-quick-introduction" title="Permalink to this headline">¶</a></h2>
<p>Although this article focusses on practical usage to enable you to quickly use PySpark, you do need to understand some basic theory of Spark and distributed computing.</p>
<p>Spark is a powerful tool used to process huge data in an efficient way. We can access Spark in Python with the PySpark package. Spark has DataFrames, consisting of rows and columns, similar to pandas. Many of the operations are also similar, if not identically named: e.g. you can select and add columns, filter rows, group and aggregate.</p>
<p>The key difference between PySpark and pandas is where the DataFrame is processed:</p>
<ul class="simple">
<li><p>pandas DataFrames are processed on the <em>driver</em>; this could be on a local machine using a desktop IDE such as Spyder or PyCharm, or on a server, e.g. in a dedicated Docker container (such as a CDSW session). The amount of data you can process is limited to the driver memory, so pandas is suitable for smaller data.</p></li>
<li><p>PySpark DataFrames are processed on the <em>Spark cluster</em>. This is a big pool of linked machines, called <em>nodes</em>. PySpark DataFrames are distributed into <em>partitions</em>, and are processed in parallel on the nodes in the Spark cluster. You can have much greater memory capacity with Spark and so is suitable for big data.</p></li>
</ul>
<p>The DataFrame is also processed differently:</p>
<ul class="simple">
<li><p>In pandas, the DataFrame changes in memory at each point, e.g. you could create a DataFrame by reading from a CSV file, select some columns, filter the rows, add a column and then write the data out. With each operation, the DataFrame is physically changing in memory. This can be useful for debugging as it is easy to see intermediate outputs.</p></li>
<li><p>In PySpark, DataFrames are <em>lazily evaluated</em>. We give Spark a set of instructions, called <em>transformations</em>, which are only evaluated when necessary, for instance to get a row count or write out data to a file, referred to as an <em>action</em>. In the example above, the plan is triggered once the data are set to write out to a file.</p></li>
</ul>
<p>For more detail on how Spark works, you can refer to the articles in the Understanding and Optimising Spark chapter of this book. <a class="reference external" href="https://pages.databricks.com/rs/094-YMS-629/images/LearningSpark2.0.pdf">Databricks: Learning Spark</a> is another useful resource.</p>
</div>
<div class="section" id="the-pyspark-package">
<h2>The <code class="docutils literal notranslate"><span class="pre">pyspark</span></code> Package<a class="headerlink" href="#the-pyspark-package" title="Permalink to this headline">¶</a></h2>
<p>As with all coding scripts or notebooks the first thing we do is to import the relevant packages. When coding in PySpark there are two particular imports we need.</p>
<p>Firstly we will import the <a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.SparkSession.html"><code class="docutils literal notranslate"><span class="pre">SparkSession</span></code></a> class, which we will use to create a <code class="docutils literal notranslate"><span class="pre">SparkSession</span></code> object for processing data using Spark.</p>
<p>The second is the <a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#functions"><code class="docutils literal notranslate"><span class="pre">pyspark.sql.functions</span></code></a> module, which contain functions that can be applied to Spark DataFrames, or columns within the DataFrames. The standard method is to import the <code class="docutils literal notranslate"><span class="pre">functions</span></code> module with the alias <code class="docutils literal notranslate"><span class="pre">F</span></code>, which means whenever we want to call a function from this module we write <code class="docutils literal notranslate"><span class="pre">F.function_name()</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>
<span class="kn">import</span> <span class="nn">pyspark.sql.functions</span> <span class="k">as</span> <span class="nn">F</span>
</pre></div>
</div>
</div>
</div>
<p>We will also want to import the <a class="reference external" href="https://pandas.pydata.org/"><code class="docutils literal notranslate"><span class="pre">pandas</span></code></a> module as <code class="docutils literal notranslate"><span class="pre">pd</span></code> as it makes viewing data much easier and neater, and <code class="docutils literal notranslate"><span class="pre">yaml</span></code> for reading the config file, which contains the file path of the source data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">yaml</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;../../config.yaml&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">config</span> <span class="o">=</span> <span class="n">yaml</span><span class="o">.</span><span class="n">safe_load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>As <code class="docutils literal notranslate"><span class="pre">pyspark.sql.functions</span></code> is just a Python package, <a class="reference external" href="https://docs.python.org/3/library/functions.html#dir"><code class="docutils literal notranslate"><span class="pre">dir()</span></code></a> to list the functions and <a class="reference external" href="https://docs.python.org/3/library/functions.html#help"><code class="docutils literal notranslate"><span class="pre">help()</span></code></a> both work as normal, although the easiest way is to look at the <a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#functions">documentation</a>. There are a lot of functions in this module and you will be very unlikely to use them all.</p>
</div>
<div class="section" id="create-a-spark-session-sparksession-builder">
<h2>Create a Spark session: <code class="docutils literal notranslate"><span class="pre">SparkSession.builder</span></code><a class="headerlink" href="#create-a-spark-session-sparksession-builder" title="Permalink to this headline">¶</a></h2>
<p>With our <code class="docutils literal notranslate"><span class="pre">SparkSession</span></code> class imported we now want to create a connection to the Spark cluster. We use <a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.SparkSession.html#pyspark.sql.SparkSession.builder"><code class="docutils literal notranslate"><span class="pre">SparkSession.builder</span></code></a> and assign this to <code class="docutils literal notranslate"><span class="pre">spark</span></code>. <code class="docutils literal notranslate"><span class="pre">SparkSession.builder</span></code> has many options; see the guidance on Spark sessions and also Sample Spark Sessions to get an idea of what sized session to use.</p>
<p>For this article, we are using a tiny dataset by Spark standards, and so are using a <em>local</em> session. This also means that you can run this code without having access to a Spark cluster.</p>
<p>Note that only one Spark session can be running at once. If a session already exists then a new one will not created, instead the connection to the existing session will be used, hence the <a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.SparkSession.builder.getOrCreate.html"><code class="docutils literal notranslate"><span class="pre">.getOrCreate()</span></code></a> method.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">spark</span> <span class="o">=</span> <span class="p">(</span><span class="n">SparkSession</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">master</span><span class="p">(</span><span class="s2">&quot;local[2]&quot;</span><span class="p">)</span>
         <span class="o">.</span><span class="n">appName</span><span class="p">(</span><span class="s2">&quot;pyspark-intro&quot;</span><span class="p">)</span>
         <span class="o">.</span><span class="n">getOrCreate</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="reading-data-spark-read-csv">
<h2>Reading data: <code class="docutils literal notranslate"><span class="pre">spark.read.csv()</span></code><a class="headerlink" href="#reading-data-spark-read-csv" title="Permalink to this headline">¶</a></h2>
<p>For this article we will look at some open data on animal rescue incidents from the London Fire Brigade. The data are stored as a CSV, although the parquet file format is the most common when using Spark. The reason for using CSV in this article is because it is a familiar file format and allows you to adapt this code easily for your own sample data.</p>
<p>Often your data will be large and stored using Hadoop, on the Hadoop Distributed File System (HDFS). This example uses a local file, enabling us to get started quickly; see the article on HDFS for more information.</p>
<p>To read in from a CSV file, use <a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrameReader.csv.html"><code class="docutils literal notranslate"><span class="pre">spark.read.csv()</span></code></a>. The file path is stored in the config file as <code class="docutils literal notranslate"><span class="pre">rescue_path_csv</span></code>. Using <code class="docutils literal notranslate"><span class="pre">header=True</span></code> means that the DataFrame will use the column headers from the CSV as the column names. CSV files do not contain information about the data types, so use <code class="docutils literal notranslate"><span class="pre">inferSchema=True</span></code> which makes Spark scan the file to infer the data types.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">rescue_path_csv</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="s2">&quot;rescue_path_csv&quot;</span><span class="p">]</span>
<span class="n">rescue</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">csv</span><span class="p">(</span><span class="n">rescue_path_csv</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">inferSchema</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="preview-data-printschema">
<h2>Preview data: <code class="docutils literal notranslate"><span class="pre">.printSchema()</span></code><a class="headerlink" href="#preview-data-printschema" title="Permalink to this headline">¶</a></h2>
<p>To view the column names and data types use <a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.printSchema.html"><code class="docutils literal notranslate"><span class="pre">.printSchema()</span></code></a>. This will not return any actual data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">rescue</span><span class="o">.</span><span class="n">printSchema</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>root
 |-- IncidentNumber: string (nullable = true)
 |-- DateTimeOfCall: string (nullable = true)
 |-- CalYear: integer (nullable = true)
 |-- FinYear: string (nullable = true)
 |-- TypeOfIncident: string (nullable = true)
 |-- PumpCount: double (nullable = true)
 |-- PumpHoursTotal: double (nullable = true)
 |-- HourlyNotionalCost(£): integer (nullable = true)
 |-- IncidentNotionalCost(£): double (nullable = true)
 |-- FinalDescription: string (nullable = true)
 |-- AnimalGroupParent: string (nullable = true)
 |-- OriginofCall: string (nullable = true)
 |-- PropertyType: string (nullable = true)
 |-- PropertyCategory: string (nullable = true)
 |-- SpecialServiceTypeCategory: string (nullable = true)
 |-- SpecialServiceType: string (nullable = true)
 |-- WardCode: string (nullable = true)
 |-- Ward: string (nullable = true)
 |-- BoroughCode: string (nullable = true)
 |-- Borough: string (nullable = true)
 |-- StnGroundName: string (nullable = true)
 |-- PostcodeDistrict: string (nullable = true)
 |-- Easting_m: double (nullable = true)
 |-- Northing_m: double (nullable = true)
 |-- Easting_rounded: integer (nullable = true)
 |-- Northing_rounded: integer (nullable = true)
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="show-data-show">
<h2>Show data: <code class="docutils literal notranslate"><span class="pre">.show()</span></code><a class="headerlink" href="#show-data-show" title="Permalink to this headline">¶</a></h2>
<p>The <a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.show.html"><code class="docutils literal notranslate"><span class="pre">.show()</span></code></a> function is an action that previews a DataFrame. <code class="docutils literal notranslate"><span class="pre">.show()</span></code> is an <em>action</em>, meaning that all previous <em>transformations</em> will be ran on the Spark cluster; often this will have many <em>transformations</em>, but here we only have one, reading in the data.</p>
<p>By default <code class="docutils literal notranslate"><span class="pre">.show()</span></code> will display 20 rows and will truncate the columns to a fixed width.</p>
<p>When there are many columns the output can be hard to read, e.g. although the output of <code class="docutils literal notranslate"><span class="pre">.show(3)</span></code> looks fine in this article, in the notebook version every row will appear over several lines:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">rescue</span><span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>+--------------+----------------+-------+-------+---------------+---------+--------------+---------------------+-----------------------+--------------------+-----------------+------------------+--------------------+-----------------+--------------------------+--------------------+---------+--------------------+-----------+-------+-------------+----------------+---------+----------+---------------+----------------+
|IncidentNumber|  DateTimeOfCall|CalYear|FinYear| TypeOfIncident|PumpCount|PumpHoursTotal|HourlyNotionalCost(£)|IncidentNotionalCost(£)|    FinalDescription|AnimalGroupParent|      OriginofCall|        PropertyType| PropertyCategory|SpecialServiceTypeCategory|  SpecialServiceType| WardCode|                Ward|BoroughCode|Borough|StnGroundName|PostcodeDistrict|Easting_m|Northing_m|Easting_rounded|Northing_rounded|
+--------------+----------------+-------+-------+---------------+---------+--------------+---------------------+-----------------------+--------------------+-----------------+------------------+--------------------+-----------------+--------------------------+--------------------+---------+--------------------+-----------+-------+-------------+----------------+---------+----------+---------------+----------------+
|        139091|01/01/2009 03:01|   2009|2008/09|Special Service|      1.0|           2.0|                  255|                  510.0|DOG WITH JAW TRAP...|              Dog|Person (land line)|House - single oc...|         Dwelling|      Other animal assi...|Animal assistance...|E05011467|Crystal Palace &amp; ...|  E09000008|Croydon|      Norbury|            SE19|     null|      null|         532350|          170050|
|        275091|01/01/2009 08:51|   2009|2008/09|Special Service|      1.0|           1.0|                  255|                  255.0|ASSIST RSPCA WITH...|              Fox|Person (land line)|            Railings|Outdoor Structure|      Other animal assi...|Animal assistance...|E05000169|            Woodside|  E09000008|Croydon|     Woodside|            SE25| 534785.0|  167546.0|         534750|          167550|
|       2075091|04/01/2009 10:07|   2009|2008/09|Special Service|      1.0|           1.0|                  255|                  255.0|DOG CAUGHT IN DRA...|              Dog|   Person (mobile)|      Pipe or drain |Outdoor Structure|      Animal rescue fro...|Animal rescue fro...|E05000558|  Carshalton Central|  E09000029| Sutton|   Wallington|             SM5| 528041.0|  164923.0|         528050|          164950|
+--------------+----------------+-------+-------+---------------+---------+--------------+---------------------+-----------------------+--------------------+-----------------+------------------+--------------------+-----------------+--------------------------+--------------------+---------+--------------------+-----------+-------+-------------+----------------+---------+----------+---------------+----------------+
only showing top 3 rows
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="convert-to-pandas-topandas">
<h2>Convert to pandas: <code class="docutils literal notranslate"><span class="pre">.toPandas()</span></code><a class="headerlink" href="#convert-to-pandas-topandas" title="Permalink to this headline">¶</a></h2>
<p>The returned results can look pretty ugly with <code class="docutils literal notranslate"><span class="pre">.show()</span></code> when you have a lot of columns, so often the best way to view the data is to convert to a pandas DataFrame. Be careful: the DataFrame is currently on the <em>Spark cluster</em> with lots of memory capacity, whereas pandas DataFrames are stored on the <em>driver</em>, which will have much less. Trying to use <a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.toPandas.html"><code class="docutils literal notranslate"><span class="pre">.toPandas()</span></code></a> and a huge PySpark DF will not work. If converting to a pandas DF just to view the data, use <a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.limit.html"><code class="docutils literal notranslate"><span class="pre">.limit()</span></code></a> to just bring back a small number of rows.</p>
<p><code class="docutils literal notranslate"><span class="pre">.toPandas()</span></code> is an <em>action</em> and will process the whole plan on the Spark cluster. In this example, it will read the CSV file, return three rows, and then convert the result to the driver as a pandas DF.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">rescue</span><span class="o">.</span><span class="n">limit</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">toPandas</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>IncidentNumber</th>
      <th>DateTimeOfCall</th>
      <th>CalYear</th>
      <th>FinYear</th>
      <th>TypeOfIncident</th>
      <th>PumpCount</th>
      <th>PumpHoursTotal</th>
      <th>HourlyNotionalCost(£)</th>
      <th>IncidentNotionalCost(£)</th>
      <th>FinalDescription</th>
      <th>...</th>
      <th>WardCode</th>
      <th>Ward</th>
      <th>BoroughCode</th>
      <th>Borough</th>
      <th>StnGroundName</th>
      <th>PostcodeDistrict</th>
      <th>Easting_m</th>
      <th>Northing_m</th>
      <th>Easting_rounded</th>
      <th>Northing_rounded</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>139091</td>
      <td>01/01/2009 03:01</td>
      <td>2009</td>
      <td>2008/09</td>
      <td>Special Service</td>
      <td>1.0</td>
      <td>2.0</td>
      <td>255</td>
      <td>510.0</td>
      <td>DOG WITH JAW TRAPPED IN MAGAZINE RACK,B15</td>
      <td>...</td>
      <td>E05011467</td>
      <td>Crystal Palace &amp; Upper Norwood</td>
      <td>E09000008</td>
      <td>Croydon</td>
      <td>Norbury</td>
      <td>SE19</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>532350</td>
      <td>170050</td>
    </tr>
    <tr>
      <th>1</th>
      <td>275091</td>
      <td>01/01/2009 08:51</td>
      <td>2009</td>
      <td>2008/09</td>
      <td>Special Service</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>255</td>
      <td>255.0</td>
      <td>ASSIST RSPCA WITH FOX TRAPPED,B15</td>
      <td>...</td>
      <td>E05000169</td>
      <td>Woodside</td>
      <td>E09000008</td>
      <td>Croydon</td>
      <td>Woodside</td>
      <td>SE25</td>
      <td>534785.0</td>
      <td>167546.0</td>
      <td>534750</td>
      <td>167550</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2075091</td>
      <td>04/01/2009 10:07</td>
      <td>2009</td>
      <td>2008/09</td>
      <td>Special Service</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>255</td>
      <td>255.0</td>
      <td>DOG CAUGHT IN DRAIN,B15</td>
      <td>...</td>
      <td>E05000558</td>
      <td>Carshalton Central</td>
      <td>E09000029</td>
      <td>Sutton</td>
      <td>Wallington</td>
      <td>SM5</td>
      <td>528041.0</td>
      <td>164923.0</td>
      <td>528050</td>
      <td>164950</td>
    </tr>
  </tbody>
</table>
<p>3 rows × 26 columns</p>
</div></div></div>
</div>
</div>
<div class="section" id="select-columns-select">
<h2>Select columns <code class="docutils literal notranslate"><span class="pre">.select()</span></code><a class="headerlink" href="#select-columns-select" title="Permalink to this headline">¶</a></h2>
<p>Often your data will have too many columns that are not relevant, so we can use <a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.select.html"><code class="docutils literal notranslate"><span class="pre">.select()</span></code></a> to just get the ones that are of interest. In our example, we can reduce the number of columns returned so that the output of <code class="docutils literal notranslate"><span class="pre">.show()</span></code> is much neater.</p>
<p>Selecting columns is a <em>transformation</em>, and so will only be processed once an <em>action</em> is called. As such we are chaining this with <code class="docutils literal notranslate"><span class="pre">.show()</span></code> to preview the data:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">rescue</span>
    <span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s2">&quot;IncidentNumber&quot;</span><span class="p">,</span> <span class="s2">&quot;DateTimeofCall&quot;</span><span class="p">,</span> <span class="s2">&quot;FinalDescription&quot;</span><span class="p">)</span>
    <span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">truncate</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>+--------------+----------------+-----------------------------------------+
|IncidentNumber|DateTimeofCall  |FinalDescription                         |
+--------------+----------------+-----------------------------------------+
|139091        |01/01/2009 03:01|DOG WITH JAW TRAPPED IN MAGAZINE RACK,B15|
|275091        |01/01/2009 08:51|ASSIST RSPCA WITH FOX TRAPPED,B15        |
|2075091       |04/01/2009 10:07|DOG CAUGHT IN DRAIN,B15                  |
|2872091       |05/01/2009 12:27|HORSE TRAPPED IN LAKE,J17                |
|3553091       |06/01/2009 15:23|RABBIT TRAPPED UNDER SOFA,B15            |
+--------------+----------------+-----------------------------------------+
only showing top 5 rows
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="get-the-row-count-count">
<h2>Get the row count: <code class="docutils literal notranslate"><span class="pre">.count()</span></code><a class="headerlink" href="#get-the-row-count-count" title="Permalink to this headline">¶</a></h2>
<p>Sometimes you data will be small enough that you do not even need to use Spark. As such it is useful to know the row count, and then make the decision on whether to use Spark or just use pandas.</p>
<p>Note that unlike pandas DataFrames the row count is not automatically determined when the data are read in as a PySpark DataFrame. This is an example of <em>lazy evaluation</em>. As such, <a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.count.html"><code class="docutils literal notranslate"><span class="pre">.count()</span></code></a> is an <em>action</em> and has to be explicitly called.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">rescue</span><span class="o">.</span><span class="n">count</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>5898
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="drop-columns-drop">
<h2>Drop columns: <code class="docutils literal notranslate"><span class="pre">.drop()</span></code><a class="headerlink" href="#drop-columns-drop" title="Permalink to this headline">¶</a></h2>
<p><a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.drop.html"><code class="docutils literal notranslate"><span class="pre">.drop()</span></code></a> is the opposite of <code class="docutils literal notranslate"><span class="pre">select()</span></code>; we specify the columns that we want to remove. There are a lot of columns related to the location of the animal rescue incidents that we will not use that can be removed with <code class="docutils literal notranslate"><span class="pre">.drop()</span></code>. You do not need to specify the columns as a list with <code class="docutils literal notranslate"><span class="pre">[]</span></code>, just use a comma separator.</p>
<p>Note that we have written over the our previous DataFrame by re-assiging to <code class="docutils literal notranslate"><span class="pre">rescue</span></code>; unlike pandas DFs, PySpark DFs are <em>immutable</em>.</p>
<p>We then use <code class="docutils literal notranslate"><span class="pre">.printSchema()</span></code> to verify that the columns have been removed.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">rescue</span> <span class="o">=</span> <span class="n">rescue</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span>
    <span class="s2">&quot;WardCode&quot;</span><span class="p">,</span>
    <span class="s2">&quot;BoroughCode&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Easting_m&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Northing_m&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Easting_rounded&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Northing_rounded&quot;</span>
<span class="p">)</span>

<span class="n">rescue</span><span class="o">.</span><span class="n">printSchema</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>root
 |-- IncidentNumber: string (nullable = true)
 |-- DateTimeOfCall: string (nullable = true)
 |-- CalYear: integer (nullable = true)
 |-- FinYear: string (nullable = true)
 |-- TypeOfIncident: string (nullable = true)
 |-- PumpCount: double (nullable = true)
 |-- PumpHoursTotal: double (nullable = true)
 |-- HourlyNotionalCost(£): integer (nullable = true)
 |-- IncidentNotionalCost(£): double (nullable = true)
 |-- FinalDescription: string (nullable = true)
 |-- AnimalGroupParent: string (nullable = true)
 |-- OriginofCall: string (nullable = true)
 |-- PropertyType: string (nullable = true)
 |-- PropertyCategory: string (nullable = true)
 |-- SpecialServiceTypeCategory: string (nullable = true)
 |-- SpecialServiceType: string (nullable = true)
 |-- Ward: string (nullable = true)
 |-- Borough: string (nullable = true)
 |-- StnGroundName: string (nullable = true)
 |-- PostcodeDistrict: string (nullable = true)
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="rename-columns-withcolumnrenamed">
<h2>Rename columns: <code class="docutils literal notranslate"><span class="pre">.withColumnRenamed()</span></code><a class="headerlink" href="#rename-columns-withcolumnrenamed" title="Permalink to this headline">¶</a></h2>
<p>The source data has the column names in <code class="docutils literal notranslate"><span class="pre">CamelCase</span></code>, but when using Python we generally prefer to use <code class="docutils literal notranslate"><span class="pre">snake_case</span></code>. The source data also has columns containing special characters (<code class="docutils literal notranslate"><span class="pre">£</span></code>) which can be problematic when writing out data as it is not compatible will all storage formats.</p>
<p>To rename columns, use <a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.withColumnRenamed.html"><code class="docutils literal notranslate"><span class="pre">.withColumnRenamed()</span></code></a>. This has two arguments: the original column name, followed by the new name. You need a separate <code class="docutils literal notranslate"><span class="pre">.withColumnRenamed()</span></code> statement for each column name but can chain these together. We then use <code class="docutils literal notranslate"><span class="pre">.select()</span></code> to only select these columns. Once again we are re-assigning the DataFrame as it is immutable.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">rescue</span> <span class="o">=</span> <span class="p">(</span><span class="n">rescue</span>
          <span class="o">.</span><span class="n">withColumnRenamed</span><span class="p">(</span><span class="s2">&quot;IncidentNumber&quot;</span><span class="p">,</span> <span class="s2">&quot;incident_number&quot;</span><span class="p">)</span>
          <span class="o">.</span><span class="n">withColumnRenamed</span><span class="p">(</span><span class="s2">&quot;AnimalGroupParent&quot;</span><span class="p">,</span> <span class="s2">&quot;animal_group&quot;</span><span class="p">)</span>
          <span class="o">.</span><span class="n">withColumnRenamed</span><span class="p">(</span><span class="s2">&quot;CalYear&quot;</span><span class="p">,</span> <span class="s2">&quot;cal_year&quot;</span><span class="p">)</span>
          <span class="o">.</span><span class="n">withColumnRenamed</span><span class="p">(</span><span class="s2">&quot;IncidentNotionalCost(£)&quot;</span><span class="p">,</span> <span class="s2">&quot;total_cost&quot;</span><span class="p">)</span>
          <span class="o">.</span><span class="n">withColumnRenamed</span><span class="p">(</span><span class="s2">&quot;PumpHoursTotal&quot;</span><span class="p">,</span> <span class="s2">&quot;job_hours&quot;</span><span class="p">)</span>
          <span class="o">.</span><span class="n">withColumnRenamed</span><span class="p">(</span><span class="s2">&quot;PumpCount&quot;</span><span class="p">,</span> <span class="s2">&quot;engine_count&quot;</span><span class="p">)</span>
          <span class="o">.</span><span class="n">withColumnRenamed</span><span class="p">(</span><span class="s2">&quot;FinalDescription&quot;</span><span class="p">,</span> <span class="s2">&quot;description&quot;</span><span class="p">)</span>
          <span class="o">.</span><span class="n">withColumnRenamed</span><span class="p">(</span><span class="s2">&quot;PostcodeDistrict&quot;</span><span class="p">,</span> <span class="s2">&quot;postcode_district&quot;</span><span class="p">)</span>
          <span class="o">.</span><span class="n">select</span><span class="p">(</span>
              <span class="s2">&quot;incident_number&quot;</span><span class="p">,</span>
              <span class="s2">&quot;animal_group&quot;</span><span class="p">,</span>
              <span class="s2">&quot;cal_year&quot;</span><span class="p">,</span>
              <span class="s2">&quot;total_cost&quot;</span><span class="p">,</span>
              <span class="s2">&quot;job_hours&quot;</span><span class="p">,</span>
              <span class="s2">&quot;engine_count&quot;</span><span class="p">,</span>
              <span class="s2">&quot;description&quot;</span><span class="p">,</span>
              <span class="s2">&quot;postcode_district&quot;</span><span class="p">))</span>

<span class="n">rescue</span><span class="o">.</span><span class="n">printSchema</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>root
 |-- incident_number: string (nullable = true)
 |-- animal_group: string (nullable = true)
 |-- cal_year: integer (nullable = true)
 |-- total_cost: double (nullable = true)
 |-- job_hours: double (nullable = true)
 |-- engine_count: double (nullable = true)
 |-- description: string (nullable = true)
 |-- postcode_district: string (nullable = true)
</pre></div>
</div>
</div>
</div>
<p>Be careful using <code class="docutils literal notranslate"><span class="pre">.withColumnRenamed()</span></code>; if the column is not in the DataFrame then nothing will happen and an error will not be raised until you try and use the new column name.</p>
<p>You can now try Exercise 1.</p>
</div>
<div class="section" id="filter-rows-filter-and-f-col">
<h2>Filter rows: <code class="docutils literal notranslate"><span class="pre">.filter()</span></code> and <code class="docutils literal notranslate"><span class="pre">F.col()</span></code><a class="headerlink" href="#filter-rows-filter-and-f-col" title="Permalink to this headline">¶</a></h2>
<p>Rows of PySpark DataFrames can be filtered with <a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.filter.html"><code class="docutils literal notranslate"><span class="pre">.filter()</span></code></a>, which takes a logical condition. <a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.col.html"><code class="docutils literal notranslate"><span class="pre">F.col()</span></code></a> is used to reference a column in a DataFrame by name and is the most robust method to use.</p>
<p>For instance, if we want to select all the rows where <code class="docutils literal notranslate"><span class="pre">animal_group</span></code> is equal to <code class="docutils literal notranslate"><span class="pre">Hamster</span></code>, we can use <code class="docutils literal notranslate"><span class="pre">F.col(&quot;animal_group&quot;)</span> <span class="pre">==</span> <span class="pre">&quot;Hamster&quot;)</span></code>. Note the double equals sign used in a condition. We do not want to change the <code class="docutils literal notranslate"><span class="pre">rescue</span></code> DataFrame, so assign it to a new DF, <code class="docutils literal notranslate"><span class="pre">hamsters</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">hamsters</span> <span class="o">=</span> <span class="n">rescue</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">col</span><span class="p">(</span><span class="s2">&quot;animal_group&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="s2">&quot;Hamster&quot;</span><span class="p">)</span>
<span class="n">hamsters</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s2">&quot;incident_number&quot;</span><span class="p">,</span> <span class="s2">&quot;animal_group&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>+---------------+------------+
|incident_number|animal_group|
+---------------+------------+
|       37009101|     Hamster|
|       58746101|     Hamster|
|      212716101|     Hamster|
+---------------+------------+
only showing top 3 rows
</pre></div>
</div>
</div>
</div>
<p>Alternatively you can just input a string into <code class="docutils literal notranslate"><span class="pre">.filter()</span></code>, although this can be messy to look at if the condition contains a string:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cats</span> <span class="o">=</span> <span class="n">rescue</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="s2">&quot;animal_group == &#39;Cat&#39;&quot;</span><span class="p">)</span>
<span class="n">cats</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s2">&quot;incident_number&quot;</span><span class="p">,</span> <span class="s2">&quot;animal_group&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>+---------------+------------+
|incident_number|animal_group|
+---------------+------------+
|        5186091|         Cat|
|        5724091|         Cat|
|        5770091|         Cat|
+---------------+------------+
only showing top 3 rows
</pre></div>
</div>
</div>
</div>
<p>Multiple conditions should be in brackets; putting each condition on a new line makes the code easier to read:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">expensive_olympic_dogs</span> <span class="o">=</span> <span class="n">rescue</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span>
    <span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">col</span><span class="p">(</span><span class="s2">&quot;animal_group&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="s2">&quot;Dog&quot;</span><span class="p">)</span> <span class="o">&amp;</span>
    <span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">col</span><span class="p">(</span><span class="s2">&quot;total_cost&quot;</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">750</span><span class="p">)</span> <span class="o">&amp;</span>
    <span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">col</span><span class="p">(</span><span class="s2">&quot;cal_year&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2012</span><span class="p">))</span>

<span class="p">(</span><span class="n">expensive_olympic_dogs</span>
    <span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s2">&quot;incident_number&quot;</span><span class="p">,</span> <span class="s2">&quot;animal_group&quot;</span><span class="p">,</span> <span class="s2">&quot;cal_year&quot;</span><span class="p">,</span> <span class="s2">&quot;total_cost&quot;</span><span class="p">)</span>
    <span class="o">.</span><span class="n">show</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>+---------------+------------+--------+----------+
|incident_number|animal_group|cal_year|total_cost|
+---------------+------------+--------+----------+
|       16209121|         Dog|    2012|     780.0|
|       17531121|         Dog|    2012|     780.0|
|       20818121|         Dog|    2012|     780.0|
|       38636121|         Dog|    2012|     780.0|
|       64764121|         Dog|    2012|    1040.0|
+---------------+------------+--------+----------+
</pre></div>
</div>
</div>
</div>
<p>You can now try Exercise 2.</p>
</div>
<div class="section" id="adding-columns-withcolumn">
<h2>Adding Columns: <code class="docutils literal notranslate"><span class="pre">.withColumn()</span></code><a class="headerlink" href="#adding-columns-withcolumn" title="Permalink to this headline">¶</a></h2>
<p><a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.withColumn.html"><code class="docutils literal notranslate"><span class="pre">.withColumn()</span></code></a> can be used to either create a new column, or overwrite an existing one. The first argument is the new column name, the second is the value of the column. Often this will be derived from other columns. For instance, we do not have a column for how long an incident took in the data, but do have the columns available to derive this:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">job_hours</span></code> gives the total number of hours for engines attending the incident, e.g. if 2 engines attended for an hour <code class="docutils literal notranslate"><span class="pre">job_hours</span></code> will be <code class="docutils literal notranslate"><span class="pre">2</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">engine_count</span></code> gives the number of engines in attendance</p></li>
</ul>
<p>So to get the duration of the incident, which we will call <code class="docutils literal notranslate"><span class="pre">incident_duration</span></code>, we have to divide <code class="docutils literal notranslate"><span class="pre">job_hours</span></code> by <code class="docutils literal notranslate"><span class="pre">engine_count</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">rescue</span> <span class="o">=</span> <span class="n">rescue</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span>
    <span class="s2">&quot;incident_duration&quot;</span><span class="p">,</span> 
    <span class="n">F</span><span class="o">.</span><span class="n">col</span><span class="p">(</span><span class="s2">&quot;job_hours&quot;</span><span class="p">)</span> <span class="o">/</span> <span class="n">F</span><span class="o">.</span><span class="n">col</span><span class="p">(</span><span class="s2">&quot;engine_count&quot;</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Now preview the data with <code class="docutils literal notranslate"><span class="pre">.limit(5).toPandas()</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">rescue</span><span class="o">.</span><span class="n">limit</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">toPandas</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>incident_number</th>
      <th>animal_group</th>
      <th>cal_year</th>
      <th>total_cost</th>
      <th>job_hours</th>
      <th>engine_count</th>
      <th>description</th>
      <th>postcode_district</th>
      <th>incident_duration</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>139091</td>
      <td>Dog</td>
      <td>2009</td>
      <td>510.0</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>DOG WITH JAW TRAPPED IN MAGAZINE RACK,B15</td>
      <td>SE19</td>
      <td>2.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>275091</td>
      <td>Fox</td>
      <td>2009</td>
      <td>255.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>ASSIST RSPCA WITH FOX TRAPPED,B15</td>
      <td>SE25</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2075091</td>
      <td>Dog</td>
      <td>2009</td>
      <td>255.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>DOG CAUGHT IN DRAIN,B15</td>
      <td>SM5</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2872091</td>
      <td>Horse</td>
      <td>2009</td>
      <td>255.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>HORSE TRAPPED IN LAKE,J17</td>
      <td>UB9</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>3553091</td>
      <td>Rabbit</td>
      <td>2009</td>
      <td>255.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>RABBIT TRAPPED UNDER SOFA,B15</td>
      <td>RM3</td>
      <td>1.0</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Note that previewing the data took longer to process than defining the new column. Why? Remember that Spark is built on the concept of <strong>transformations</strong> and <strong>actions</strong>:</p>
<ul class="simple">
<li><p><strong>Transformations</strong> are lazily evaluated expressions. These form the set of instructions called the execution plan.</p></li>
<li><p><strong>Actions</strong> trigger computation to be performed on the cluster and results returned to the driver. It is actions that trigger the execution plan.</p></li>
</ul>
<p>Multiple transformations can be combined, as we did to preprocess the <code class="docutils literal notranslate"><span class="pre">rescue</span></code> DataFrame above. Only when an action is called, for example <code class="docutils literal notranslate"><span class="pre">.toPandas()</span></code>, <code class="docutils literal notranslate"><span class="pre">.show()</span></code> or <code class="docutils literal notranslate"><span class="pre">.count()</span></code>, are these transformations and action executed on the cluster, after which the results are returned to the driver.</p>
</div>
<div class="section" id="sorting-orderby">
<h2>Sorting: <code class="docutils literal notranslate"><span class="pre">.orderBy()</span></code><a class="headerlink" href="#sorting-orderby" title="Permalink to this headline">¶</a></h2>
<p>An important Spark concept is that DataFrames are not ordered by default, unlike a pandas DF which has an index. Remember that a Spark DataFrame is distributed into partitions, and there is no guarantee of the order of the rows within these partitions, or which partition a particular row is on.</p>
<p>To sort the data, use <a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.orderBy.html"><code class="docutils literal notranslate"><span class="pre">.orderBy()</span></code></a>. By default this will sort ascending; to sort descending, use <code class="docutils literal notranslate"><span class="pre">ascending=False</span></code>. To show the highest cost incidents:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">rescue</span>
    <span class="o">.</span><span class="n">orderBy</span><span class="p">(</span><span class="s2">&quot;total_cost&quot;</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s2">&quot;incident_number&quot;</span><span class="p">,</span> <span class="s2">&quot;total_cost&quot;</span><span class="p">,</span> <span class="s2">&quot;animal_group&quot;</span><span class="p">)</span>
    <span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>+---------------+----------+--------------------+
|incident_number|total_cost|        animal_group|
+---------------+----------+--------------------+
|098141-28072016|    3912.0|                 Cat|
|       48360131|    3480.0|               Horse|
|       62700151|    2980.0|               Horse|
|092389-09072018|    2664.0|               Horse|
|       49076141|    2655.0|                 Cat|
|       49189111|    2340.0|               Horse|
|       82423111|    2340.0|               Horse|
|      101755111|    2340.0|                Deer|
|030477-09032018|    2296.0|Unknown - Wild An...|
|028258-08032017|    2282.0|                 Cat|
+---------------+----------+--------------------+
only showing top 10 rows
</pre></div>
</div>
</div>
</div>
<p>Horses make up a lot of the more expensive calls, which makes sense, given that they are large animals.</p>
<p>There are actually multiple ways to sort data in PySpark; as well as <code class="docutils literal notranslate"><span class="pre">.orderBy()</span></code>, you can use <a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.sort.html"><code class="docutils literal notranslate"><span class="pre">.sort()</span></code></a> or even an SQL expression. The same is true of sorting the data descending, where <a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.desc.html"><code class="docutils literal notranslate"><span class="pre">F.desc(column_name)</span></code></a> can be used instead of <code class="docutils literal notranslate"><span class="pre">ascending=False</span></code>. The most important principle here is consistency; try and use the same syntax as your colleagues to make the code easier to read.</p>
<p>Note that sorting the DataFrame is an expensive operation, as the rows move between partitions. This is a key Spark concept called a <em>shuffle</em>. When you are ready to optimise your Spark code you will want to read the article on Shuffling.</p>
<p>You can now try Exercise 3.</p>
</div>
<div class="section" id="grouping-and-aggregating-groupby-agg-and-alias">
<h2>Grouping and Aggregating: <code class="docutils literal notranslate"><span class="pre">.groupBy()</span></code>, <code class="docutils literal notranslate"><span class="pre">.agg()</span></code> and <code class="docutils literal notranslate"><span class="pre">.alias()</span></code><a class="headerlink" href="#grouping-and-aggregating-groupby-agg-and-alias" title="Permalink to this headline">¶</a></h2>
<p>In most cases, we want to get insights into the raw data, for instance, by taking the sum or average of a column, or getting the largest or smallest values. This is key to what the Office for National Statistics does: we release statistics!</p>
<p>In Spark, grouping and aggregating is similar to pandas or SQL: we first group the data with <a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.groupBy.html"><code class="docutils literal notranslate"><span class="pre">.groupBy()</span></code></a>, then aggregate it in some way with a function from the <code class="docutils literal notranslate"><span class="pre">functions</span></code> module inside <a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.agg.html"><code class="docutils literal notranslate"><span class="pre">.agg()</span></code></a>, e.g. <a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.sum.html"><code class="docutils literal notranslate"><span class="pre">F.sum()</span></code></a>, <a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.max.html"><code class="docutils literal notranslate"><span class="pre">F.max()</span></code></a>. For instance, to find the average cost by <code class="docutils literal notranslate"><span class="pre">animal_group</span></code> we use <a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.mean.html"><code class="docutils literal notranslate"><span class="pre">F.mean()</span></code></a>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cost_by_animal</span> <span class="o">=</span> <span class="p">(</span><span class="n">rescue</span>
                  <span class="o">.</span><span class="n">groupBy</span><span class="p">(</span><span class="s2">&quot;animal_group&quot;</span><span class="p">)</span>
                  <span class="o">.</span><span class="n">agg</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="s2">&quot;total_cost&quot;</span><span class="p">)))</span>
<span class="n">cost_by_animal</span><span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>+--------------------+------------------+
|        animal_group|   avg(total_cost)|
+--------------------+------------------+
|Unknown - Animal ...| 709.6666666666666|
|                 Cow| 624.1666666666666|
|               Horse| 747.4350649350649|
|             Hamster|311.07142857142856|
|Unknown - Heavy L...|362.54545454545456|
+--------------------+------------------+
only showing top 5 rows
</pre></div>
</div>
</div>
</div>
<p>The new column has been returned as <code class="docutils literal notranslate"><span class="pre">avg(total_cost)</span></code>. We want to give it a more sensible name and avoid using brackets. We saw earlier that we could use <code class="docutils literal notranslate"><span class="pre">.withColumnRenamed()</span></code> once the column has been created, but it is easier to use <a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.Column.alias.html"><code class="docutils literal notranslate"><span class="pre">.alias()</span></code></a> to rename the column directly in the aggregation:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cost_by_animal</span> <span class="o">=</span> <span class="p">(</span><span class="n">rescue</span>
                  <span class="o">.</span><span class="n">groupBy</span><span class="p">(</span><span class="s2">&quot;animal_group&quot;</span><span class="p">)</span>
                  <span class="o">.</span><span class="n">agg</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="s2">&quot;total_cost&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&quot;average_cost&quot;</span><span class="p">)))</span>

<span class="n">cost_by_animal</span><span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>+--------------------+------------------+
|        animal_group|      average_cost|
+--------------------+------------------+
|Unknown - Animal ...| 709.6666666666666|
|                 Cow| 624.1666666666666|
|               Horse| 747.4350649350649|
|             Hamster|311.07142857142856|
|Unknown - Heavy L...|362.54545454545456|
+--------------------+------------------+
only showing top 5 rows
</pre></div>
</div>
</div>
</div>
<p>Remember that Spark DFs are not ordered unless we specifically do so with <code class="docutils literal notranslate"><span class="pre">.orderBy()</span></code>; now we have renamed the column <code class="docutils literal notranslate"><span class="pre">average_cost</span></code> this is easy to do:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cost_by_animal</span><span class="o">.</span><span class="n">orderBy</span><span class="p">(</span><span class="s2">&quot;average_cost&quot;</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>+--------------------+------------------+
|        animal_group|      average_cost|
+--------------------+------------------+
|                Goat|            1180.0|
|                Fish|             780.0|
|                Bull|             780.0|
|               Horse| 747.4350649350649|
|Unknown - Animal ...| 709.6666666666666|
|                 Cow| 624.1666666666666|
|            Hedgehog|             520.0|
|                Lamb|             520.0|
|                Deer| 423.8829787234043|
|Unknown - Wild An...|390.03636363636366|
+--------------------+------------------+
only showing top 10 rows
</pre></div>
</div>
</div>
</div>
<p>It looks like <code class="docutils literal notranslate"><span class="pre">Goat</span></code> could be an outlier as it is significantly higher than the other higher average cost incidents. We can investigate this in more detail using <code class="docutils literal notranslate"><span class="pre">.filter()</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">goats</span> <span class="o">=</span> <span class="n">rescue</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">col</span><span class="p">(</span><span class="s2">&quot;animal_group&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="s2">&quot;Goat&quot;</span><span class="p">)</span>
<span class="n">goats</span><span class="o">.</span><span class="n">count</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1
</pre></div>
</div>
</div>
</div>
<p>Just one expensive goat incident! Lets see the description:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">goats</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s2">&quot;incident_number&quot;</span><span class="p">,</span> <span class="s2">&quot;animal_group&quot;</span><span class="p">,</span> <span class="s2">&quot;description&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">toPandas</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>incident_number</th>
      <th>animal_group</th>
      <th>description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>72214141</td>
      <td>Goat</td>
      <td>GOAT TRAPPED BELOW GROUND LEVEL ON LEDGE</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Note that we did not use <code class="docutils literal notranslate"><span class="pre">.limit()</span></code> before <code class="docutils literal notranslate"><span class="pre">.toPandas()</span></code> here. This is because we know the row count is tiny, and so there was no danger of overloading the driver with too much data.</p>
</div>
<div class="section" id="reading-data-from-a-parquet-file-spark-read-parquet">
<h2>Reading data from a Parquet file: <code class="docutils literal notranslate"><span class="pre">spark.read.parquet()</span></code><a class="headerlink" href="#reading-data-from-a-parquet-file-spark-read-parquet" title="Permalink to this headline">¶</a></h2>
<p>The next section covers how to join data in Spark, but before we do, we need to read in another dataset. In our rescue data, we have a column for the postcode district, which represents the first part of the postcode. We have data for the population by postcode in another dataset, <code class="docutils literal notranslate"><span class="pre">population</span></code>.</p>
<p>This data are stored as a parquet file. Parquet files the most efficient way to store data when using Spark. They are compressed and so take up much less storage space, and reading parquet files with Spark is many times quicker than reading CSVs. The drawback is that they are not human readable, although you can store them as a Hive table which means they can easily be interrogated with SQL. See the article on Parquet files for more information.</p>
<p>The syntax for reading in a parquet file is similar to a CSV: <a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrameReader.parquet.html"><code class="docutils literal notranslate"><span class="pre">spark.read.parquet()</span></code></a>. There is no need for the <code class="docutils literal notranslate"><span class="pre">header</span></code> or <code class="docutils literal notranslate"><span class="pre">inferSchema</span></code> argument as unlike CSVs parquet files already have the schema defined. We can then preview the data with <code class="docutils literal notranslate"><span class="pre">.limit(5).toPandas()</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">population_path</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="s2">&quot;population_path&quot;</span><span class="p">]</span>
<span class="n">population</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="n">population_path</span><span class="p">)</span>
<span class="n">population</span><span class="o">.</span><span class="n">limit</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">toPandas</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>postcode_district</th>
      <th>population</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>DH7</td>
      <td>41076</td>
    </tr>
    <tr>
      <th>1</th>
      <td>NW3</td>
      <td>52376</td>
    </tr>
    <tr>
      <th>2</th>
      <td>NR4</td>
      <td>22331</td>
    </tr>
    <tr>
      <th>3</th>
      <td>SO31</td>
      <td>44742</td>
    </tr>
    <tr>
      <th>4</th>
      <td>CT18</td>
      <td>14357</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</div>
<div class="section" id="joining-data-join">
<h2>Joining Data <code class="docutils literal notranslate"><span class="pre">.join()</span></code><a class="headerlink" href="#joining-data-join" title="Permalink to this headline">¶</a></h2>
<p>Now we have read the population data in, we can join it to the rescue data to get the population by postcode. This can be done with the <a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.join.html"><code class="docutils literal notranslate"><span class="pre">.join()</span></code></a> method.</p>
<p>This article assumes that you are familiar with joins. Those who know SQL will be familiar with this term, although pandas and R users sometimes use the term <em>merge</em>. If you do not know how a join works, please read about <a class="reference external" href="https://en.wikipedia.org/wiki/Join_(SQL)">joins in SQL</a> first; the principles are the same in Spark. Joins are expensive in Spark as they involve shuffling the data and this can make larger joins slow. See the article on Optimising Joins for more information on how to make them more efficient.</p>
<p><code class="docutils literal notranslate"><span class="pre">.join()</span></code> is a DataFrame method, so we start with the <code class="docutils literal notranslate"><span class="pre">rescue</span></code> DataFrame. The other arguments we need are:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">other</span></code>: the DataFrame on the right hand side of the join, <code class="docutils literal notranslate"><span class="pre">population</span></code>;</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">on</span></code>: which specifies the mapping. Here we have a common column name and so can simply supply the column name;</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">how</span></code>: the type of join to use, <code class="docutils literal notranslate"><span class="pre">&quot;left&quot;</span></code> in this case.</p></li>
</ul>
<p>To make the code easier to read we have put these arguments on new lines:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">rescue_with_pop</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">rescue</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
        <span class="n">population</span><span class="p">,</span>
        <span class="n">on</span><span class="o">=</span><span class="s2">&quot;postcode_district&quot;</span><span class="p">,</span>
        <span class="n">how</span><span class="o">=</span><span class="s2">&quot;left&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>Once again, note how quick this code runs. This is because although a join is an expensive operation, we have only created the plan at this point. We need an action to run the plan and return a result; sort the joined DataFrame, subset the columns and then use <code class="docutils literal notranslate"><span class="pre">.limit(5).toPandas()</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">rescue_with_pop</span> <span class="o">=</span> <span class="p">(</span><span class="n">rescue_with_pop</span>
                   <span class="o">.</span><span class="n">orderBy</span><span class="p">(</span><span class="s2">&quot;incident_number&quot;</span><span class="p">)</span>
                   <span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s2">&quot;incident_number&quot;</span><span class="p">,</span> <span class="s2">&quot;animal_group&quot;</span><span class="p">,</span> <span class="s2">&quot;postcode_district&quot;</span><span class="p">,</span> <span class="s2">&quot;population&quot;</span><span class="p">))</span>

<span class="n">rescue_with_pop</span><span class="o">.</span><span class="n">limit</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">toPandas</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>incident_number</th>
      <th>animal_group</th>
      <th>postcode_district</th>
      <th>population</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>000014-03092018M</td>
      <td>Unknown - Heavy Livestock Animal</td>
      <td>CR8</td>
      <td>32307</td>
    </tr>
    <tr>
      <th>1</th>
      <td>000099-01012017</td>
      <td>Dog</td>
      <td>BR2</td>
      <td>44958</td>
    </tr>
    <tr>
      <th>2</th>
      <td>000260-01012017</td>
      <td>Bird</td>
      <td>CR0</td>
      <td>153812</td>
    </tr>
    <tr>
      <th>3</th>
      <td>000375-01012017</td>
      <td>Dog</td>
      <td>TW8</td>
      <td>20330</td>
    </tr>
    <tr>
      <th>4</th>
      <td>000477-01012017</td>
      <td>Deer</td>
      <td>HA7</td>
      <td>36046</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</div>
<div class="section" id="writing-data-file-choice">
<h2>Writing data: file choice<a class="headerlink" href="#writing-data-file-choice" title="Permalink to this headline">¶</a></h2>
<p>In this article so far, we have been calling actions to preview the data, bringing back only a handful of rows each time. This is useful when developing and debugging code, but in production pipelines you will want to write the results.</p>
<p>The format in which the results are written out depends on what you want to do next with the data:</p>
<ul class="simple">
<li><p>If the data are intended to be human readable, e.g. as the basis for a presentation, or as a publication on the ONS website, then you will likely want to output the data as a CSV</p></li>
<li><p>If the data are intended to be used as an input to another Spark process, then use parquet or a Hive table.</p></li>
</ul>
<p>There are other use cases, e.g. JSON can be useful if you want the results to analysed with a different programming language, although here we only focus on CSV and parquet. See the article on Output File Choices for more information.</p>
</div>
<div class="section" id="write-to-a-parquet-write-parquet">
<h2>Write to a parquet: <code class="docutils literal notranslate"><span class="pre">.write.parquet()</span></code><a class="headerlink" href="#write-to-a-parquet-write-parquet" title="Permalink to this headline">¶</a></h2>
<p>To write out our DataFrame as a parquet file, use <a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrameWriter.parquet.html"><code class="docutils literal notranslate"><span class="pre">rescue_with_pop.write.parquet()</span></code></a>, using the file path as the argument.</p>
<p>The key difference between writing out data with Spark and writing out data with pandas is that the data will be distributed, which means that multiple files will be created, stored in a parent folder. Spark can read in these parent folders as one DataFrame. There will be one file written out per partition of the DataFrame.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">output_path_parquet</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="s2">&quot;rescue_with_pop_path_parquet&quot;</span><span class="p">]</span>
<span class="n">rescue_with_pop</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="n">output_path_parquet</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>It is worth looking at the raw data that is written out to see that it has been stored in several files in a parent folder.</p>
<p>When reading the data in, Spark will treat every individual file as a partition. See the article on Partitions for more information.</p>
</div>
<div class="section" id="write-to-a-csv-write-csv-and-coalesce">
<h2>Write to a CSV: <code class="docutils literal notranslate"><span class="pre">.write.csv()</span></code> and <code class="docutils literal notranslate"><span class="pre">.coalesce()</span></code><a class="headerlink" href="#write-to-a-csv-write-csv-and-coalesce" title="Permalink to this headline">¶</a></h2>
<p>CSVs will also be written out in a distributed manner as multiple files. While this is desirable in a parquet, it is not very useful with CSV, as the main benefit is to make them human readable. First, write out the data with <a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrameWriter.csv.html"><code class="docutils literal notranslate"><span class="pre">.write.csv()</span></code></a>, using the path defined in the config:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">output_path_csv</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="s2">&quot;rescue_with_pop_path_csv&quot;</span><span class="p">]</span>
<span class="n">rescue_with_pop</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">csv</span><span class="p">(</span><span class="n">output_path_csv</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Again, look at the raw data in a file browser. You can see that it has written out a folder called <code class="docutils literal notranslate"><span class="pre">rescue_with_pop.csv</span></code>, with multiple files inside. Each of these on their own is a legitimate CSV file, with the correct headers.</p>
<p>To reduce the number of partitions, use <a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.coalesce.html"><code class="docutils literal notranslate"><span class="pre">.coalesce(numPartitions)</span></code></a>; this will combine existing partitions. Setting <code class="docutils literal notranslate"><span class="pre">numPartitions</span></code> to <code class="docutils literal notranslate"><span class="pre">1</span></code> will put all of the data on the same partition.</p>
<p>As the file will already exist, we need to tell Spark to overwrite the existing file. Use <a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrameWriter.mode.html"><code class="docutils literal notranslate"><span class="pre">.mode(&quot;overwrite&quot;)</span></code></a> after <code class="docutils literal notranslate"><span class="pre">.write</span></code> to do this:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">rescue_with_pop</span>
    <span class="o">.</span><span class="n">coalesce</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="o">.</span><span class="n">write</span>
    <span class="o">.</span><span class="n">mode</span><span class="p">(</span><span class="s2">&quot;overwrite&quot;</span><span class="p">)</span>
    <span class="o">.</span><span class="n">csv</span><span class="p">(</span><span class="n">output_path_csv</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>Checking the file again, you can see that although the folder still exists, it will contain only one CSV file.</p>
<p>A neater way of writing out CSV files to a Hadoop file system is with Pydoop, allowing you to convert the Spark DataFrame to pandas first before making use of <a class="reference external" href="https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html"><code class="docutils literal notranslate"><span class="pre">.to_csv()</span></code></a> from pandas, which has many options to control the output format.</p>
</div>
<div class="section" id="removing-files">
<h2>Removing files<a class="headerlink" href="#removing-files" title="Permalink to this headline">¶</a></h2>
<p>Spark has no native way of removing files, so either use the standard Python methods, or delete them manually through a file browser. If on a local file system, use <a class="reference external" href="https://docs.python.org/3/library/os.html#os.remove"><code class="docutils literal notranslate"><span class="pre">os.remove()</span></code></a> to delete a file and <a class="reference external" href="https://docs.python.org/3/library/shutil.html#shutil.rmtree"><code class="docutils literal notranslate"><span class="pre">shutil.rmtree()</span></code></a> to remove a directory. If using HDFS or similar, then use <a class="reference external" href="https://docs.python.org/3/library/subprocess.html#subprocess.run"><code class="docutils literal notranslate"><span class="pre">subprocess.run()</span></code></a>. Be careful when using the <code class="docutils literal notranslate"><span class="pre">subprocess</span></code> module as you will not get a warning when deleting files.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">subprocess</span>
<span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="p">[</span><span class="n">output_path_parquet</span><span class="p">,</span> <span class="n">output_path_csv</span><span class="p">]:</span>
    <span class="n">cmd</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;hdfs dfs -rm -r -skipTrash </span><span class="si">{</span><span class="n">f</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="n">subprocess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">cmd</span><span class="p">,</span> <span class="n">shell</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="further-resources">
<h2>Further Resources<a class="headerlink" href="#further-resources" title="Permalink to this headline">¶</a></h2>
<p>PySpark Documentation:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.SparkSession.html"><code class="docutils literal notranslate"><span class="pre">SparkSession</span></code></a></p></li>
<li><p><a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#functions"><code class="docutils literal notranslate"><span class="pre">pyspark.sql.functions</span></code></a></p></li>
<li><p><a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.SparkSession.html#pyspark.sql.SparkSession.builder"><code class="docutils literal notranslate"><span class="pre">SparkSession.builder</span></code></a></p></li>
<li><p><a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.SparkSession.builder.getOrCreate.html"><code class="docutils literal notranslate"><span class="pre">.getOrCreate()</span></code></a></p></li>
<li><p><a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrameReader.csv.html"><code class="docutils literal notranslate"><span class="pre">spark.read.csv()</span></code></a></p></li>
<li><p><a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.printSchema.html"><code class="docutils literal notranslate"><span class="pre">.printSchema()</span></code></a></p></li>
<li><p><a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.show.html"><code class="docutils literal notranslate"><span class="pre">.show()</span></code></a></p></li>
<li><p><a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.toPandas.html"><code class="docutils literal notranslate"><span class="pre">.toPandas()</span></code></a></p></li>
<li><p><a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.limit.html"><code class="docutils literal notranslate"><span class="pre">.limit()</span></code></a></p></li>
<li><p><a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.select.html"><code class="docutils literal notranslate"><span class="pre">.select()</span></code></a></p></li>
<li><p><a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.count.html"><code class="docutils literal notranslate"><span class="pre">.count()</span></code></a></p></li>
<li><p><a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.drop.html"><code class="docutils literal notranslate"><span class="pre">.drop()</span></code></a></p></li>
<li><p><a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.withColumnRenamed.html"><code class="docutils literal notranslate"><span class="pre">.withColumnRenamed()</span></code></a></p></li>
<li><p><a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.filter.html"><code class="docutils literal notranslate"><span class="pre">.filter()</span></code></a></p></li>
<li><p><a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.col.html"><code class="docutils literal notranslate"><span class="pre">F.col()</span></code></a></p></li>
<li><p><a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.withColumn.html"><code class="docutils literal notranslate"><span class="pre">.withColumn()</span></code></a></p></li>
<li><p><a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.orderBy.html"><code class="docutils literal notranslate"><span class="pre">.orderBy()</span></code></a></p></li>
<li><p><a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.sort.html"><code class="docutils literal notranslate"><span class="pre">.sort()</span></code></a></p></li>
<li><p><a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.desc.html"><code class="docutils literal notranslate"><span class="pre">F.desc()</span></code></a></p></li>
<li><p><a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.groupBy.html"><code class="docutils literal notranslate"><span class="pre">.groupBy()</span></code></a></p></li>
<li><p><a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.agg.html"><code class="docutils literal notranslate"><span class="pre">.agg()</span></code></a></p></li>
<li><p><a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.Column.alias.html"><code class="docutils literal notranslate"><span class="pre">.alias()</span></code></a></p></li>
<li><p><a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.sum.html"><code class="docutils literal notranslate"><span class="pre">F.sum()</span></code></a></p></li>
<li><p><a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.max.html"><code class="docutils literal notranslate"><span class="pre">F.max()</span></code></a></p></li>
<li><p><a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.mean.html"><code class="docutils literal notranslate"><span class="pre">F.mean()</span></code></a></p></li>
<li><p><a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrameReader.parquet.html"><code class="docutils literal notranslate"><span class="pre">spark.read.parquet()</span></code></a></p></li>
<li><p><a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.join.html"><code class="docutils literal notranslate"><span class="pre">.join()</span></code></a></p></li>
<li><p><a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrameWriter.parquet.html"><code class="docutils literal notranslate"><span class="pre">.write.parquet()</span></code></a></p></li>
<li><p><a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrameWriter.csv.html"><code class="docutils literal notranslate"><span class="pre">.write.csv()</span></code></a></p></li>
<li><p><a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.coalesce.html"><code class="docutils literal notranslate"><span class="pre">.coalesce()</span></code></a></p></li>
<li><p><a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrameWriter.mode.html"><code class="docutils literal notranslate"><span class="pre">.mode()</span></code></a></p></li>
</ul>
<p>Spark in ONS material:</p>
<ul class="simple">
<li><p>Introduction to PySpark: Exercises</p></li>
<li><p>Understanding and Optimising Spark chapter</p></li>
<li><p>Guidance on Spark sessions</p></li>
<li><p>Sample Spark Sessions</p></li>
<li><p>Introduction to PySpark</p></li>
<li><p>F.col()</p></li>
<li><p>Parquet files</p></li>
<li><p>Optimising Joins</p></li>
<li><p>Output File Choices</p></li>
<li><p>Writing Data</p></li>
<li><p>Pydoop</p></li>
</ul>
<p>Other links:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://docs.python.org">Python documentation</a></p>
<ul>
<li><p><a class="reference external" href="https://docs.python.org/3/tutorial/">Python Tutorial</a></p></li>
<li><p><a class="reference external" href="https://docs.python.org/3/library/functions.html#dir"><code class="docutils literal notranslate"><span class="pre">dir()</span></code></a></p></li>
<li><p><a class="reference external" href="https://docs.python.org/3/library/functions.html#help"><code class="docutils literal notranslate"><span class="pre">help()</span></code></a></p></li>
<li><p><a class="reference external" href="https://docs.python.org/3/library/os.html#os.remove"><code class="docutils literal notranslate"><span class="pre">os.remove()</span></code></a></p></li>
<li><p><a class="reference external" href="https://docs.python.org/3/library/shutil.html#shutil.rmtree"><code class="docutils literal notranslate"><span class="pre">shutil.rmtree()</span></code></a></p></li>
<li><p><a class="reference external" href="https://docs.python.org/3/library/subprocess.html#subprocess.run"><code class="docutils literal notranslate"><span class="pre">subprocess.run()</span></code></a></p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://pandas.pydata.org/"><code class="docutils literal notranslate"><span class="pre">pandas</span></code> documentation</a>:</p>
<ul>
<li><p><a class="reference external" href="https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html">10 Minutes to pandas</a></p></li>
<li><p><a class="reference external" href="https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html"><code class="docutils literal notranslate"><span class="pre">.to_csv()</span></code></a></p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://en.wikipedia.org/wiki/Join_(SQL)">Wikipedia; Join (SQL)</a></p></li>
<li><p><a class="reference external" href="https://pages.databricks.com/rs/094-YMS-629/images/LearningSpark2.0.pdf">Databricks: Learning Spark</a></p></li>
</ul>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./pyspark-intro"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="../spark-overview/example-spark-sessions.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Example Spark Sessions</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="pyspark-intro-exercises.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Introduction to PySpark: Exercises</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Wil Roberts & Adrian Prince<br/>
    
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>